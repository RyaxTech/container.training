<!DOCTYPE html>
<html>
  <head>
    <title>Kubernetes Introduction, Architecture and Installation </title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <link rel="stylesheet" href="workshop.css">
    <link rel="stylesheet" href="override.css">
  </head>
  <body>
    <!--
    <div style="position: absolute; left: 20%; right: 20%; top: 30%;">
      <h1 style="font-size: 3em;">Loading ...</h1>
      The slides should show up here. If they don't, it might be
      because you are accessing this file directly from your filesystem.
      It needs to be served from a web server. You can try this:
      <pre>
        docker-compose up -d
        open http://localhost:8888/workshop.html # on MacOS
        xdg-open http://localhost:8888/workshop.html # on Linux
      </pre>
      Once the slides are loaded, this notice disappears when you
      go full screen (e.g. by hitting "f").
    </div>
    -->
    <textarea id="source">class: title, self-paced

Kubernetes<br/>Introduction, Architecture and Installation<br/>

.nav[*Self-paced version*]

.debug[
```
 M dock-kube-day1.yml.html
 M dock-kube-day2.yml.html
 M dock-kube-day3.yml.html
 M dock-kube-day4.yml.html
 M intro-fullday.yml.html
 M intro-selfpaced.yml.html
 M kube-day1.yml.html
 M kube-fullday.yml.html
 M kube-halfday.yml.html
 M kube-jour1.yml.html
 M kube-jour2.yml.html
 M kube-jour3.yml.html
 M kube-selfpaced.yml.html
 M kube/kubeobjects.md
 M swarm-fullday.yml.html
 M swarm-halfday.yml.html
 M swarm-selfpaced.yml.html
 M swarm-video.yml.html

```

These slides have been built from commit: e097b43


[shared/title.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//shared/title.md)]
---

class: title, in-person

Kubernetes<br/>Introduction, Architecture and Installation<br/><br/></br>


.debug[[shared/title.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//shared/title.md)]
---

name: toc-chapter-1

## Chapter 1

- [Working with volumes](#toc-working-with-volumes)

- [Compose for development stacks](#toc-compose-for-development-stacks)

- [Security and Docker Containers](#toc-security-and-docker-containers)

.debug[(auto-generated TOC)]
---
name: toc-chapter-2

## Chapter 2

- [Our sample application](#toc-our-sample-application)

- [Identifying bottlenecks](#toc-identifying-bottlenecks)

.debug[(auto-generated TOC)]
---
name: toc-chapter-3

## Chapter 3

- [Kubernetes concepts](#toc-kubernetes-concepts)

- [Declarative vs imperative](#toc-declarative-vs-imperative)

- [Kubernetes network model](#toc-kubernetes-network-model)

- [First contact with `kubectl`](#toc-first-contact-with-kubectl)

- [Running our first containers on Kubernetes](#toc-running-our-first-containers-on-kubernetes)

.debug[(auto-generated TOC)]
---
name: toc-chapter-4

## Chapter 4

- [Setting up Kubernetes](#toc-setting-up-kubernetes)

- [The Kubernetes dashboard](#toc-the-kubernetes-dashboard)

- [Security implications of `kubectl apply`](#toc-security-implications-of-kubectl-apply)

.debug[(auto-generated TOC)]



.debug[[common/toc.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/toc.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/Container-Ship-Freighter-Navigation-Elbe-Romance-1782991.jpg)]

---

name: toc-working-with-volumes
class: title

Working with volumes

.nav[
[Section pr√©c√©dente](#toc-)
|
[Retour table des mati√®res](#toc-chapter-1)
|
[Section suivante](#toc-compose-for-development-stacks)
]

.debug[(automatically generated title slide)]

---

class: title

# Working with volumes

![volume](images/title-working-with-volumes.jpg)

.debug[[intro/Working_With_Volumes.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Working_With_Volumes.md)]
---

## Objectives

At the end of this section, you will be able to:

* Create containers holding volumes.

* Share volumes across containers.

* Share a host directory with one or many containers.

.debug[[intro/Working_With_Volumes.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Working_With_Volumes.md)]
---

## Working with volumes

Docker volumes can be used to achieve many things, including:

* Sharing a directory between multiple containers.

* Sharing a directory between the host and a container.

* Sharing a *single file* between the host and a container.

.debug[[intro/Working_With_Volumes.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Working_With_Volumes.md)]
---

## Volumes are special directories in a container

Volumes can be declared in two different ways.

* Within a `Dockerfile`, with a `VOLUME` instruction.

```dockerfile
VOLUME /uploads
```

* On the command-line, with the `-v` flag for `docker run`.

```bash
$ docker run -d -v /uploads myapp
```

In both cases, `/uploads` (inside the container) will be a volume.

.debug[[intro/Working_With_Volumes.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Working_With_Volumes.md)]
---

class: extra-details

## Volumes bypass the copy-on-write system

Volumes act as passthroughs to the host filesystem.

* The I/O performance on a volume is exactly the same as I/O performance
  on the Docker host.

* When you `docker commit`, the content of volumes is not brought into
  the resulting image.

* If a `RUN` instruction in a `Dockerfile` changes the content of a
  volume, those changes are not recorded neither.

* If a container is started with the `--read-only` flag, the volume
  will still be writable (unless the volume is a read-only volume).

.debug[[intro/Working_With_Volumes.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Working_With_Volumes.md)]
---

class: extra-details

## Volumes can be shared across containers

You can start a container with *exactly the same volumes* as another one.

The new container will have the same volumes, in the same directories.

They will contain exactly the same thing, and remain in sync.

Under the hood, they are actually the same directories on the host anyway.

This is done using the `--volumes-from` flag for `docker run`.

We will see an example in the following slides.

.debug[[intro/Working_With_Volumes.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Working_With_Volumes.md)]
---

class: extra-details

## Sharing app server logs with another container

Let's start a Tomcat container:

```bash
$ docker run --name webapp -d -p 8080:8080 -v /usr/local/tomcat/logs tomcat
```

Now, start an `alpine` container accessing the same volume:

```bash
$ docker run --volumes-from webapp alpine sh -c "tail -f /usr/local/tomcat/logs/*"
```

Then, from another window, send requests to our Tomcat container:
```bash
$ curl localhost:8080
```

.debug[[intro/Working_With_Volumes.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Working_With_Volumes.md)]
---

## Volumes exist independently of containers

If a container is stopped, its volumes still exist and are available.

Volumes can be listed and manipulated with `docker volume` subcommands:

```bash
$ docker volume ls
DRIVER              VOLUME NAME
local               5b0b65e4316da67c2d471086640e6005ca2264f3...
local               pgdata-prod
local               pgdata-dev
local               13b59c9936d78d109d094693446e174e5480d973...
```

Some of those volume names were explicit (pgdata-prod, pgdata-dev).

The others (the hex IDs) were generated automatically by Docker.

.debug[[intro/Working_With_Volumes.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Working_With_Volumes.md)]
---

## Naming volumes

* Volumes can be created without a container, then used in multiple containers.

Let's create a couple of volumes directly.

```bash
$ docker volume create webapps
webapps
```

```bash
$ docker volume create logs
logs
```

Volumes are not anchored to a specific path.

.debug[[intro/Working_With_Volumes.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Working_With_Volumes.md)]
---

## Using our named volumes

* Volumes are used with the `-v` option.

* When a host path does not contain a /, it is considered to be a volume name.

Let's start a web server using the two previous volumes.

```bash
$ docker run -d -p 1234:8080 \
         -v logs:/usr/local/tomcat/logs \
         -v webapps:/usr/local/tomcat/webapps \
         tomcat
```

Check that it's running correctly:

```bash
$ curl localhost:1234
... (Tomcat tells us how happy it is to be up and running) ...
```

.debug[[intro/Working_With_Volumes.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Working_With_Volumes.md)]
---

## Using a volume in another container

* We will make changes to the volume from another container.

* In this example, we will run a text editor in the other container.

  (But this could be a FTP server, a WebDAV server, a Git receiver...)

Let's start another container using the `webapps` volume.

```bash
$ docker run -v webapps:/webapps -w /webapps -ti alpine vi ROOT/index.jsp
```

Vandalize the page, save, exit.

Then run `curl localhost:1234` again to see your changes.

.debug[[intro/Working_With_Volumes.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Working_With_Volumes.md)]
---

## Managing volumes explicitly

In some cases, you want a specific directory on the host to be mapped
inside the container:

* You want to manage storage and snapshots yourself.

    (With LVM, or a SAN, or ZFS, or anything else!)

* You have a separate disk with better performance (SSD) or resiliency (EBS)
  than the system disk, and you want to put important data on that disk.

* You want to share your source directory between your host (where the
  source gets edited) and the container (where it is compiled or executed).

Wait, we already met the last use-case in our example development workflow!
Nice.

```bash
$ docker run -d -v /path/on/the/host:/path/in/container image ...
```

.debug[[intro/Working_With_Volumes.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Working_With_Volumes.md)]
---

class: extra-details

## Migrating data with `--volumes-from`

The `--volumes-from` option tells Docker to re-use all the volumes
of an existing container.

* Scenario: migrating from Redis 2.8 to Redis 3.0.

* We have a container (`myredis`) running Redis 2.8.

* Stop the `myredis` container.

* Start a new container, using the Redis 3.0 image, and the `--volumes-from` option.

* The new container will inherit the data of the old one.

* Newer containers can use `--volumes-from` too.

.debug[[intro/Working_With_Volumes.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Working_With_Volumes.md)]
---

class: extra-details

## Data migration in practice

Let's create a Redis container.

```bash
$ docker run -d --name redis28 redis:2.8
```

Connect to the Redis container and set some data.

```bash
$ docker run -ti --link redis28:redis alpine telnet redis 6379
```

Issue the following commands:

```bash
SET counter 42
INFO server
SAVE
QUIT
```

.debug[[intro/Working_With_Volumes.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Working_With_Volumes.md)]
---

class: extra-details

## Upgrading Redis

Stop the Redis container.

```bash
$ docker stop redis28
```

Start the new Redis container.

```bash
$ docker run -d --name redis30 --volumes-from redis28 redis:3.0
```

.debug[[intro/Working_With_Volumes.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Working_With_Volumes.md)]
---

class: extra-details

## Testing the new Redis

Connect to the Redis container and see our data.

```bash
docker run -ti --link redis30:redis alpine telnet redis 6379
```

Issue a few commands.

```bash
GET counter
INFO server
QUIT
```

.debug[[intro/Working_With_Volumes.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Working_With_Volumes.md)]
---

## Volumes lifecycle

* When you remove a container, its volumes are kept around.

* You can list them with `docker volume ls`.

* You can access them by creating a container with `docker run -v`.

* You can remove them with `docker volume rm` or `docker system prune`.

Ultimately, _you_ are the one responsible for logging,
monitoring, and backup of your volumes.

.debug[[intro/Working_With_Volumes.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Working_With_Volumes.md)]
---

class: extra-details

## Checking volumes defined by an image

Wondering if an image has volumes? Just use `docker inspect`:

```bash
$ # docker inspect training/datavol
[{
  "config": {
    . . .
    "Volumes": {
        "/var/webapp": {}
    },
    . . .
}]
```

.debug[[intro/Working_With_Volumes.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Working_With_Volumes.md)]
---

class: extra-details

## Checking volumes used by a container

To look which paths are actually volumes, and to what they are bound,
use `docker inspect` (again):

```bash
$ docker inspect <yourContainerID>
[{
  "ID": "<yourContainerID>",
. . .
  "Volumes": {
     "/var/webapp": "/var/lib/docker/vfs/dir/f4280c5b6207ed531efd4cc673ff620cef2a7980f747dbbcca001db61de04468"
  },
  "VolumesRW": {
     "/var/webapp": true
  },
}]
```

* We can see that our volume is present on the file system of the Docker host.

.debug[[intro/Working_With_Volumes.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Working_With_Volumes.md)]
---

## Sharing a single file

The same `-v` flag can be used to share a single file (instead of a directory).

One of the most interesting examples is to share the Docker control socket.

```bash
$ docker run -it -v /var/run/docker.sock:/var/run/docker.sock docker sh
```

From that container, you can now run `docker` commands communicating with
the Docker Engine running on the host. Try `docker ps`!

.warning[Since that container has access to the Docker socket, it
has root-like access to the host.]

.debug[[intro/Working_With_Volumes.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Working_With_Volumes.md)]
---

## Volume plugins

You can install plugins to manage volumes backed by particular storage systems,
or providing extra features. For instance:

* [dvol](https://github.com/ClusterHQ/dvol) - allows to commit/branch/rollback volumes;
* [Flocker](https://clusterhq.com/flocker/introduction/), [REX-Ray](https://github.com/emccode/rexray) - create and manage volumes backed by an enterprise storage system (e.g. SAN or NAS), or by cloud block stores (e.g. EBS);
* [Blockbridge](http://www.blockbridge.com/), [Portworx](http://portworx.com/) - provide distributed block store for containers;
* and much more!

.debug[[intro/Working_With_Volumes.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Working_With_Volumes.md)]
---

## Volumes vs. Mounts

* Since Docker 17.06, a new options is available: `--mount`.

* It offers a new, richer syntax to manipulate data in containers.

* It makes an explicit difference between:

  - volumes (identified with a unique name, managed by a storage plugin),

  - bind mounts (identified with a host path, not managed).

* The former `-v` / `--volume` option is still usable.

.debug[[intro/Working_With_Volumes.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Working_With_Volumes.md)]
---

## `--mount` syntax

Binding a host path to a container path:

```bash
$ docker run \
  --mount type=bind,source=/path/on/host,target=/path/in/container alpine
```

Mounting a volume to a container path:

```bash
$ docker run \
  --mount source=myvolume,target=/path/in/container alpine
```

Mounting a tmpfs (in-memory, for temporary files):

```bash
$ docker run \
  --mount type=tmpfs,destination=/path/in/container,tmpfs-size=1000000 alpine
```

.debug[[intro/Working_With_Volumes.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Working_With_Volumes.md)]
---

## Section summary

We've learned how to:

* Create and manage volumes.

* Share volumes across containers.

* Share a host directory with one or many containers.

.debug[[intro/Working_With_Volumes.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Working_With_Volumes.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/ShippingContainerSFBay.jpg)]

---

name: toc-compose-for-development-stacks
class: title

Compose for development stacks

.nav[
[Section pr√©c√©dente](#toc-working-with-volumes)
|
[Retour table des mati√®res](#toc-chapter-1)
|
[Section suivante](#toc-security-and-docker-containers)
]

.debug[(automatically generated title slide)]

---
# Compose for development stacks

Dockerfiles are great to build container images.

But what if we work with a complex stack made of multiple containers?

Eventually, we will want to write some custom scripts and automation to build, run, and connect
our containers together.

There is a better way: using Docker Compose.

In this section, you will use Compose to bootstrap a development environment.

.debug[[intro/Compose_For_Dev_Stacks.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Compose_For_Dev_Stacks.md)]
---

## What is Docker Compose?

Docker Compose (formerly known as `fig`) is an external tool.

Unlike the Docker Engine, it is written in Python. It's open source as well.

The general idea of Compose is to enable a very simple, powerful onboarding workflow:

1. Checkout your code.

2. Run `docker-compose up`.

3. Your app is up and running!

.debug[[intro/Compose_For_Dev_Stacks.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Compose_For_Dev_Stacks.md)]
---

## Compose overview

This is how you work with Compose:

* You describe a set (or stack) of containers in a YAML file called `docker-compose.yml`.

* You run `docker-compose up`.

* Compose automatically pulls images, builds containers, and starts them.

* Compose can set up links, volumes, and other Docker options for you.

* Compose can run the containers in the background, or in the foreground.

* When containers are running in the foreground, their aggregated output is shown.

Before diving in, let's see a small example of Compose in action.

.debug[[intro/Compose_For_Dev_Stacks.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Compose_For_Dev_Stacks.md)]
---

## Compose steps

- There are three steps to using Docker Compose:

1. Define each service in a Dockerfile.
2. Define the services and their relation to each other in the `docker-compose.yml` file.
3. Use `docker-compose up` to start the system.

.debug[[intro/Compose_For_Dev_Stacks.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Compose_For_Dev_Stacks.md)]
---


## Compose in action

![composeup](images/composeup.gif)

.debug[[intro/Compose_For_Dev_Stacks.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Compose_For_Dev_Stacks.md)]
---

## Checking if Compose is installed

If you are using the official training virtual machines, Compose has been
pre-installed.

If you are using Docker for Mac/Windows or the Docker Toolbox, Compose comes with them.

If you are on Linux (desktop or server environment), you will need to install Compose from its [release page](https://github.com/docker/compose/releases) or with `pip install docker-compose`.

You can always check that it is installed by running:

```bash
$ docker-compose --version
```

.debug[[intro/Compose_For_Dev_Stacks.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Compose_For_Dev_Stacks.md)]
---

## Launching Our First Stack with Compose

First step: clone the source code for the app we will be working on.

```bash
$ cd
$ git clone git://github.com/jpetazzo/trainingwheels
...
$ cd trainingwheels
```


Second step: start your app.

```bash
$ docker-compose up
```

Watch Compose build and run your app with the correct parameters,
including linking the relevant containers together.

.debug[[intro/Compose_For_Dev_Stacks.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Compose_For_Dev_Stacks.md)]
---

## Launching Our First Stack with Compose

Verify that the app is running at `http://<yourHostIP>:8000`.

![composeapp](images/composeapp.png)

.debug[[intro/Compose_For_Dev_Stacks.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Compose_For_Dev_Stacks.md)]
---

## Stopping the app

When you hit `^C`, Compose tries to gracefully terminate all of the containers.

After ten seconds (or if you press `^C` again) it will forcibly kill
them.

.debug[[intro/Compose_For_Dev_Stacks.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Compose_For_Dev_Stacks.md)]
---

## The `docker-compose.yml` file

Here is the file used in the demo:

.small[
```yaml
version: "2"

services:
  www:
    build: www
    ports:
      - 8000:5000
    user: nobody
    environment:
      DEBUG: 1
    command: python counter.py
    volumes:
      - ./www:/src

  redis:
    image: redis
```
]

.debug[[intro/Compose_For_Dev_Stacks.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Compose_For_Dev_Stacks.md)]
---

## Compose file structure

A Compose file has multiple sections:

* `version` is mandatory. (We should use `"2"` or later; version 1 is deprecated.)

* `services` is mandatory. A service is one or more replicas of the same image running as containers.

* `networks` is optional and indicates to which networks containers should be connected.
  <br/>(By default, containers will be connected on a private, per-compose-file network.)

* `volumes` is optional and can define volumes to be used and/or shared by the containers.

.debug[[intro/Compose_For_Dev_Stacks.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Compose_For_Dev_Stacks.md)]
---

## Compose file versions

* Version 1 is legacy and shouldn't be used.

  (If you see a Compose file without `version` and `services`, it's a legacy v1 file.)

* Version 2 added support for networks and volumes.

* Version 3 added support for deployment options (scaling, rolling updates, etc).

The [Docker documentation](https://docs.docker.com/compose/compose-file/)
has excellent information about the Compose file format if you need to know more about versions.

.debug[[intro/Compose_For_Dev_Stacks.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Compose_For_Dev_Stacks.md)]
---

## Containers in `docker-compose.yml`

Each service in the YAML file must contain either `build`, or `image`.

* `build` indicates a path containing a Dockerfile.

* `image` indicates an image name (local, or on a registry).

* If both are specified, an image will be built from the `build` directory and named `image`.

The other parameters are optional.

They encode the parameters that you would typically add to `docker run`.

Sometimes they have several minor improvements.

.debug[[intro/Compose_For_Dev_Stacks.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Compose_For_Dev_Stacks.md)]
---

## Container parameters

* `command` indicates what to run (like `CMD` in a Dockerfile).

* `ports` translates to one (or multiple) `-p` options to map ports.
  <br/>You can specify local ports (i.e. `x:y` to expose public port `x`).

* `volumes` translates to one (or multiple) `-v` options.
  <br/>You can use relative paths here.

For the full list, check: https://docs.docker.com/compose/compose-file/

.debug[[intro/Compose_For_Dev_Stacks.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Compose_For_Dev_Stacks.md)]
---

## Compose commands

We already saw `docker-compose up`, but another one is `docker-compose build`.

It will execute `docker build` for all containers mentioning a `build` path.

It can also be invoked automatically when starting the application:

```bash
docker-compose up --build
```

Another common option is to start containers in the background:

```bash
docker-compose up -d
```

.debug[[intro/Compose_For_Dev_Stacks.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Compose_For_Dev_Stacks.md)]
---

## Check container status

It can be tedious to check the status of your containers with `docker ps`,
especially when running multiple apps at the same time.

Compose makes it easier; with `docker-compose ps` you will see only the status of the
containers of the current stack:


```bash
$ docker-compose ps
Name                      Command             State           Ports          
----------------------------------------------------------------------------
trainingwheels_redis_1   /entrypoint.sh red   Up      6379/tcp               
trainingwheels_www_1     python counter.py    Up      0.0.0.0:8000->5000/tcp 
```

.debug[[intro/Compose_For_Dev_Stacks.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Compose_For_Dev_Stacks.md)]
---

## Cleaning up (1)

If you have started your application in the background with Compose and
want to stop it easily, you can use the `kill` command:

```bash
$ docker-compose kill
```

Likewise, `docker-compose rm` will let you remove containers (after confirmation):

```bash
$ docker-compose rm
Going to remove trainingwheels_redis_1, trainingwheels_www_1
Are you sure? [yN] y
Removing trainingwheels_redis_1...
Removing trainingwheels_www_1...
```

.debug[[intro/Compose_For_Dev_Stacks.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Compose_For_Dev_Stacks.md)]
---

## Cleaning up (2)

Alternatively, `docker-compose down` will stop and remove containers.

It will also remove other resources, like networks that were created for the application.

```bash
$ docker-compose down
Stopping trainingwheels_www_1 ... done
Stopping trainingwheels_redis_1 ... done
Removing trainingwheels_www_1 ... done
Removing trainingwheels_redis_1 ... done
```

Use `docker-compose down -v` to remove everything including volumes.

.debug[[intro/Compose_For_Dev_Stacks.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Compose_For_Dev_Stacks.md)]
---

## Special handling of volumes

Compose is smart. If your container uses volumes, when you restart your
application, Compose will create a new container, but carefully re-use
the volumes it was using previously.

This makes it easy to upgrade a stateful service, by pulling its
new image and just restarting your stack with Compose.

.debug[[intro/Compose_For_Dev_Stacks.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Compose_For_Dev_Stacks.md)]
---

## Compose project name

* When you run a Compose command, Compose infers the "project name" of your app.

* By default, the "project name" is the name of the current directory.

* For instance, if you are in `/home/zelda/src/ocarina`, the project name is `ocarina`.

* All resources created by Compose are tagged with this project name.

* The project name also appears as a prefix of the names of the resources.

  E.g. in the previous example, service `www` will create a container `ocarina_www_1`.

* The project name can be overridden with `docker-compose -p`.

.debug[[intro/Compose_For_Dev_Stacks.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Compose_For_Dev_Stacks.md)]
---

## Running two copies of the same app

If you want to run two copies of the same app simultaneously, all you have to do is to
make sure that each copy has a different project name.

You can:

* copy your code in a directory with a different name

* start each copy with `docker-compose -p myprojname up`

Each copy will run in a different network, totally isolated from the other.

This is ideal to debug regressions, do side-by-side comparisons, etc.

.debug[[intro/Compose_For_Dev_Stacks.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Compose_For_Dev_Stacks.md)]
---

## Exercise with Docker-compose, Volume and Database

.exercise[

- Download the ready docker-compose.yml that makes use of a Persistent Data Storage for Postgresql

```bash
curl -s https://raw.githubusercontent.com/RyaxTech/kube-tutorial/master/docker-compose.yml --output docker-compose.yml
```
]

- `external: true` tells Docker Compose to use a pre-existing external data volume. 
- If no volume named data is present, starting the application will cause an error. Create the volume:

```bash
docker volume create --name=data
```

- and then start your compose with
```bash
docker-compose up
```
]


.debug[[intro/Compose_For_Dev_Stacks.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Compose_For_Dev_Stacks.md)]
---

## Exercise with Docker-compose, Volume and Database

- Create some content within the postgresql
- Start another app with docker-compose so that another postgresql is create that makes use of the same database
- Create data on one container and see if the data exist on the other
- Kill the applications and start one and see again if the data are available.


.debug[[intro/Compose_For_Dev_Stacks.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Compose_For_Dev_Stacks.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/aerial-view-of-containers.jpg)]

---

name: toc-security-and-docker-containers
class: title

Security and Docker Containers

.nav[
[Section pr√©c√©dente](#toc-compose-for-development-stacks)
|
[Retour table des mati√®res](#toc-chapter-1)
|
[Section suivante](#toc-our-sample-application)
]

.debug[(automatically generated title slide)]

---

# Security and Docker Containers

The principal security vulnerabilities in Docker:

1. **Kernel exploits**: Unlike a virtual machine, the kernel is shared among all containers and the host. If a container causes a kernel to panic, it will take down the whole host.
2. **Poisoned images**: If an attacker can trick you into running their image, the host and data are at risk.
3. **Denial-of-Service (DoS) attacks**: Containers share kernel resources, so if one container is able to monopolise the access to certain resources, it can starve out other containers on the host. This results in a denial-of-service (DoS). Users are then unable to access part or all of the system.
4. **Container breakouts**: Be aware of potential privilege escalation attacks, where a user gains elevated privileges through a bug in application code that must run with extra privileges. While unlikely, breakouts are possible and should be considered when developing a continuity plan.
5. **Compromising secrets**: When a container accesses a database or service it will require a secret, like an API key or username and password. An attacker that gains access to the secret will also have access to the service.

.debug[[intro/Compose_For_Dev_Stacks.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Compose_For_Dev_Stacks.md)]
---

## Security for Docker Containers best practices

- **1. Use trusted images** 

Make sure that the images that are running are up to date. Developers often assemble Docker images rather than build them from scratch. Be sure to set up a trusted registry of base images, which are the only image developers would be allowed to use.

- **2. Limit the Resource Utilization**

Since Docker containers are lightweight processes, you can run many more containers than virtual machines. This increased density is beneficial, as it increases host resource utilization and allows you to optimize total cost of ownership. It also implies that a far greater number of processes are competing for host resources. To reduce the threat of vulnerabilities such as denial-of-service attacks, and performance impacts due to noisy neighbors, you can put limits on the system resources that individual containers can consume, through container orchestration frameworks such as Kubernetes

.debug[[intro/Compose_For_Dev_Stacks.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Compose_For_Dev_Stacks.md)]
---

## Security for Docker Containers best practices

- **3. Docker Host, Application Runtime, and Code-Level Security**

Keep the host operating system properly patched and updated. Processes running inside your container should have the latest security updates. Incorporate security best practices into your application code. As you build Docker container images, you need to know exactly what goes into each layer. Ensure that containers installed by third-party vendors do not download and run anything at runtime. Everything that a Docker container runs must be declared and included in the static container image.

- **4. Manage secrets** 

Putting secrets in the container image exposes it to many users and processes and puts it in jeopardy of being misused. You want to provide the container with access to the secrets it needs as it‚Äôs running, and not before. The secret should only be accessible to the relevant containers, and should not be stored on disk or be exposed at the host level. Once the container is stopped, the secret disappears.

- By taking a proactive approach, creating and implementing security policies throughout the entire container lifecycle, a containerized environment can be secured very effectively..

.debug[[intro/Compose_For_Dev_Stacks.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Compose_For_Dev_Stacks.md)]
---



.debug[[intro/Compose_For_Dev_Stacks.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Compose_For_Dev_Stacks.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/blue-containers.jpg)]

---

name: toc-our-sample-application
class: title

Our sample application

.nav[
[Section pr√©c√©dente](#toc-security-and-docker-containers)
|
[Retour table des mati√®res](#toc-chapter-2)
|
[Section suivante](#toc-identifying-bottlenecks)
]

.debug[(automatically generated title slide)]

---
# Our sample application

- We will clone the GitHub repository onto our `node1`

- The repository also contains scripts and tools that we will use through the workshop

.exercise[

- Clone the repository on `node1`:
  ```bash
  git clone https://github.com/RyaxTech/kube.training
  ```

]

(You can also fork the repository on GitHub and clone your fork if you prefer that.)

.debug[[common/sampleapp.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/sampleapp.md)]
---

## Downloading and running the application

Let's start this before we look around, as downloading will take a little time...

.exercise[

- Go to the `dockercoins` directory, in the cloned repo:
  ```bash
  cd ~/kube.training/dockercoins
  ```

- Use Compose to build and run all containers:
  ```bash
  docker-compose up
  ```

]

Compose tells Docker to build all container images (pulling
the corresponding base images), then starts all containers,
and displays aggregated logs.

.debug[[common/sampleapp.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/sampleapp.md)]
---

## More detail on our sample application

- Visit the GitHub repository with all the materials of this workshop:
  <br/> https://github.com/RyaxTech/kube.training

- The application is in the [dockercoins](
  https://github.com/RyaxTech/kube.training/tree/master/dockercoins)
  subdirectory

- Let's look at the general layout of the source code:

  there is a Compose file [docker-compose.yml](
  https://github.com/RyaxTech/kube.training/blob/master/dockercoins/docker-compose.yml) ...

  ... and 4 other services, each in its own directory:

  - `rng` = web service generating random bytes
  - `hasher` = web service computing hash of POSTed data
  - `worker` = background process using `rng` and `hasher`
  - `webui` = web interface to watch progress

.debug[[common/sampleapp.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/sampleapp.md)]
---

class: extra-details

## Compose file format version

*Particularly relevant if you have used Compose before...*

- Compose 1.6 introduced support for a new Compose file format (aka "v2")

- Services are no longer at the top level, but under a `services` section

- There has to be a `version` key at the top level, with value `"2"` (as a string, not an integer)

- Containers are placed on a dedicated network, making links unnecessary

- There are other minor differences, but upgrade is easy and straightforward

.debug[[common/sampleapp.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/sampleapp.md)]
---

## Service discovery in container-land

- We do not hard-code IP addresses in the code

- We do not hard-code FQDN in the code, either

- We just connect to a service name, and container-magic does the rest

  (And by container-magic, we mean "a crafty, dynamic, embedded DNS server")

.debug[[common/sampleapp.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/sampleapp.md)]
---

## Example in `worker/worker.py`

```python
redis = Redis("`redis`")


def get_random_bytes():
    r = requests.get("http://`rng`/32")
    return r.content


def hash_bytes(data):
    r = requests.post("http://`hasher`/",
                      data=data,
                      headers={"Content-Type": "application/octet-stream"})
```

(Full source code available [here](
https://github.com/RyaxTech/kube.training/blob/8279a3bce9398f7c1a53bdd95187c53eda4e6435/dockercoins/worker/worker.py#L17
))

.debug[[common/sampleapp.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/sampleapp.md)]
---

class: extra-details

## Links, naming, and service discovery

- Containers can have network aliases (resolvable through DNS)

- Compose file version 2+ makes each container reachable through its service name

- Compose file version 1 did require "links" sections

- Network aliases are automatically namespaced

  - you can have multiple apps declaring and using a service named `database`

  - containers in the blue app will resolve `database` to the IP of the blue database

  - containers in the green app will resolve `database` to the IP of the green database

.debug[[common/sampleapp.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/sampleapp.md)]
---

## What's this application?

--

- It is a DockerCoin miner! .emoji[üí∞üê≥üì¶üö¢]

--

- No, you can't buy coffee with DockerCoins

--

- How DockerCoins works:

  - `worker` asks to `rng` to generate a few random bytes

  - `worker` feeds these bytes into `hasher`

  - and repeat forever!

  - every second, `worker` updates `redis` to indicate how many loops were done

  - `webui` queries `redis`, and computes and exposes "hashing speed" in your browser

.debug[[common/sampleapp.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/sampleapp.md)]
---

## Our application at work

- On the left-hand side, the "rainbow strip" shows the container names

- On the right-hand side, we see the output of our containers

- We can see the `worker` service making requests to `rng` and `hasher`

- For `rng` and `hasher`, we see HTTP access logs

.debug[[common/sampleapp.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/sampleapp.md)]
---

## Connecting to the web UI

- "Logs are exciting and fun!" (No-one, ever)

- The `webui` container exposes a web dashboard; let's view it

.exercise[

- With a web browser, connect to `node1` on port 8000

- Remember: the `nodeX` aliases are valid only on the nodes themselves

- In your browser, you need to enter the IP address of your node

<!-- ```open http://node1:8000``` -->

]

A drawing area should show up, and after a few seconds, a blue
graph will appear.

.debug[[common/sampleapp.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/sampleapp.md)]
---

class: self-paced, extra-details

## If the graph doesn't load

If you just see a `Page not found` error, it might be because your
Docker Engine is running on a different machine. This can be the case if:

- you are using the Docker Toolbox

- you are using a VM (local or remote) created with Docker Machine

- you are controlling a remote Docker Engine

When you run DockerCoins in development mode, the web UI static files
are mapped to the container using a volume. Alas, volumes can only
work on a local environment, or when using Docker4Mac or Docker4Windows.

How to fix this?

Stop the app with `^C`, edit `dockercoins.yml`, comment out the `volumes` section, and try again.

.debug[[common/sampleapp.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/sampleapp.md)]
---

class: extra-details

## Why does the speed seem irregular?

- It *looks like* the speed is approximately 4 hashes/second

- Or more precisely: 4 hashes/second, with regular dips down to zero

- Why?

--

class: extra-details

- The app actually has a constant, steady speed: 3.33 hashes/second
  <br/>
  (which corresponds to 1 hash every 0.3 seconds, for *reasons*)

- Yes, and?

.debug[[common/sampleapp.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/sampleapp.md)]
---

class: extra-details

## The reason why this graph is *not awesome*

- The worker doesn't update the counter after every loop, but up to once per second

- The speed is computed by the browser, checking the counter about once per second

- Between two consecutive updates, the counter will increase either by 4, or by 0

- The perceived speed will therefore be 4 - 4 - 4 - 0 - 4 - 4 - 0 etc.

- What can we conclude from this?

--

class: extra-details

- "I'm clearly incapable of writing good frontend code!" üòÄ ‚Äî J√©r√¥me

.debug[[common/sampleapp.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/sampleapp.md)]
---

## Stopping the application

- If we interrupt Compose (with `^C`), it will politely ask the Docker Engine to stop the app

- The Docker Engine will send a `TERM` signal to the containers

- If the containers do not exit in a timely manner, the Engine sends a `KILL` signal

.exercise[

- Stop the application by hitting `^C`

<!--
```keys ^C```
-->

]

--

Some containers exit immediately, others take longer.

The containers that do not handle `SIGTERM` end up being killed after a 10s timeout. If we are very impatient, we can hit `^C` a second time!


.debug[[common/sampleapp.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/sampleapp.md)]
---
## Restarting in the background

- Many flags and commands of Compose are modeled after those of `docker`

.exercise[

- Start the app in the background with the `-d` option:
  ```bash
  docker-compose up -d
  ```

- Check that our app is running with the `ps` command:
  ```bash
  docker-compose ps
  ```

]

`docker-compose ps` also shows the ports exposed by the application.

.debug[[common/composescale.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/composescale.md)]
---

class: extra-details

## Viewing logs

- The `docker-compose logs` command works like `docker logs`

.exercise[

- View all logs since container creation and exit when done:
  ```bash
  docker-compose logs
  ```

- Stream container logs, starting at the last 10 lines for each container:
  ```bash
  docker-compose logs --tail 10 --follow
  ```

<!--
```wait units of work done```
```keys ^C```
-->

]

Tip: use `^S` and `^Q` to pause/resume log output.

.debug[[common/composescale.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/composescale.md)]
---

## Scaling up the application

- Our goal is to make that performance graph go up (without changing a line of code!)

--

- Before trying to scale the application, we'll figure out if we need more resources

  (CPU, RAM...)

- For that, we will use good old UNIX tools on our Docker node

.debug[[common/composescale.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/composescale.md)]
---

## Looking at resource usage

- Let's look at CPU, memory, and I/O usage

.exercise[

- run `top` to see CPU and memory usage (you should see idle cycles)

<!--
```bash top```

```wait Tasks```
```keys ^C```
-->

- run `vmstat 1` to see I/O usage (si/so/bi/bo)
  <br/>(the 4 numbers should be almost zero, except `bo` for logging)

<!--
```bash vmstat 1```

```wait memory```
```keys ^C```
-->

]

We have available resources.

- Why?
- How can we use them?

.debug[[common/composescale.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/composescale.md)]
---

## Scaling workers on a single node

- Docker Compose supports scaling
- Let's scale `worker` and see what happens!

.exercise[

- Start one more `worker` container:
  ```bash
  docker-compose up --scale worker=2
  ```

- Look at the performance graph (it should show a x2 improvement)

- Look at the aggregated logs of our containers (`worker_2` should show up)

- Look at the impact on CPU load with e.g. top (it should be negligible)

]

.debug[[common/composescale.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/composescale.md)]
---

## Adding more workers

- Great, let's add more workers and call it a day, then!

.exercise[

- Start eight more `worker` containers:
  ```bash
  docker-compose up --scale worker=10
  ```

- Look at the performance graph: does it show a x10 improvement?

- Look at the aggregated logs of our containers

- Look at the impact on CPU load and memory usage

]

.debug[[common/composescale.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/composescale.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/chinook-helicopter-container.jpg)]

---

name: toc-identifying-bottlenecks
class: title

Identifying bottlenecks

.nav[
[Section pr√©c√©dente](#toc-our-sample-application)
|
[Retour table des mati√®res](#toc-chapter-2)
|
[Section suivante](#toc-kubernetes-concepts)
]

.debug[(automatically generated title slide)]

---

# Identifying bottlenecks

- You should have seen a 3x speed bump (not 10x)

- Adding workers didn't result in linear improvement

- *Something else* is slowing us down

--

- ... But what?

--

- The code doesn't have instrumentation

- Let's use state-of-the-art HTTP performance analysis!
  <br/>(i.e. good old tools like `ab`, `httping`...)

.debug[[common/composescale.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/composescale.md)]
---

## Accessing internal services

- `rng` and `hasher` are exposed on ports 8001 and 8002

- This is declared in the Compose file:

  ```yaml
    ...
    rng:
      build: rng
      ports:
      - "8001:80"

    hasher:
      build: hasher
      ports:
      - "8002:80"
    ...
  ```

.debug[[common/composescale.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/composescale.md)]
---

## Measuring latency under load

We will use `httping`.

.exercise[

- Check the latency of `rng`:
  ```bash
  httping -c 3 localhost:8001
  ```

- Check the latency of `hasher`:
  ```bash
  httping -c 3 localhost:8002
  ```

]

`rng` has a much higher latency than `hasher`.

.debug[[common/composescale.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/composescale.md)]
---

## Let's draw hasty conclusions

- The bottleneck seems to be `rng`

- *What if* we don't have enough entropy and can't generate enough random numbers?

- We need to scale out the `rng` service on multiple machines!

Note: this is a fiction! We have enough entropy. But we need a pretext to scale out.

(In fact, the code of `rng` uses `/dev/urandom`, which never runs out of entropy...
<br/>
...and is [just as good as `/dev/random`](http://www.slideshare.net/PacSecJP/filippo-plain-simple-reality-of-entropy).)

.debug[[common/composescale.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/composescale.md)]
---
## Clean up

- Before moving on, let's remove those containers

.exercise[

- Tell Compose to remove everything:
  ```bash
  docker-compose down
  ```

]

.debug[[common/composedown.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/composedown.md)]
---

## To go even further with Docker Compose

- Read the [overview](https://docs.docker.com/compose/overview/) of the official documentation.

- The examples are also interesting such as: [Quickstart: Compose and WordPress](https://docs.docker.com/compose/wordpress/).


.debug[[common/composedown.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/composedown.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/container-cranes.jpg)]

---

name: toc-kubernetes-concepts
class: title

Kubernetes concepts

.nav[
[Section pr√©c√©dente](#toc-identifying-bottlenecks)
|
[Retour table des mati√®res](#toc-chapter-3)
|
[Section suivante](#toc-declarative-vs-imperative)
]

.debug[(automatically generated title slide)]

---
# Kubernetes concepts

- Kubernetes is a container management system

- It runs and manages containerized applications on a cluster

--

- What does that really mean?

.debug[[kube/concepts-k8s.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/concepts-k8s.md)]
---

## Kubernetes Introduction

--

- It is a software to deploy and manage containerized applications while offering an optimal usage of the compute platform.

--

- It abstracts the underlying infrastructure by simplifying the development of applications and the management of resources.

.debug[[kube/concepts-k8s.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/concepts-k8s.md)]
---

## Kubernetes Benefits

--

- Simplify Application Deployment.

--

- Achieving better utilization of hardware.

--

- Automatic Scaling.

--

- Simplifying applications development

--

- High Availability, heath check and self-healing

.debug[[kube/concepts-k8s.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/concepts-k8s.md)]
---



## Basic things we can ask Kubernetes to do

--

- Start 5 containers using image `atseashop/api:v1.3`

--

- Place an internal load balancer in front of these containers

--

- Start 10 containers using image `atseashop/webfront:v1.3`

--

- Place a public load balancer in front of these containers

--

- It's Christmas, traffic spikes, grow our cluster and add containers

--

- New release! Replace my containers with the new image `atseashop/webfront:v1.4`

--

- Keep processing requests during the upgrade; update my containers one at a time

.debug[[kube/concepts-k8s.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/concepts-k8s.md)]
---

## Other things that Kubernetes can do for us

- Basic autoscaling

- Blue/green deployment, canary deployment

- Long running services, but also batch (one-off) jobs

- Overcommit our cluster and *evict* low-priority jobs

- Run services with *stateful* data (databases etc.)

- Fine-grained access control defining *what* can be done by *whom* on *which* resources

- Integrating third party services (*service catalog*)

- Automating complex tasks (*operators*)

.debug[[kube/concepts-k8s.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/concepts-k8s.md)]
---

## Kubernetes architecture

.debug[[kube/concepts-k8s.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/concepts-k8s.md)]
---

class: pic

![haha only kidding](images/k8s-arch1.png)

.debug[[kube/concepts-k8s.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/concepts-k8s.md)]
---

## Kubernetes architecture

- Ha ha ha ha

- OK, I was trying to scare you, it's much simpler than that ‚ù§Ô∏è

.debug[[kube/concepts-k8s.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/concepts-k8s.md)]
---

class: pic

![Celui-l√† ressemble plus √† la r√©alit√©](images/kube_archi_simple.png)

.debug[[kube/concepts-k8s.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/concepts-k8s.md)]
---

class: pic

![that one is more like the real thing](images/k8s-arch2.png)

.debug[[kube/concepts-k8s.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/concepts-k8s.md)]
---

## Credits

- The first schema is a Kubernetes cluster with storage backed by multi-path iSCSI

  (Courtesy of [Yongbok Kim](https://www.yongbok.net/blog/))

- The second has been taken from the book of Marko Luksa "Kubernetes in Action"

- The third one is a simplified representation of a Kubernetes cluster

  (Courtesy of [Imesh Gunaratne](https://medium.com/containermind/a-reference-architecture-for-deploying-wso2-middleware-on-kubernetes-d4dee7601e8e))

.debug[[kube/concepts-k8s.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/concepts-k8s.md)]
---

## Kubernetes name

- How do we pronounce kubernetes?
    - The word comes from the greek work Œ∫œÖŒ≤ŒµœÅŒΩŒÆœÑŒ∑œÇ, pronounced "kivernitis" meaning the captain of a ship or plane
    - In english we say : "coubernetis"
    - In french : "cubernetesse" ou "cubernette"

- It is quite common to write Kubernetes as k8s


.debug[[kube/concepts-k8s.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/concepts-k8s.md)]
---

## Kubernetes architecture: the nodes

- The nodes executing our containers run a collection of services:

  - a container Engine (typically Docker)

  - kubelet (the "node agent")

  - kube-proxy (a necessary but not sufficient network component)

- Nodes were formerly called "minions"

  (You might see that word in older articles or documentation)

.debug[[kube/concepts-k8s.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/concepts-k8s.md)]
---

## Kubernetes architecture: the control plane

- The Kubernetes logic (its "brains") is a collection of services:

  - the API server (our point of entry to everything!)

  - core services like the scheduler and controller manager

  - `etcd` (a highly available key/value store; the "database" of Kubernetes)

- Together, these services form the control plane of our cluster

- The control plane is also called the "master"

.debug[[kube/concepts-k8s.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/concepts-k8s.md)]
---

## Running the control plane on special nodes

- It is common to reserve a dedicated node for the control plane

  (Except for single-node development clusters, like when using minikube)

- This node is then called a "master"

  (Yes, this is ambiguous: is the "master" a node, or the whole control plane?)

- Normal applications are restricted from running on this node

  (By using a mechanism called ["taints"](https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/))

- When high availability is required, each service of the control plane must be resilient

- The control plane is then replicated on multiple nodes

  (This is sometimes called a "multi-master" setup)

.debug[[kube/concepts-k8s.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/concepts-k8s.md)]
---

## Running the control plane outside containers

- The services of the control plane can run in or out of containers

- For instance: since `etcd` is a critical service, some people
  deploy it directly on a dedicated cluster (without containers)

  (This is illustrated on the first "super complicated" schema)

- In some hosted Kubernetes offerings (e.g. GKE), the control plane is invisible

  (We only "see" a Kubernetes API endpoint)

- In that case, there is no "master node"

*For this reason, it is more accurate to say "control plane" rather than "master".*

.debug[[kube/concepts-k8s.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/concepts-k8s.md)]
---

## Do we need to run Docker at all?

No!

--

- By default, Kubernetes uses the Docker Engine to run containers

- We could also use `rkt` ("Rocket") from CoreOS

- Or leverage other pluggable runtimes through the *Container Runtime Interface*

  (like CRI-O, or containerd)

.debug[[kube/concepts-k8s.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/concepts-k8s.md)]
---

## Do we need to run Docker at all?

Yes!

--

- In this workshop, we run our app on a single node first

- We will need to build images and ship them around

- We can do these things without Docker
  <br/>
  (and get diagnosed with NIH¬π syndrome)

- Docker is still the most stable container engine today
  <br/>
  (but other options are maturing very quickly)

.footnote[¬π[Not Invented Here](https://en.wikipedia.org/wiki/Not_invented_here)]

.debug[[kube/concepts-k8s.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/concepts-k8s.md)]
---

## Do we need to run Docker at all?

- On our development environments, CI pipelines ... :

  *Yes, almost certainly*

- On our production servers:

  *Yes (today)*

  *Probably not (in the future)*

.footnote[More information about CRI [on the Kubernetes blog](https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes)]

.debug[[kube/concepts-k8s.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/concepts-k8s.md)]
---

## Kubernetes resources

- The Kubernetes API defines a lot of objects called *resources*

- These resources are organized by type, or `Kind` (in the API)

- A few common resource types are:

  - node (a machine ‚Äî physical or virtual ‚Äî in our cluster)
  - pod (group of containers running together on a node)
  - service (stable network endpoint to connect to one or multiple containers)
  - namespace (more-or-less isolated group of things)
  - secret (bundle of sensitive data to be passed to a container)
 
  And much more! (We can see the full list by running `kubectl get`)

.debug[[kube/concepts-k8s.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/concepts-k8s.md)]
---

class: pic

![Node, pod, container](images/k8s-arch3-thanks-weave.png)

.debug[[kube/concepts-k8s.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/concepts-k8s.md)]
---

class: pic

![One of the best Kubernetes architecture diagrams available](images/k8s-arch4-thanks-luxas.png)

.debug[[kube/concepts-k8s.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/concepts-k8s.md)]
---

## Credits

- The first diagram is courtesy of Weave Works

  - a *pod* can have multiple containers working together

  - IP addresses are associated with *pods*, not with individual containers

- The second diagram is courtesy of Lucas K√§ldstr√∂m, in [this presentation](https://speakerdeck.com/luxas/kubeadm-cluster-creation-internals-from-self-hosting-to-upgradability-and-ha)

  - it's one of the best Kubernetes architecture diagrams available!

Both diagrams used with permission.

.debug[[kube/concepts-k8s.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/concepts-k8s.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/container-housing.jpg)]

---

name: toc-declarative-vs-imperative
class: title

Declarative vs imperative

.nav[
[Section pr√©c√©dente](#toc-kubernetes-concepts)
|
[Retour table des mati√®res](#toc-chapter-3)
|
[Section suivante](#toc-kubernetes-network-model)
]

.debug[(automatically generated title slide)]

---
# Declarative vs imperative

- Our container orchestrator puts a very strong emphasis on being *declarative*

- Declarative:

  *I would like a cup of tea.*

- Imperative:

  *Boil some water. Pour it in a teapot. Add tea leaves. Steep for a while. Serve in a cup.*

--

- Declarative seems simpler at first ... 

--

- ... As long as you know how to brew tea

.debug[[common/declarative.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/declarative.md)]
---

## Declarative vs imperative

- What declarative would really be:

  *I want a cup of tea, obtained by pouring an infusion¬π of tea leaves in a cup.*

--

  *¬πAn infusion is obtained by letting the object steep a few minutes in hot¬≤ water.*

--

  *¬≤Hot liquid is obtained by pouring it in an appropriate container¬≥ and setting it on a stove.*

--

  *¬≥Ah, finally, containers! Something we know about. Let's get to work, shall we?*

--

.footnote[Did you know there was an [ISO standard](https://en.wikipedia.org/wiki/ISO_3103)
specifying how to brew tea?]

.debug[[common/declarative.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/declarative.md)]
---

## Declarative vs imperative

- Imperative systems:

  - simpler

  - if a task is interrupted, we have to restart from scratch

- Declarative systems:

  - if a task is interrupted (or if we show up to the party half-way through),
    we can figure out what's missing and do only what's necessary

  - we need to be able to *observe* the system

  - ... and compute a "diff" between *what we have* and *what we want*

.debug[[common/declarative.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/declarative.md)]
---
## Declarative vs imperative in Kubernetes

- Virtually everything we create in Kubernetes is created from a *spec*

- Watch for the `spec` fields in the YAML files later!

- The *spec* describes *how we want the thing to be*

- Kubernetes will *reconcile* the current state with the spec
  <br/>(technically, this is done by a number of *controllers*)

- When we want to change some resource, we update the *spec*

- Kubernetes will then *converge* that resource

.debug[[kube/declarative.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/declarative.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/containers-by-the-water.jpg)]

---

name: toc-kubernetes-network-model
class: title

Kubernetes network model

.nav[
[Section pr√©c√©dente](#toc-declarative-vs-imperative)
|
[Retour table des mati√®res](#toc-chapter-3)
|
[Section suivante](#toc-first-contact-with-kubectl)
]

.debug[(automatically generated title slide)]

---
# Kubernetes network model

- TL,DR:

  *Our cluster (nodes and pods) is one big flat IP network.*

--

- In detail:

 - all nodes must be able to reach each other, without NAT

 - all pods must be able to reach each other, without NAT

 - pods and nodes must be able to reach each other, without NAT

 - each pod is aware of its IP address (no NAT)

- Kubernetes doesn't mandate any particular implementation

.debug[[kube/kubenet.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubenet.md)]
---

## Kubernetes network model: the good

- Everything can reach everything

- No address translation

- No port translation

- No new protocol

- Pods cannot move from a node to another and keep their IP address

- IP addresses don't have to be "portable" from a node to another

  (We can use e.g. a subnet per node and use a simple routed topology)

- The specification is simple enough to allow many various implementations

.debug[[kube/kubenet.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubenet.md)]
---

## Kubernetes network model: the less good

- Everything can reach everything

  - if you want security, you need to add network policies

  - the network implementation that you use needs to support them

- There are literally dozens of implementations out there

  (15 are listed in the Kubernetes documentation)

- Pods have level 3 (IP) connectivity, but *services* are level 4

  (Services map to a single UDP or TCP port; no port ranges or arbitrary IP packets)

- `kube-proxy` is on the data path when connecting to a pod or container,
  <br/>and it's not particularly fast (relies on userland proxying or iptables)

.debug[[kube/kubenet.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubenet.md)]
---

## Kubernetes network model: in practice

- The nodes that we are using have been set up to use [Weave](https://github.com/weaveworks/weave)

- We don't endorse Weave in a particular way, it just Works For Us

- Don't worry about the warning about `kube-proxy` performance

- Unless you:

  - routinely saturate 10G network interfaces
  - count packet rates in millions per second
  - run high-traffic VOIP or gaming platforms
  - do weird things that involve millions of simultaneous connections
    <br/>(in which case you're already familiar with kernel tuning)

- If necessary, there are alternatives to `kube-proxy`; e.g.
  [`kube-router`](https://www.kube-router.io)

.debug[[kube/kubenet.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubenet.md)]
---

## The Container Network Interface (CNI)

- The CNI has a well-defined [specification](https://github.com/containernetworking/cni/blob/master/SPEC.md#network-configuration) for network plugins

- When a pod is created, Kubernetes delegates the network setup to CNI plugins

- Typically, a CNI plugin will:

  - allocate an IP address (by calling an IPAM plugin)

  - add a network interface into the pod's network namespace

  - configure the interface as well as required routes etc.

- Using multiple plugins can be done with "meta-plugins" like CNI-Genie or Multus

- Not all CNI plugins are equal

  (e.g. they don't all implement network policies, which are required to isolate pods)

.debug[[kube/kubenet.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubenet.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/distillery-containers.jpg)]

---

name: toc-first-contact-with-kubectl
class: title

First contact with `kubectl`

.nav[
[Section pr√©c√©dente](#toc-kubernetes-network-model)
|
[Retour table des mati√®res](#toc-chapter-3)
|
[Section suivante](#toc-running-our-first-containers-on-kubernetes)
]

.debug[(automatically generated title slide)]

---
# First contact with `kubectl`

- `kubectl` is (almost) the only tool we'll need to talk to Kubernetes

- It is a rich CLI tool around the Kubernetes API

  (Everything you can do with `kubectl`, you can do directly with the API)

- On our machines, there is a `~/.kube/config` file with:

  - the Kubernetes API address

  - the path to our TLS certificates used to authenticate

- You can also use the `--kubeconfig` flag to pass a config file

- Or directly `--server`, `--user`, etc.

- `kubectl` can be pronounced "Cube C T L", "Cube cuttle", "Cube cuddle"...

.debug[[kube/kubectlget.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlget.md)]
---

## `kubectl get`

- Let's look at our `Node` resources with `kubectl get`!

.exercise[

- Look at the composition of our cluster:
  ```bash
  kubectl get node
  ```

- These commands are equivalent:
  ```bash
  kubectl get no
  kubectl get node
  kubectl get nodes
  ```

]

.debug[[kube/kubectlget.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlget.md)]
---

## Obtaining machine-readable output

- `kubectl get` can output JSON, YAML, or be directly formatted

.exercise[

- Give us more info about the nodes:
  ```bash
  kubectl get nodes -o wide
  ```

- Let's have some YAML:
  ```bash
  kubectl get no -o yaml
  ```
  See that `kind: List` at the end? It's the type of our result!

]

.debug[[kube/kubectlget.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlget.md)]
---

## (Ab)using `kubectl` and `jq`

- It's super easy to build custom reports

.exercise[

- Show the capacity of all our nodes as a stream of JSON objects:
  ```bash
    kubectl get nodes -o json | 
            jq ".items[] | {name:.metadata.name} + .status.capacity"
  ```

]

.debug[[kube/kubectlget.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlget.md)]
---

## What's available?

- `kubectl` has pretty good introspection facilities

- We can list all available resource types by running `kubectl get`

- We can view details about a resource with:
  ```bash
  kubectl describe type/name
  kubectl describe type name
  ```

- We can view the definition for a resource type with:
  ```bash
  kubectl explain type
  ```

Each time, `type` can be singular, plural, or abbreviated type name.

.debug[[kube/kubectlget.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlget.md)]
---

## Services

- A *service* is a stable endpoint to connect to "something"

  (In the initial proposal, they were called "portals")

.exercise[

- List the services on our cluster with one of these commands:
  ```bash
  kubectl get services
  kubectl get svc
  ```

]

--

There is already one service on our cluster: the Kubernetes API itself.

.debug[[kube/kubectlget.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlget.md)]
---

## ClusterIP services

- A `ClusterIP` service is internal, available from the cluster only

- This is useful for introspection from within containers

.exercise[

- Try to connect to the API:
  ```bash
  curl -k https://`10.96.0.1`
  ```
  
  - `-k` is used to skip certificate verification

  - Make sure to replace 10.96.0.1 with the CLUSTER-IP shown by `kubectl get svc`

]

--

The error that we see is expected: the Kubernetes API requires authentication.

.debug[[kube/kubectlget.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlget.md)]
---

## Listing running containers

- Containers are manipulated through *pods*

- A pod is a group of containers:

 - running together (on the same node)

 - sharing resources (RAM, CPU; but also network, volumes)

.exercise[

- List pods on our cluster:
  ```bash
  kubectl get pods
  ```

]

--

*These are not the pods you're looking for.* But where are they?!?

.debug[[kube/kubectlget.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlget.md)]
---

## Namespaces

- Namespaces allow us to segregate resources

.exercise[

- List the namespaces on our cluster with one of these commands:
  ```bash
  kubectl get namespaces
  kubectl get namespace
  kubectl get ns
  ```

]

--

*You know what ... This `kube-system` thing looks suspicious.*

.debug[[kube/kubectlget.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlget.md)]
---

## Accessing namespaces

- By default, `kubectl` uses the `default` namespace

- We can switch to a different namespace with the `-n` option

.exercise[

- List the pods in the `kube-system` namespace:
  ```bash
  kubectl -n kube-system get pods
  ```

]

--

*Ding ding ding ding ding!*

The `kube-system` namespace is used for the control plane.

.debug[[kube/kubectlget.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlget.md)]
---

## What are all these control plane pods?

- `etcd` is our etcd server

- `kube-apiserver` is the API server

- `kube-controller-manager` and `kube-scheduler` are other master components

- `kube-dns` is an additional component (not mandatory but super useful, so it's there)

- `kube-proxy` is the (per-node) component managing port mappings and such

- `weave` is the (per-node) component managing the network overlay

- the `READY` column indicates the number of containers in each pod

- the pods with a name ending with `-node1` are the master components
  <br/>
  (they have been specifically "pinned" to the master node)

.debug[[kube/kubectlget.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlget.md)]
---

## What about `kube-public`?

.exercise[

- List the pods in the `kube-public` namespace:
  ```bash
  kubectl -n kube-public get pods
  ```

]

--

- Maybe it doesn't have pods, but what secrets is `kube-public` keeping?

--

.exercise[

- List the secrets in the `kube-public` namespace:
  ```bash
  kubectl -n kube-public get secrets
  ```

]
--

- `kube-public` is created by kubeadm & [used for security bootstrapping](https://kubernetes.io/blog/2017/01/stronger-foundation-for-creating-and-managing-kubernetes-clusters)

.debug[[kube/kubectlget.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlget.md)]
---

## To go even further

- Read the [basic conceptes](https://kubernetes.io/docs/concepts/overview/components/) of Kubernetes.

- Understand [the objects](https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/) of Kubernetes.






.debug[[kube/kubectlget.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlget.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/lots-of-containers.jpg)]

---

name: toc-running-our-first-containers-on-kubernetes
class: title

Running our first containers on Kubernetes

.nav[
[Section pr√©c√©dente](#toc-first-contact-with-kubectl)
|
[Retour table des mati√®res](#toc-chapter-3)
|
[Section suivante](#toc-setting-up-kubernetes)
]

.debug[(automatically generated title slide)]

---
# Running our first containers on Kubernetes

- First things first: we cannot run a container

--

- We are going to run a pod, and in that pod there will be a single container

--

- In that container in the pod, we are going to run a simple `ping` command

- Then we are going to start additional copies of the pod

.debug[[kube/kubectlrun.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlrun.md)]
---

## Starting a simple pod with `kubectl run`

- We need to specify at least a *name* and the image we want to use

.exercise[

- Let's ping `1.1.1.1`, Cloudflare's 
  [public DNS resolver](https://blog.cloudflare.com/announcing-1111/):
  ```bash
  kubectl run pingpong --image alpine ping 1.1.1.1
  ```

]

--

OK, what just happened?

.debug[[kube/kubectlrun.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlrun.md)]
---

## Behind the scenes of `kubectl run`

- Let's look at the resources that were created by `kubectl run`

.exercise[

- List most resource types:
  ```bash
  kubectl get all
  ```

]

--

We should see the following things:
- `deployment.apps/pingpong` (the *deployment* that we just created)
- `replicaset.apps/pingpong-xxxxxxxxxx` (a *replica set* created by the deployment)
- `pod/pingpong-xxxxxxxxxx-yyyyy` (a *pod* created by the replica set)

Note: as of 1.10.1, resource types are displayed in more detail.

.debug[[kube/kubectlrun.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlrun.md)]
---

## What are these different things?

- A *deployment* is a high-level construct

  - allows scaling, rolling updates, rollbacks

  - multiple deployments can be used together to implement a
    [canary deployment](https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#canary-deployments)

  - delegates pods management to *replica sets*

- A *replica set* is a low-level construct

  - makes sure that a given number of identical pods are running

  - allows scaling

  - rarely used directly

- A *replication controller* is the (deprecated) predecessor of a replica set

.debug[[kube/kubectlrun.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlrun.md)]
---

## Our `pingpong` deployment

- `kubectl run` created a *deployment*, `deployment.apps/pingpong`

```
NAME                       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/pingpong   1         1         1            1           10m
```

- That deployment created a *replica set*, `replicaset.apps/pingpong-xxxxxxxxxx`

```
NAME                                  DESIRED   CURRENT   READY     AGE
replicaset.apps/pingpong-7c8bbcd9bc   1         1         1         10m
```

- That replica set created a *pod*, `pod/pingpong-xxxxxxxxxx-yyyyy`

```
NAME                            READY     STATUS    RESTARTS   AGE
pod/pingpong-7c8bbcd9bc-6c9qz   1/1       Running   0          10m
```

- We'll see later how these folks play together for:

  - scaling, high availability, rolling updates

.debug[[kube/kubectlrun.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlrun.md)]
---

## Viewing container output

- Let's use the `kubectl logs` command

- We will pass either a *pod name*, or a *type/name*

  (E.g. if we specify a deployment or replica set, it will get the first pod in it)

- Unless specified otherwise, it will only show logs of the first container in the pod

  (Good thing there's only one in ours!)

.exercise[

- View the result of our `ping` command:
  ```bash
  kubectl logs deploy/pingpong
  ```

]

.debug[[kube/kubectlrun.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlrun.md)]
---

## Streaming logs in real time

- Just like `docker logs`, `kubectl logs` supports convenient options:

  - `-f`/`--follow` to stream logs in real time (√† la `tail -f`)

  - `--tail` to indicate how many lines you want to see (from the end)

  - `--since` to get logs only after a given timestamp

.exercise[

- View the latest logs of our `ping` command:
  ```bash
  kubectl logs deploy/pingpong --tail 1 --follow
  ```

<!--
```wait seq=3```
```keys ^C```
-->

]

.debug[[kube/kubectlrun.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlrun.md)]
---

## Scaling our application

- We can create additional copies of our container (I mean, our pod) with `kubectl scale`

.exercise[

- Scale our `pingpong` deployment:
  ```bash
  kubectl scale deploy/pingpong --replicas 8
  ```

]

Note: what if we tried to scale `replicaset.apps/pingpong-xxxxxxxxxx`?

We could! But the *deployment* would notice it right away, and scale back to the initial level.

.debug[[kube/kubectlrun.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlrun.md)]
---

## Resilience

- The *deployment* `pingpong` watches its *replica set*

- The *replica set* ensures that the right number of *pods* are running

- What happens if pods disappear?

.exercise[

- In a separate window, list pods, and keep watching them:
  ```bash
  kubectl get pods -w
  ```

<!--
```wait Running```
```keys ^C```
-->

- Destroy a pod:
  ```bash
  kubectl delete pod pingpong-xxxxxxxxxx-yyyyy
  ```
]

.debug[[kube/kubectlrun.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlrun.md)]
---

## What if we wanted something different?

- What if we wanted to start a "one-shot" container that *doesn't* get restarted?

- We could use `kubectl run --restart=OnFailure` or `kubectl run --restart=Never`

- These commands would create *jobs* or *pods* instead of *deployments*

- Under the hood, `kubectl run` invokes "generators" to create resource descriptions

- We could also write these resource descriptions ourselves (typically in YAML),
  <br/>and create them on the cluster with `kubectl apply -f` (discussed later)

- With `kubectl run --schedule=...`, we can also create *cronjobs*

.debug[[kube/kubectlrun.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlrun.md)]
---

## Viewing logs of multiple pods

- When we specify a deployment name, only one single pod's logs are shown

- We can view the logs of multiple pods by specifying a *selector*

- A selector is a logic expression using *labels*

- Conveniently, when you `kubectl run somename`, the associated objects have a `run=somename` label

.exercise[

- View the last line of log from all pods with the `run=pingpong` label:
  ```bash
  kubectl logs -l run=pingpong --tail 1
  ```

]

Unfortunately, `--follow` cannot (yet) be used to stream the logs from multiple containers.

.debug[[kube/kubectlrun.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlrun.md)]
---

## Aren't we flooding 1.1.1.1?

- If you're wondering this, good question!

- Don't worry, though:

  *APNIC's research group held the IP addresses 1.1.1.1 and 1.0.0.1. While the addresses were valid, so many people had entered them into various random systems that they were continuously overwhelmed by a flood of garbage traffic. APNIC wanted to study this garbage traffic but any time they'd tried to announce the IPs, the flood would overwhelm any conventional network.*

  (Source: https://blog.cloudflare.com/announcing-1111/)

- It's very unlikely that our concerted pings manage to produce
  even a modest blip at Cloudflare's NOC!

.debug[[kube/kubectlrun.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlrun.md)]
---

## To stop everything

.exercise[

- Stop the deployment:
  ```bash
  kubectl delete deploy/pingpong
  ```
- What is the state of the application ?
  ```bash
  kubectl get all
  ```

]

- To go even further:

  - [Launch a deployment from a YAML file](https://kubernetes.io/docs/tasks/run-application/run-stateless-application-deployment/)

  - [Launch a *cronjob*](https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/)





.debug[[kube/kubectlrun.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlrun.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/plastic-containers.JPG)]

---

name: toc-setting-up-kubernetes
class: title

Setting up Kubernetes

.nav[
[Section pr√©c√©dente](#toc-running-our-first-containers-on-kubernetes)
|
[Retour table des mati√®res](#toc-chapter-4)
|
[Section suivante](#toc-the-kubernetes-dashboard)
]

.debug[(automatically generated title slide)]

---
# Setting up Kubernetes

- How did we set up these Kubernetes clusters that we're using?

--

- We used `kubeadm` on freshly installed VM instances running Ubuntu 16.04 LTS

    1. Install Docker

    2. Install Kubernetes packages

    3. Run `kubeadm init` on the master node

    4. Set up Weave (the overlay network)
       <br/>
       (that step is just one `kubectl apply` command; discussed later)

    5. Run `kubeadm join` on the other nodes (with the token produced by `kubeadm init`)

    6. Copy the configuration file generated by `kubeadm init`

- Check the [prepare VMs README](https://github.com/RyaxTech/kube.training/blob/master/prepare-vms/README.md) for more dedtails

.debug[[kube/setup-k8s.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/setup-k8s.md)]
---

## `kubeadm` drawbacks

- Doesn't set up Docker or any other container engine

- Doesn't set up the overlay network

- Doesn't set up multi-master (no high availability)

--

  (At least ... not yet!)

--

- "It's still twice as many steps as setting up a Swarm cluster üòï" -- J√©r√¥me

.debug[[kube/setup-k8s.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/setup-k8s.md)]
---

## Other deployment options

- If you are on Azure:
  [AKS](https://azure.microsoft.com/services/container-service/)

- If you are on Google Cloud:
  [GKE](https://cloud.google.com/kubernetes-engine/)

- If you are on AWS:
  [EKS](https://aws.amazon.com/eks/)
  or
  [kops](https://github.com/kubernetes/kops)

- On a local machine:
  [minikube](https://kubernetes.io/docs/getting-started-guides/minikube/),
  [kubespawn](https://github.com/kinvolk/kube-spawn),
  [Docker4Mac](https://docs.docker.com/docker-for-mac/kubernetes/)

- If you want something customizable:
  [kubicorn](https://github.com/kubicorn/kubicorn)

  Probably the closest to a multi-cloud/hybrid solution so far, but in development

.debug[[kube/setup-k8s.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/setup-k8s.md)]
---

## Even more deployment options

- If you like Ansible:
  [kubespray](https://github.com/kubernetes-incubator/kubespray)

- If you like Terraform:
  [typhoon](https://github.com/poseidon/typhoon/)

- You can also learn how to install every component manually, with
  the excellent tutorial [Kubernetes The Hard Way](https://github.com/kelseyhightower/kubernetes-the-hard-way)

  *Kubernetes The Hard Way is optimized for learning, which means taking the long route to ensure you understand each task required to bootstrap a Kubernetes cluster.*

- There are also many commercial options available!

- For a longer list, check the Kubernetes documentation:
  <br/>
  it has a great guide to [pick the right solution](https://kubernetes.io/docs/setup/pick-right-solution/) to set up Kubernetes.

.debug[[kube/setup-k8s.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/setup-k8s.md)]
---

## Installation with Kubeadm

- We are not going to perform the installation of Kubernetes.

- The following slides show us how to do it with Kubeadm (you can do it on your own after the training).

.debug[[kube/setup-k8s.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/setup-k8s.md)]
---

## Installation with Kubeadm


.exercise[

- Installation of Docker packages, if they are not installed, on all nodes of the cluster:
  ```bash
    sudo su
    apt-get update
    apt-get install -y apt-transport-https ca-certificates curl software-properties-common
    curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
    add-apt-repository "deb https://download.docker.com/linux/$(. /etc/os-release; echo "$ID") $(lsb_release -cs) stable"
    apt-get update && apt-get install -y docker-ce docker-compose
    exit
    sudo groupadd docker
    sudo usermod -aG docker $USER
  ```
]

.debug[[kube/setup-k8s.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/setup-k8s.md)]
---

## Installation with Kubeadm 


.exercise[

- Installation of Kubernetes packages if they are not installed on every node of the cluster:
  ```bash
    sudo apt-get update && sudo apt-get install -y apt-transport-https curl
    curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
    sudo su
    cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
    deb http://apt.kubernetes.io/ kubernetes-xenial main
    EOF
    exit
    sudo apt-get update
    sudo apt-get install -y kubelet kubeadm kubectl
   ```
]

.debug[[kube/setup-k8s.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/setup-k8s.md)]
---

## Installation with Kubeadm

.exercise[

- Configuration of Kubernetes with Kubeadm on the first node of the cluster:
  ```bash
    sudo kubeadm init 
    sudo mkdir -p $HOME/.kube /home/docker/.kube
    sudo cp /etc/kubernetes/admin.conf $HOME/.kube/config
    sudo cp /etc/kubernetes/admin.conf /home/docker/.kube/config
    sudo chown -R $(id -u) $HOME/.kube
    kubever=$(kubectl version | base64 | tr -d '\n')
    kubectl apply -f https://cloud.weave.works/k8s/net?k8s-version=$kubever
  ```

- Configuration of Kubernetes on the other nodes of the cluster:
- Connect on each compute node and apply the last command returned at the end of the `kubeadm init` command launched on the master node
- Test if the nodes are correctly configured by returning on the master node and executing:
  ```bash
  kubectl get nodes
  ```
]






.debug[[kube/setup-k8s.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/setup-k8s.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/train-of-containers-1.jpg)]

---

name: toc-the-kubernetes-dashboard
class: title

The Kubernetes dashboard

.nav[
[Section pr√©c√©dente](#toc-setting-up-kubernetes)
|
[Retour table des mati√®res](#toc-chapter-4)
|
[Section suivante](#toc-security-implications-of-kubectl-apply)
]

.debug[(automatically generated title slide)]

---
# The Kubernetes dashboard

- Kubernetes resources can also be viewed with a web dashboard

- We are going to deploy that dashboard with *three commands:*

  1) actually *run* the dashboard

  2) bypass SSL for the dashboard

  3) bypass authentication for the dashboard

--

There is an additional step to make the dashboard available from outside (we'll get to that)

--

.footnote[.warning[Yes, this will open our cluster to all kinds of shenanigans. Don't do this at home.]]

.debug[[kube/dashboard.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/dashboard.md)]
---

## 1) Running the dashboard

- We need to create a *deployment* and a *service* for the dashboard

- But also a *secret*, a *service account*, a *role* and a *role binding*

- All these things can be defined in a YAML file and created with `kubectl apply -f`

.exercise[

- Create all the dashboard resources, with the following command:
  ```bash
  kubectl apply -f https://goo.gl/Qamqab
  ```

]

The goo.gl URL expands to:
<br/>
.small[https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml]

.debug[[kube/dashboard.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/dashboard.md)]
---


## 2) Bypassing SSL for the dashboard

- The Kubernetes dashboard uses HTTPS, but we don't have a certificate

- Recent versions of Chrome (63 and later) and Edge will refuse to connect

  (You won't even get the option to ignore a security warning!)

- We could (and should!) get a certificate, e.g. with [Let's Encrypt](https://letsencrypt.org/)

- ... But for convenience, for this workshop, we'll forward HTTP to HTTPS

.warning[Do not do this at home, or even worse, at work!]

.debug[[kube/dashboard.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/dashboard.md)]
---

## Running the SSL unwrapper

- We are going to run [`socat`](http://www.dest-unreach.org/socat/doc/socat.html), telling it to accept TCP connections and relay them over SSL

- Then we will expose that `socat` instance with a `NodePort` service

- For convenience, these steps are neatly encapsulated into another YAML file

.exercise[

- Apply the convenient YAML file, and defeat SSL protection:
  ```bash
  kubectl apply -f https://goo.gl/tA7GLz
  ```

]

The goo.gl URL expands to:
<br/>
.small[.small[https://gist.githubusercontent.com/jpetazzo/c53a28b5b7fdae88bc3c5f0945552c04/raw/da13ef1bdd38cc0e90b7a4074be8d6a0215e1a65/socat.yaml]]

.warning[All our dashboard traffic is now clear-text, including passwords!]

.debug[[kube/dashboard.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/dashboard.md)]
---

## Connecting to the dashboard

.exercise[

- Check which port the dashboard is on:
  ```bash
  kubectl -n kube-system get svc socat
  ```

]

You'll want the `3xxxx` port.


.exercise[

- Connect to http://oneofournodes:3xxxx/

<!-- ```open https://node1:3xxxx/``` -->

]

The dashboard will then ask you which authentication you want to use.

.debug[[kube/dashboard.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/dashboard.md)]
---

## Dashboard authentication

- We have three authentication options at this point:

  - token (associated with a role that has appropriate permissions)

  - kubeconfig (e.g. using the `~/.kube/config` file from `node1`)

  - "skip" (use the dashboard "service account")

- Let's use "skip": we get a bunch of warnings and don't see much

.debug[[kube/dashboard.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/dashboard.md)]
---

## 3) Bypass authentication for the dashboard

- The dashboard documentation [explains how to do this](https://github.com/kubernetes/dashboard/wiki/Access-control#admin-privileges)

- We just need to load another YAML file!

.exercise[

- Grant admin privileges to the dashboard so we can see our resources:
  ```bash
  kubectl apply -f https://goo.gl/CHsLTA
  ```

- Reload the dashboard and enjoy!

]

--

.warning[By the way, we just added a backdoor to our Kubernetes cluster!]

.debug[[kube/dashboard.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/dashboard.md)]
---

## Exposing the dashboard over HTTPS

- We took a shortcut by forwarding HTTP to HTTPS inside the cluster

- Let's expose the dashboard over HTTPS!

- The dashboard is exposed through a `ClusterIP` service (internal traffic only)

- We will change that into a `NodePort` service (accepting outside traffic)

.exercise[

- Edit the service:
  ```bash
  kubectl edit service kubernetes-dashboard
  ```

]

--

`NotFound`?!? Y U NO WORK?!?

.debug[[kube/dashboard.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/dashboard.md)]
---

## Editing the `kubernetes-dashboard` service

- If we look at the [YAML](https://goo.gl/Qamqab) that we loaded before, we'll get a hint

--

- The dashboard was created in the `kube-system` namespace

--

.exercise[

- Edit the service:
  ```bash
  kubectl -n kube-system edit service kubernetes-dashboard
  ```

- Change `ClusterIP` to `NodePort`, save, and exit

- Check the port that was assigned with `kubectl -n kube-system get services`

- Connect to https://oneofournodes:3xxxx/ (yes, https)

]

.debug[[kube/dashboard.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/dashboard.md)]
---

## Running the Kubernetes dashboard securely

- The steps that we just showed you are *for educational purposes only!*

- If you do that on your production cluster, people [can and will abuse it](https://blog.redlock.io/cryptojacking-tesla)

- For an in-depth discussion about securing the dashboard,
  <br/>
  check [this excellent post on Heptio's blog](https://blog.heptio.com/on-securing-the-kubernetes-dashboard-16b09b1b7aca)

.debug[[kube/dashboard.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/dashboard.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/train-of-containers-2.jpg)]

---

name: toc-security-implications-of-kubectl-apply
class: title

Security implications of `kubectl apply`

.nav[
[Section pr√©c√©dente](#toc-the-kubernetes-dashboard)
|
[Retour table des mati√®res](#toc-chapter-4)
|
[Section suivante](#toc-)
]

.debug[(automatically generated title slide)]

---

# Security implications of `kubectl apply`

- When we do `kubectl apply -f <URL>`, we create arbitrary resources

- Resources can be evil; imagine a `deployment` that ...

--

  - starts bitcoin miners on the whole cluster

--

  - hides in a non-default namespace

--

  - bind-mounts our nodes' filesystem

--

  - inserts SSH keys in the root account (on the node)

--

  - encrypts our data and ransoms it

--

  - ‚ò†Ô∏è‚ò†Ô∏è‚ò†Ô∏è

.debug[[kube/dashboard.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/dashboard.md)]
---

## `kubectl apply` is the new `curl | sh`

- `curl | sh` is convenient

- It's safe if you use HTTPS URLs from trusted sources

--

- `kubectl apply -f` is convenient

- It's safe if you use HTTPS URLs from trusted sources

- Example: the official setup instructions for most pod networks

--

- It introduces new failure modes (like if you try to apply yaml from a link that's no longer valid)

.debug[[kube/dashboard.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/dashboard.md)]
---

## To go further


.exercise[
- Relaunch the previous example:
  ```bash
  kubectl run pingpong --image alpine ping 1.1.1.1
  ```
- Observe the deployment of the pod. Can you find the logs of the pod?

- Stop the deployment.

]

You can stop the dashboard or leave it. Since the dashboard is not secure we propose you to stop it.
```bash
  kubectl delete -f https://goo.gl/CHsLTA
  ```



.debug[[kube/dashboard.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/dashboard.md)]</textarea>
    <script src="remark.min.js" type="text/javascript">
    </script>
    <script type="text/javascript">
      var slideshow = remark.create({
        ratio: '16:9',
        highlightSpans: true,
        excludedClasses: ["self-paced"]
      });
    </script>
    
    <!-- 
    These two scripts will be available only when loading the
    content using the pub/sub server. Otherwise, they'll just
    404 and that's OK.
    -->
    <script src="/socket.io/socket.io.js">
    </script>
    <script src="/remote.js">
    </script>

  </body>
</html>
