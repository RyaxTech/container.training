<!DOCTYPE html>
<html>
  <head>
    <title>Kubernetes Introduction, Architecture and Installation </title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <link rel="stylesheet" href="workshop.css">
    <link rel="stylesheet" href="override.css">
  </head>
  <body>
    <!--
    <div style="position: absolute; left: 20%; right: 20%; top: 30%;">
      <h1 style="font-size: 3em;">Loading ...</h1>
      The slides should show up here. If they don't, it might be
      because you are accessing this file directly from your filesystem.
      It needs to be served from a web server. You can try this:
      <pre>
        docker-compose up -d
        open http://localhost:8888/workshop.html # on MacOS
        xdg-open http://localhost:8888/workshop.html # on Linux
      </pre>
      Once the slides are loaded, this notice disappears when you
      go full screen (e.g. by hitting "f").
    </div>
    -->
    <textarea id="source">class: title, self-paced

Kubernetes<br/>Introduction, Architecture and Installation<br/>

.nav[*Self-paced version*]

.debug[
```
 M dock-kube-day1.yml.html
 M dock-kube-day2.yml.html
 M dock-kube-day3.yml.html
 M dock-kube-day4.yml.html
 M intro-fullday.yml.html
 M intro-selfpaced.yml.html
 M kube-day1.yml.html
 D kube/tp_wordpress.md
 D prepare-vms/infra/aws
 D prepare-vms/lib/commands.sh_new
 D prepare-vms/lib/commands.sh_old
 M tp_wordpress.yml
?? .directory
?? common/.directory
?? intro/.directory
?? kube/.directory
?? kube/exo-wordpress/
?? prepare-vms/Dockerfile
?? prepare-vms/README.md
?? prepare-vms/azuredeploy.json
?? prepare-vms/azuredeploy.parameters.json
?? prepare-vms/cards.html
?? prepare-vms/clusters.csv
?? prepare-vms/cncsetup.sh
?? prepare-vms/docker-compose.yml
?? prepare-vms/docker.png
?? prepare-vms/lib/.directory
?? prepare-vms/lib/aws.sh
?? prepare-vms/lib/cli.sh
?? prepare-vms/lib/colors.sh
?? prepare-vms/lib/commands.sh
?? prepare-vms/lib/docker-prompt
?? prepare-vms/lib/find-ubuntu-ami.sh
?? prepare-vms/lib/ips-txt-to-html.py
?? prepare-vms/lib/postprep.py
?? prepare-vms/lib/pssh.sh
?? prepare-vms/lib/wkhtmltopdf
?? prepare-vms/settings/
?? prepare-vms/workshopctl
?? swarm/.directory
?? tp_wordpress.yml.html

```

These slides have been built from commit: 1d2b6fe


[common/title_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/title_fr.md)]
---

class: title, in-person

Kubernetes<br/>Introduction, Architecture and Installation<br/><br/></br>


.debug[[common/title_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/title_fr.md)]
---
## Some infos about the instructor

- Yiannis Georgiou - CTO Ryax Technologies

- PhD Universit√© Grenoble-Alpes - Resource Management and Scheduling on High Performance Computing

- 11 ans at Bull/Atos Technologies - R&D Architect / Software Engineer 

.debug[[logistics.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//logistics.md)]
---

## Logistics

- The training will take place from  9h until 17h30

- There will be a break for lunch from 12h30 to 14h

- Feel free to interrupt for questions at any time

- Especially when you see full screen container pictures!

.debug[[logistics.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//logistics.md)]
---
## Les slides de la formation

<!-- Tout le contenu est disponible dans un depot Github: -->

<!-- ¬† https://github.com/RyaxTech/kube.training -->

- Le contenu est bas√© sur les slides √©crites initiallement par [J√©r√¥me Petazzoni](https://twitter.com/jpetazzo) pour supporter de workshops et tutoriels autour des Conteneurs et de Kubernetes.¬†

- Pour cette formation, les slides ont √©t√© traduites en fran√ßais et adapt√©es.

- Plusieurs mots et termes li√©es aux concepts de Kubernetes n'ont pas √©t√© traduit pour des raisons de simplicit√©

* En dehors de la formation: 
  * ...si vous voulez passer plus de temps pour approfondir sur Kubernetes vous pouvez suivre la [documentation](https://kubernetes.io/docs/) officiel ...
  * ...si vous avez de questions particuliers vous pouvez aussi voir sur [StackOverflow](http://stackoverflow.com/questions/tagged/kubernetes)


.debug[[common/about-slides_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/about-slides_fr.md)]
---

name: toc-chapter-1

## Chapter 1

- [Vue d'ensemble de Docker](#toc-vue-densemble-de-docker)

- [Histoire des conteneurs ... et Docker](#toc-histoire-des-conteneurs--et-docker)

- [Pr√©-requis](#toc-pr-requis)

- [Notre application sample](#toc-notre-application-sample)

- [Identifier les goulots d'√©tranglement](#toc-identifier-les-goulots-dtranglement)

.debug[(auto-generated TOC)]
---
name: toc-chapter-2

## Chapter 2

- [Introduction de Kubernetes](#toc-introduction-de-kubernetes)

- [D√©claratif vs imp√©ratif](#toc-dclaratif-vs-impratif)

- [Mod√®le de r√©seau de Kubernetes](#toc-modle-de-rseau-de-kubernetes)

- [Premier contact avec `kubectl`](#toc-premier-contact-avec-kubectl)

- [G√©rer nos premiers conteneurs sur Kubernetes](#toc-grer-nos-premiers-conteneurs-sur-kubernetes)

.debug[(auto-generated TOC)]
---
name: toc-chapter-3

## Chapter 3

- [Installation de Kubernetes](#toc-installation-de-kubernetes)

- [Le dashboard de Kubernetes](#toc-le-dashboard-de-kubernetes)

- [Les implications de s√©curit√© de `kubectl apply`](#toc-les-implications-de-scurit-de-kubectl-apply)

.debug[(auto-generated TOC)]



.debug[[common/toc.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/toc.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/Container-Ship-Freighter-Navigation-Elbe-Romance-1782991.jpg)]

---

name: toc-vue-densemble-de-docker
class: title

Vue d'ensemble de Docker

.nav[
[Section pr√©c√©dente](#toc-)
|
[Retour table des mati√®res](#toc-chapter-1)
|
[Section suivante](#toc-histoire-des-conteneurs--et-docker)
]

.debug[(automatically generated title slide)]

---
# Vue d'ensemble de Docker

Dans cette partie, nous allons apprendre:

* Pourquoi les conteneurs ('elevator pitch' non-technique)

* Pourquoi les conteneurs ('elevator pitch' technique)

* Comment Docker nous aide √† construire, exp√©dier et ex√©cuter

* L'histoire des conteneurs

Nous n'utiliserons pas Docker ni les conteneurs dans ce chapitre (pour l'instant!).

Ne vous inqui√©tez pas, nous y arriverons assez vite!

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Docker_Overview_fr.md)]
---

## Elevator pitch

### (pour votre manager, votre patron ...)

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Docker_Overview_fr.md)]
---

## OK ... Pourquoi le buzz autour des conteneurs?

* L'industrie du logiciel a chang√©

* Avant:
  * applications monolithiques
  * longs cycles de d√©veloppement
  * environnement unique
  * passage √† l'echelle lente

* √Ä pr√©sent:
  * services d√©coupl√©s
  * am√©liorations rapides et it√©ratives
  * plusieurs environnements
  * passage √† l'echelle rapide

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Docker_Overview_fr.md)]
---

class: pic

## Monolithic VS Microservices

![problem](images/microservices1.png)



.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Docker_Overview_fr.md)]
---

class: pic

## Passage √† l'echelle de microservices

![problem](images/microservice2.png)

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Docker_Overview_fr.md)]
---

## Le d√©ploiement devient tr√®s complexe

* Beaucoup de stacks diff√©rentes:
  * langages
  * frameworks
  * des bases de donn√©es

* Beaucoup de cibles diff√©rentes:
  * environnements de d√©veloppement individuels
  * pr√©-production, QA, stagin ...
  * production: on-prem, cloud, hybride

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Docker_Overview_fr.md)]
---

class: pic

## Le probl√®me de d√©ploiement

![problem](images/shipping-software-problem.png)

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Docker_Overview_fr.md)]
---

class: pic

## La matrice de l'enfer

![matrix](images/shipping-matrix-from-hell.png)

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Docker_Overview_fr.md)]
---

class: pic

## Le parall√®le avec l'industrie de transport

![history](images/shipping-industry-problem.png)

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Docker_Overview_fr.md)]
---

class: pic

## Conteneurs d'exp√©dition intermodaux

![shipping](images/shipping-industry-solution.png)

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Docker_Overview_fr.md)]
---

class: pic

## Un nouvel √©cosyst√®me d'exp√©dition

![shipeco](images/shipping-indsutry-results.png)

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Docker_Overview_fr.md)]
---

class: pic

## Un syst√®me de conteneur d'exp√©dition pour les applications

![shipapp](images/shipping-software-solution.png)

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Docker_Overview_fr.md)]
---

class: pic

## √âliminer la matrice de l'enfer

![elimatrix](images/shipping-matrix-solved.png)

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Docker_Overview_fr.md)]
---

## R√©sultats

* [Dev-to-prod r√©duit de 9 mois √† 15 minutes (ING)](
  https://www.docker.com/sites/default/files/CS_ING_01.25.2015_1.pdf)

* [Temps d'int√©gration continue r√©duit de plus de 60% (BBC)](
  https://www.docker.com/sites/default/files/CS_BBCNews_01.25.2015_1.pdf)

* [D√©ployer 100 fois par jour au lieu d'une fois par semaine (GILT)](
  https://www.docker.com/sites/default/files/CS_Gilt%20Groupe_03.18.2015_0.pdf)

* [Consolidation de l'infrastructure de 70% (MetLife)](
  https://www.docker.com/customers/metlife-transforms-customer-experience-legacy-and-microservices-mashup)

* [Consolidation de l'infrastructure de 60% (Intesa Sanpaolo)](
  https://blog.docker.com/2017/11/intesa-sanpaolo-builds-resilient-foundation-banking-docker-enterprise-edition/)

* [14x densit√© d'application; 60% du centre de donn√©es existant migr√© en 4 mois (GE Appliances)](
  https://www.docker.com/customers/ge-uses-docker-enable-self-service-their-developers)

* etc.

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Docker_Overview_fr.md)]
---

## Elevator pitch

###(pour vos coll√®gues devs et ops)

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Docker_Overview_fr.md)]
---

## √âchapper √† la d√©pendance d'enfer

1. √âcrire les instructions d'installation dans un fichier `INSTALL.txt`

2. En utilisant ce fichier, √©crivez un script `install.sh` qui *vous convient*

3. Transformez ce fichier dans un `Dockerfile`, testez-le sur votre machine

4. Si le Dockerfile se construit sur votre machine, il se construira *n'importe o√π*

5. Nous sommes tranquille en √©vitant l'enfer de la d√©pendance et "ca marche sur ma machine"

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Docker_Overview_fr.md)]
---

## D√©veloppeurs et contributeurs embarqu√©s rapidement

1. √âcrire des fichiers Docker pour vos composants d'application

2. Utilisez des images pr√©-faites depuis le Docker Hub (mysql, redis ...)

3. D√©crivez votre stack avec un fichier Compose

4. Embarquez quelqu'un avec deux commandes:

```bash
git clone ...
docker-compose up
```

Avec cela, vous pouvez cr√©er des environnements de d√©veloppement, d'int√©gration et d'assurance qualit√© en quelques minutes!

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Docker_Overview_fr.md)]
---

class: extra-details

## Mise en ≈ìuvre un CI fiable facilement

1. Construction d'un environnement de test avec un fichier Dockerfile ou Compose

2. Pour chaque s√©rie de tests, placez un nouveau conteneur ou une nouvelle stack

3. Chaque run est maintenant dans un environnement propre

4. Aucune pollution des tests pr√©c√©dents

Beaucoup plus rapide et moins cher que de cr√©er des machines virtuelles √† chaque fois!

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Docker_Overview_fr.md)]
---

class: extra-details

## Utilisation des images de conteneur comme composants de construction

1. Construisez votre application depuis Dockerfiles

2. Stocker les images r√©sultantes dans un registre

3. Garde-les pour toujours (ou aussi longtemps que n√©cessaire)

4. Testez ces images dans QA, CI, int√©gration ...

5. Ex√©cutez les m√™mes images en production

6. Quelque chose ne va pas? Retour √† l'image pr√©c√©dente

7. Inquetude sur l'ancienne r√©gression? L'ancienne image vous couvre!

Les images contiennent toutes les biblioth√®ques, d√©pendances, etc. n√©cessaires pour ex√©cuter l'application.

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Docker_Overview_fr.md)]
---

class: extra-details

## D√©couplage de la "plomberie" de la logique de l'application

1. Ecrivez votre code pour vous connecter aux services nomm√©s ("db", "api" ...)

2. Utilisez Compose pour commencer votre stack

3. Docker va configurer le r√©solveur DNS par conteneur pour ces noms

4. Vous pouvez maintenant redimensionner, ajouter des √©quilibreurs de charge, r√©plication ... sans changer votre code

Note: ceci n'est pas couvert dans cet atelier de niveau intro!

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Docker_Overview_fr.md)]
---

class: extra-details

## Qu'est-ce que Docker a apport√© √† la table?

### Docker avant / apr√®s

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Docker_Overview_fr.md)]
---

class: extra-details

## Formats et API, avant Docker

* Pas de format d'√©change standardis√©.
  <br/>(Non, une archive tar de rootfs *n'est pas* un format!)

* Les conteneurs sont difficiles √† utiliser pour les d√©veloppeurs.
  <br/>(O√π est l'√©quivalent de `docker run debian`?)

* En cons√©quence, ils sont cach√©s aux utilisateurs finaux.

* Aucun composant, API ou outil r√©utilisable.
  <br/>(Au mieux: abstractions de VM, par exemple libvirt.)


Analogie:

* Les conteneurs d'exp√©dition ne sont pas seulement des bo√Ætes en acier.
* Ce sont des bo√Ætes en acier de taille standard, avec les m√™mes crochets et trous.

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Docker_Overview_fr.md)]
---

class: extra-details

## Formats et API, apr√®s Docker

* Normaliser le format du conteneur, car les conteneurs n'√©taient pas portables.

* Rendre les conteneurs faciles √† utiliser pour les d√©veloppeurs.

* Accent sur les composants r√©utilisables, API, √©cosyst√®me d'outils standards.

* Am√©lioration des outils sp√©cifiques ad-hoc, internes.

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Docker_Overview_fr.md)]
---

class: extra-details

## Exp√©dition, avant Docker

* Exp√©dier les paquets: deb, rpm, gem, pot, homebrew ...

* D√©pendance de l'enfer.

* "Fonctionne sur ma machine."

* D√©ploiement de base souvent fait √† partir de z√©ro (debootstrap ...) et peu fiable.

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Docker_Overview_fr.md)]
---

class: extra-details

## Exp√©dition, apr√®s Docker

* Exp√©dier des images de conteneur avec toutes leurs d√©pendances.

* Les images sont plus grandes, mais elles sont divis√©es en couches.

* Envoyez uniquement les couches qui ont chang√©.

* Enregistrer l'utilisation du disque, du r√©seau, de la m√©moire.

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Docker_Overview_fr.md)]
---

class: extra-details

## Exemple

Couches:

* CentOS
* JRE
* Matou
* D√©pendances
* Application JAR
* Configuration

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Docker_Overview_fr.md)]
---

class: extra-details

## Devs vs Ops, avant Docker

* D√©posez une archive tar (ou un hash de commit) avec des instructions.

* Environnement de d√©veloppement tr√®s diff√©rent de la production.

* Les Ops n'ont pas toujours un environnement de dev eux-m√™mes ...

* ... et quand ils le font, cela peut diff√©rer de ceux des d√©veloppeurs.

* Les op√©rations doivent trier les diff√©rences et le faire fonctionner ...

* ... ou rebondir vers les d√©veloppeurs.

* Le code de livraison provoque des frictions et des retards.

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Docker_Overview_fr.md)]
---

class: extra-details

## Devs vs Ops, apr√®s Docker

* D√©poser une image de conteneur ou un fichier de composition.

* Les op√©rations peuvent toujours ex√©cuter cette image de conteneur.

* Les op√©rations peuvent toujours ex√©cuter ce fichier de composition.

* Les op√©rations doivent encore s'adapter √† l'environnement de prod,
   mais au moins ils ont un point de r√©f√©rence.

* Les op√©rations ont des outils permettant d'utiliser la m√™me image
   en dev et prod.

* Les d√©veloppeurs peuvent √™tre autoris√©s √† faire eux-m√™mes des publications
   plus facilement.

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Docker_Overview_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/ShippingContainerSFBay.jpg)]

---

name: toc-histoire-des-conteneurs--et-docker
class: title

Histoire des conteneurs ... et Docker

.nav[
[Section pr√©c√©dente](#toc-vue-densemble-de-docker)
|
[Retour table des mati√®res](#toc-chapter-1)
|
[Section suivante](#toc-pr-requis)
]

.debug[(automatically generated title slide)]

---
# Histoire des conteneurs ... et Docker

.debug[[intro/Docker_History_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Docker_History_fr.md)]
---

## Premi√®res exp√©rimentations

* [IBM VM/370 (1972)](https://en.wikipedia.org/wiki/VM_%28operating_system%29)

* [Linux VServers (2001)](http://www.solucorp.qc.ca/changes.hc?projet=vserver)

* [Solaris Containers (2004)](https://en.wikipedia.org/wiki/Solaris_Containers)

* [FreeBSD jails (1999)](https://www.freebsd.org/cgi/man.cgi?query=jail&sektion=8&manpath=FreeBSD+4.0-RELEASE)

Les conteneurs existent depuis *tr√®s longtemps* en effet.

.debug[[intro/Docker_History_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Docker_History_fr.md)]
---

class: pic

## L'√¢ge du VPS (jusqu'en 2007-2008)

![lightcont](images/containers-as-lightweight-vms.png)

.debug[[intro/Docker_History_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Docker_History_fr.md)]
---

## Containers = moins cher que les VM

* Utilisateurs: fournisseurs d'h√©bergement.

* Audience hautement sp√©cialis√©e avec une forte culture d'op√©rations.

.debug[[intro/Docker_History_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Docker_History_fr.md)]
---

class: pic

## La p√©riode PAAS (2008-2013)

![heroku 2007](images/heroku-first-homepage.png)

.debug[[intro/Docker_History_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Docker_History_fr.md)]
---

## Containers = plus facile que les VM

* Je ne peux pas parler pour Heroku, mais les conteneurs √©taient (l'une des) arme secr√®te de dotCloud

* dotCloud utilisait un PaaS, en utilisant un moteur de conteneur personnalis√©.

* Ce moteur √©tait bas√© sur OpenVZ (et plus tard, LXC) et AUFS.

* Il a commenc√© (vers 2008) comme un seul script Python.

* En 2012, le moteur avait plusieurs (10) composants Python.
  <br/> (et ~ 100 autres micro-services!)

* Fin 2012, dotCloud refactorise ce moteur de conteneur.

* Le nom de code de ce projet est "Docker".

.debug[[intro/Docker_History_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Docker_History_fr.md)]
---

## Premi√®re version publique de Docker

* Mars 2013, PyCon, Santa Clara:
  <br/> "Docker" est pr√©sent√© au public pour la premi√®re fois.

* Il est publi√© avec une licence open source.

* R√©actions et retours tr√®s positifs!

* L'√©quipe dotCloud passe progressivement au d√©veloppement de Docker.

* La m√™me ann√©e, dotCloud change de nom pour Docker.

* En 2014, l'activit√© PaaS est vendue.

.debug[[intro/Docker_History_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Docker_History_fr.md)]
---

## Docker premiers jours (2013-2014)

.debug[[intro/Docker_History_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Docker_History_fr.md)]
---

## Premiers utilisateurs de Docker

* Constructeurs PAAS (Flynn, Dokku, Tsuru, Deis ...)

* Utilisateurs de PAAS (ceux qui sont assez grands pour justifier la construction de leurs propres)

* Plates-formes CI

* d√©veloppeurs, d√©veloppeurs, d√©veloppeurs, d√©veloppeurs

.debug[[intro/Docker_History_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Docker_History_fr.md)]
---

## Boucle de r√©troaction positive

* En 2013, la technologie sous conteneurs (cgroups, namespaces, stockage copy-on-write ...)
  avait beaucoup de taches aveugles.

* La popularit√© croissante de Docker et des conteneurs a r√©v√©l√© de nombreux bugs.

* En cons√©quence, ces bugs ont √©t√© corrig√©s, ce qui a permis d'am√©liorer la stabilit√© des conteneurs.

* Tout h√©bergeur / fournisseur de cloud d√©cent peut ex√©cuter des conteneurs aujourd'hui.

* Les conteneurs deviennent un excellent outil pour d√©ployer / d√©placer des charges de travail de / sur le site / cloud.

.debug[[intro/Docker_History_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Docker_History_fr.md)]
---

## Maturit√© (2015-2016)

.debug[[intro/Docker_History_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Docker_History_fr.md)]
---

## Docker devient un standard de l'industrie

* Docker atteint le jalon symbolique 1.0.

* Les syst√®mes existants tels que Mesos et Cloud Foundry ajoutent un support Docker.

* Normalisation autour de l'OCI (Open Containers Initiative).

* D'autres moteurs de conteneurs sont d√©velopp√©s.

* Cr√©ation de la CNCF (Cloud Native Computing Foundation).

.debug[[intro/Docker_History_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Docker_History_fr.md)]
---

## Docker devient une plateforme

* Le moteur de conteneur initial est maintenant connu sous le nom de "Moteur Docker".

* D'autres outils sont ajout√©s:
  * Docker Compose (anciennement "Fig")
  * Machine Docker
  * Docker Swarm
  * Kitematic
  * Docker Cloud (anciennement "Tutum")
  * Datacenter Docker
  * etc.

* Docker Inc. lance des offres commerciales.

.debug[[intro/Docker_History_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//intro/Docker_History_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/aerial-view-of-containers.jpg)]

---

name: toc-pr-requis
class: title

Pr√©-requis

.nav[
[Section pr√©c√©dente](#toc-histoire-des-conteneurs--et-docker)
|
[Retour table des mati√®res](#toc-chapter-1)
|
[Section suivante](#toc-notre-application-sample)
]

.debug[(automatically generated title slide)]

---
# Pr√©-requis

- Soyez √† l'aise avec la ligne de commande UNIX

  - naviguer dans les r√©pertoires

  - √©diter des fichiers

  - un peu de bash (variables d'environnement, boucles)

- Quelques connaissances de Docker

  - `docker run`,` docker ps`, `docker build`

  - id√©alement, vous savez √©crire un Dockerfile et le construire
    <br/>
    (m√™me si c'est une ligne `FROM` et quelques commandes` RUN`)

- C'est tout √† fait OK si vous n'√™tes pas un expert Docker!

.debug[[common/prereqs_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/prereqs_fr.md)]
---

class: title

*Dites-moi et j'oublie.*
<br/>
*Apprends-moi et je me souviens.*
<br/>
*Implique-moi et j'apprends.*

Misattribu√© √† Benjamin Franklin

[(Probablement inspir√© par le philosophe confuc√©en chinois Xunzi)](https://www.barrypopik.com/index.php/new_york_city/entry/tell_me_and_i_forget_teach_me_and_i_may_remember_involve_me_and_i_will_lear/)

.debug[[common/prereqs_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/prereqs_fr.md)]
---

## Sections pratiques

- Tout l'atelier est pratique

- Nous allons construire, exp√©dier et faire fonctionner des conteneurs!

- Nous allons reproduire toutes les d√©mos

- Toutes les sections pratiques sont clairement identifi√©es, comme le rectangle gris ci-dessous

.exercise[

- C'est ce que tu es cens√© faire!

- Allez dans [kube.training] (https://goo.gl/dekbTb) pour voir ces diapositives

]

.debug[[common/prereqs_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/prereqs_fr.md)]
---

class: in person

## O√π allons-nous faire fonctionner nos conteneurs?

.debug[[common/prereqs_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/prereqs_fr.md)]
---

class: in person, pic

![Vous obtenez un cluster](images/you-get-a-cluster.jpg)

.debug[[common/prereqs_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/prereqs_fr.md)]
---

class: in person

## Vous obtenez un cluster de machines virtuelles cloud

- Chaque personne re√ßoit un cluster priv√© de machines virtuelles cloud (non partag√©es avec d'autres utilisateurs)

- Ils resteront la pendant la dur√©e de la formation

- Vous pouvez automatiquement SSH d'une VM √† l'autre

- Les n≈ìuds ont des alias: `node1`, `node2`...

.debug[[common/prereqs_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/prereqs_fr.md)]
---

class: in person

## Pourquoi ne faisons-nous pas des conteneurs localement?

- L'installation de ce truc peut √™tre difficile sur certaines machines

  (32 bits CPU ou OS ... Ordinateurs portables sans acc√®s administrateur ... etc.)

- Tout ce dont vous avez besoin est un ordinateur (ou m√™me un t√©l√©phone ou une tablette!), Avec:

  - une connexion internet

  - un navigateur Web

  - un client SSH

.debug[[common/prereqs_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/prereqs_fr.md)]
---

class: in person

## Connexion √† l'environnement d'exercices

.exercise[

- Connectez-vous √† la premi√®re machine virtuelle (`node1`) avec votre client SSH

- V√©rifiez que vous pouvez SSH (sans mot de passe) √† `node2`:
  ```bash
  ssh node2
  ```
- Tapez `exit` ou` ^ D` pour revenir √† `asterix-1`

]

Si quelque chose ne va pas, demandez de l'aide!

.debug[[common/prereqs_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/prereqs_fr.md)]
---

## Faire ou refaire des exercises seul?

- Utilisez quelque chose comme
  [Play-With-Docker](http://play-with-docker.com/) ou
  [Play-With-Kubernetes](https://medium.com/@marcosnils/introducing-pwk-play-with-k8s-159fcfeb787b)

  Z√©ro effort d'installation; mais l'environnement est de courte dur√©e et
  pourrait avoir des ressources limit√©es

- Cr√©ez votre propre cluster (VM locales ou cloud)

  Petit effort d'installation; petit co√ªt; environnements flexibles

- Cr√©er un tas de clusters pour vous et vos amis
    ([instructions](https://github.com/RyaxTech/kube.training/tree/master/prepare-vms))

  Effort de configuration plus important; id√©al pour la formation de groupe

.debug[[common/prereqs_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/prereqs_fr.md)]
---

## Nous allons (surtout) interagir avec node1 seulement

*Ces remarques ne s'appliquent que lorsque vous utilisez plusieurs n≈ìuds, bien s√ªr.*

- Sauf instructions, **toutes les commandes doivent √™tre ex√©cut√©es √† partir de la premi√®re VM, `node1`**

- Nous allons seulement v√©rifier / copier le code sur `node1`

- Pendant les op√©rations normales, nous n'avons pas besoin d'acc√©der aux autres n≈ìuds

- Si nous devions r√©soudre les probl√®mes, nous utiliserions une combinaison de:

  - SSH (pour acc√©der aux logs du syst√®me, √©tat du d√©mon ...)
  
  - API Docker (pour v√©rifier l'√©tat des conteneurs en cours d'ex√©cution et du moteur de conteneur)


.debug[[common/prereqs_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/prereqs_fr.md)]
---
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/blue-containers.jpg)]

---

name: toc-notre-application-sample
class: title

Notre application sample

.nav[
[Section pr√©c√©dente](#toc-pr-requis)
|
[Retour table des mati√®res](#toc-chapter-1)
|
[Section suivante](#toc-identifier-les-goulots-dtranglement)
]

.debug[(automatically generated title slide)]

---

# Notre application sample

- Nous allons cloner le d√©p√¥t GitHub sur notre 1er noeud

- Le r√©f√©rentiel contient √©galement des scripts et des outils que nous utiliserons √† travers l'atelier

.exercise[

- Cloner le d√©p√¥t sur `node1`:
  ```bash
  git clone https://github.com/RyaxTech/kube.training 
  ```

]

(Vous pouvez √©galement faire un fork du d√©p√¥t sur GitHub et cloner votre fork si vous pr√©f√©rez cela.)

.debug[[common/sampleapp_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/sampleapp_fr.md)]
---

## T√©l√©chargement et ex√©cution de l'application

Commen√ßons la procedure, car le t√©l√©chargement prendra un peu de temps ...

.exercise[

- Allez dans le r√©pertoire `dockercoins`, dans le repo clone:
  ```bash
  cd ~/kube.training/dockercoins
  ```

- Utilisez Compose pour g√©n√©rer et ex√©cuter tous les conteneurs:
  ```bash
  docker-compose up
  ``` 
]

Compose dit √† Docker de construire toutes les images du conteneur (en tirant
les images de base correspondantes), puis d√©marre tous les conteneurs,
et affiche les logs agr√©g√©s.

.debug[[common/sampleapp_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/sampleapp_fr.md)]
---

## Plus de d√©tails sur notre exemple d'application

- Visitez le lien GitHub avec tous les mat√©riaux de cet atelier:
  <br/> https://github.com/RyaxTech/kube.training

- L'application est dans le sous-r√©pertoire [dockercoins](https://github.com/RyaxTech/kube.training/tree/master/dockercoins)

- Regardons la disposition g√©n√©rale du code source:

  il y a un fichier Compose [docker-compose.yml](https://github.com/RyaxTech/kube.training/blob/master/dockercoins/docker-compose.yml) ...

  ... et 4 autres services, chacun dans son propre r√©pertoire:

  - `rng` = service web g√©n√©rant des octets al√©atoires
  - `hasher` = hachage informatique du service Web des donn√©es POSTed
  - `worker` = processus de fond utilisant `rng` et `hasher`
  - `webui` = interface web pour suivre les progr√®s

.debug[[common/sampleapp_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/sampleapp_fr.md)]
---

## D√©couverte de service dans le terrain de conteneurs

- Nous ne codons pas les adresses IP dans le code

- Nous ne codons pas le nom de domaine complet dans le code, soit

- Nous nous connectons simplement √† un nom de service, et la magie des conteneurs fait le reste

  (Et par magie des conteneurs, nous entendons "un serveur DNS dynamique et embarqu√©")

.debug[[common/sampleapp_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/sampleapp_fr.md)]
---

## Exemple dans `worker/worker.py`

```python
redis = Redis("`redis`")


def get_random_bytes():
    r = requests.get ("http://`rng`/32")
    return r.content


def hash_bytes(data):
    r = requests.post("http://`hasher`/",
                      data = data,
                      headers = {"Content-Type": "application/octet-stream"})
```

(Code source complet disponible [ici](
https://github.com/RyaxTech/kube.training/blob/8279a3bce9398f7c1a53bdd95187c53eda4e6435/dockercoins/worker/worker.py#L17
))

.debug[[common/sampleapp_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/sampleapp_fr.md)]
---

class: extra-details

## Liens, d√©nomination et d√©couverte de service

- Les conteneurs peuvent avoir des alias r√©seau (r√©solvables via DNS)

- Compose le fichier version 2+ rend chaque conteneur accessible via son nom de service

- Compose la version 1 du fichier a n√©cessit√© des sections "liens"

- Les alias r√©seau sont automatiquement "namespaced"

  - vous pouvez avoir plusieurs applications d√©clarant et utilisant un service nomm√© `database`

  - les conteneurs dans l'application bleue vont r√©soudre `database` √† l'adresse IP de la base de donn√©es bleue

  - les conteneurs dans l'application verte vont r√©soudre `base de donn√©es` √† l'adresse IP de la base de donn√©es verte

.debug[[common/sampleapp_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/sampleapp_fr.md)]
---

## Qu'est-ce qu'elle fait cette application?

--

- C'est un mineur de DockerCoin! .emoji[üí∞üê≥üì¶üö¢]

--

- Non, vous ne pouvez pas acheter de caf√© avec DockerCoins

--

- Comment fonctionne DockerCoins:

  - `worker` demande √† `rng` de g√©n√©rer quelques octets al√©atoires

  - `worker` nourrit ces octets en `hasher`

  - et r√©p√®te pour toujours!

  - chaque seconde, `worker` met √† jour `redis` pour indiquer combien de boucles ont √©t√© faites

  - `webui` interroge `redis`, et calcule et expose la "vitesse de hachage" dans votre navigateur

.debug[[common/sampleapp_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/sampleapp_fr.md)]
---

## Notre application au travail

- Sur le c√¥t√© gauche, la "rainbow strip" montre les noms des conteneurs

- Sur le c√¥t√© droit, nous voyons la sortie de nos conteneurs

- Nous pouvons voir le service `worker` faire des requ√™tes √† `rng` et `hasher`

- Pour `rng` et `hasher`, nous voyons les logs d'acc√®s HTTP

.debug[[common/sampleapp_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/sampleapp_fr.md)]
---

## Connexion √† l'interface Web

- Le conteneur `webui` expose un dashboard Web; Voyons voir

.exercise[

- Avec un navigateur Web, connectez-vous √† `node1` sur le port 8080

- Rappel: les alias `nodeX` ne sont valables que sur les n≈ìuds eux-m√™mes

- Dans votre navigateur, vous devez entrer l'adresse IP de votre noeud

]

Une zone de dessin devrait appara√Ætre, et apr√®s quelques secondes, un graphique bleu appara√Ætra.

.debug[[common/sampleapp_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/sampleapp_fr.md)]
---

class: extra-details

## Pourquoi la vitesse semble-t-elle irr√©guli√®re?

- On *dirait que* la vitesse est d'environ 4 hashes/seconde

- Ou plus pr√©cis√©ment: 4 hashes/seconde, avec des creux r√©guliers jusqu'√† z√©ro

- Pourquoi?

--

class: extra-details

- L'application a en fait une vitesse constante et constante: 3,33 hachages / seconde
  <br/>
  (ce qui correspond √† 1 hash toutes les 0.3 secondes)

- Oui et?

.debug[[common/sampleapp_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/sampleapp_fr.md)]
---

class: extra-details

## La raison pour laquelle ce graphique n'est *pas g√©nial*

- Le "worker" ne met pas √† jour le compteur apr√®s chaque boucle, mais jusqu'√† une fois par seconde

- La vitesse est calcul√©e par le navigateur, v√©rifiant le compteur environ une fois par seconde

- Entre deux mises √† jour cons√©cutives, le compteur augmentera de 4 ou de 0

- La vitesse per√ßue sera donc 4 - 4 - 4 - 0 - 4 - 4 - 0 etc.

.debug[[common/sampleapp_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/sampleapp_fr.md)]
---

## Arr√™t de l'application

- Si nous interrompons Compose (avec `^C`), il demandera poliment au Docker Engine d'arr√™ter l'application

- Le moteur Docker enverra un signal `TERM` aux conteneurs

- Si les conteneurs ne sortent pas en temps voulu, le moteur envoie un signal "KILL"

.exercise[

- Arr√™tez l'application en tapant `^C`

]

-

Certains conteneurs sortent imm√©diatement, d'autres prennent plus de temps.

Les conteneurs qui ne g√®rent pas `SIGTERM` finissent par √™tre d√©truits apr√®s un d√©lai de 10s. Si nous sommes tr√®s impatients, nous pouvons frapper `^C` une seconde fois!

.debug[[common/sampleapp_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/sampleapp_fr.md)]
---
## Red√©marrage en arri√®re-plan

- De nombreux flags et commandes de Compose sont mod√©lis√©s apr√®s ceux de `docker`

.exercise[

- D√©marrez l'application en arri√®re-plan avec l'option `-d`:
  ```bash
  docker-compose up -d
  ```

- V√©rifiez que notre application fonctionne avec la commande `ps`:
  ```bash
  docker-compose ps
  ```

]

`docker-composer ps` montre √©galement les ports expos√©s par l'application.

.debug[[common/composescale_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/composescale_fr.md)]
---

class: extra-details

## Affichage des logs

- La commande `docker-compose logs` fonctionne comme les `docker logs`

.exercise[

- Voir tous les logs depuis la cr√©ation du conteneur et quitter lorsque vous avez termin√©:
  ```bash
  docker-compose logs
  ```

- Diffuser les logs de conteneur, en commen√ßant par les 10 derni√®res lignes pour chaque conteneur:
  ```bash
  docker-compose logs --tail 10 --follow
  ```

]

Astuce: utilisez `^ S` et` ^ Q` pour mettre en pause / reprendre la sortie du log.

.debug[[common/composescale_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/composescale_fr.md)]
---

## Passage √† l'√©chelle de l'application

- Notre objectif est de faire monter ce graphique de performance (sans changer de ligne de code!)

-

- Avant d'essayer de faire √©voluer l'application, nous d√©terminerons si nous avons besoin de plus de ressources

  (CPU, RAM ...)

- Pour cela, nous utiliserons de bons vieux outils UNIX sur notre noeud Docker

.debug[[common/composescale_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/composescale_fr.md)]
---

## Regard sur l'utilisation des ressources

- Regardons le CPU, la m√©moire et l'utilisation des I/O

.exercise[

- Ex√©cutez `top` pour voir l'utilisation du processeur et de la m√©moire (vous devriez voir les cycles d'inactivit√©)


- Ex√©cutez `vmstat 1` pour voir l'utilisation des I/O (si/so/bi/bo)
  <br/>(les 4 nombres devraient √™tre presque z√©ro, sauf `bo` pour l'enregistrement)

]

Nous avons des ressources disponibles.

- Pourquoi?
- Comment pouvons-nous les utiliser?

.debug[[common/composescale_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/composescale_fr.md)]
---

## Passage √† l'√©chelle des workers sur un seul noeud

- Docker Compose prend en charge le passage √† l'√©chelle
- Nous allons scaler  `worker` et voir ce qui se passe!

.exercise[

- D√©marrer un autre conteneur `worker`:
  ```bash
  docker-compose scale worker=2
  ```

- Regardez le graphique de performance (il devrait montrer une am√©lioration de x2)

- Regardez les logs agr√©g√©s de nos conteneurs (`worker_2` devrait appara√Ætre)

- Regardez l'impact sur la charge du processeur avec, par exemple, haut (il devrait √™tre n√©gligeable)

]

.debug[[common/composescale_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/composescale_fr.md)]
---

## Ajouter plus de workers

- Super, ajoutons plus de workers, alors!

.exercise[

- Commencez huit autres conteneurs ¬´worker¬ª:
  ```bash
  docker-compose scale worker=10
  ```

- Regardez le graphique des performances: montre-t-il une am√©lioration x10?

- Regardez les logs agr√©g√©s de nos conteneurs

- Regardez l'impact sur la charge du processeur et l'utilisation de la m√©moire

]

.debug[[common/composescale_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/composescale_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/chinook-helicopter-container.jpg)]

---

name: toc-identifier-les-goulots-dtranglement
class: title

Identifier les goulots d'√©tranglement

.nav[
[Section pr√©c√©dente](#toc-notre-application-sample)
|
[Retour table des mati√®res](#toc-chapter-1)
|
[Section suivante](#toc-introduction-de-kubernetes)
]

.debug[(automatically generated title slide)]

---

# Identifier les goulots d'√©tranglement

- Vous devriez avoir vu une amelioration de vitesse 3x (pas 10x)

- L'ajout de workers n'a pas entra√Æn√© d'am√©lioration lin√©aire

- *Quelque chose d'autre* nous ralentit

-

- ... Mais quoi?

-

- Le code n'a pas d'instrumentation

- Utilisons l'analyse de performance HTTP de pointe!
  <br/> (c'est-√†-dire de bons vieux outils comme `ab`,` https` ...)

.debug[[common/composescale_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/composescale_fr.md)]
---

## Acc√®s aux services internes

- `rng` et` hasher` sont expos√©s sur les ports 8001 et 8002

- Ceci est d√©clar√© dans le fichier Compose:

  ```yaml
    ...
    rng:
      build: rng
      ports:
      - "8001:80"

    hasher:
      build: hasher
      ports:
      - "8002:80"
    ...
  ```

.debug[[common/composescale_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/composescale_fr.md)]
---

## Mesure de la latence en charge

Nous allons utiliser `httping`.

.exercise[

- V√©rifiez la latence de `rng`:
  ```bash
  httping -c 3 localhost:8001
  ```

- V√©rifiez la latence de `hasher`:
  ```bash
  httping -c 3 localhost:8002
  ```

]

`rng` a une latence beaucoup plus √©lev√©e que` hasher`.

.debug[[common/composescale_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/composescale_fr.md)]
---

## Tirons des conclusions simplistes

- Le goulot d'√©tranglement semble √™tre `rng`

- *Que se passe-t-il si* nous n'avons pas assez d'entropie et que nous ne pouvons pas g√©n√©rer suffisamment de nombres al√©atoires?

- Nous devons √©tendre le service `rng` sur plusieurs machines!

Note: ceci est une fiction! Nous avons assez d'entropie. Mais nous avons besoin d'un pr√©texte pour l'√©tendre.

(En fait, le code de `rng` utilise `/dev/urandom`, qui ne manque jamais d'entropie ...
<br/>
... et est [aussi bon que `/dev/random`](http://www.slideshare.net/PacSecJP/filippo-plain-simple-reality-of-entropy).)

.debug[[common/composescale_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/composescale_fr.md)]
---
## Nettoyage

- Avant de continuer, enlevons ces conteneurs

.exercise[

- Dites √† Compose de tout enlever:
  ```bash
  docker-compose down
  ```

]

.debug[[common/composedown_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/composedown_fr.md)]
---

## Pour aller plus loin avec Docker Compose

- Lisez l'[overview](https://docs.docker.com/compose/overview/) de la documentation officielle.

- Les exemples sont aussi int√©r√©ssant, comme le [Quickstart: Compose and WordPress](https://docs.docker.com/compose/wordpress/).
.debug[[common/composedown_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/composedown_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/container-cranes.jpg)]

---

name: toc-introduction-de-kubernetes
class: title

Introduction de Kubernetes

.nav[
[Section pr√©c√©dente](#toc-identifier-les-goulots-dtranglement)
|
[Retour table des mati√®res](#toc-chapter-2)
|
[Section suivante](#toc-dclaratif-vs-impratif)
]

.debug[(automatically generated title slide)]

---
# Introduction de Kubernetes

- Kubernetes est un syst√®me de gestion de conteneur

- Il ex√©cute et g√®re les applications conteneuris√©es sur un cluster

--

- Qu'est-ce que cela signifie vraiment?

.debug[[kube/concepts-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/concepts-k8s_fr.md)]
---

## Introduction de Kubernetes

--

- C'est un logiciel pour *d√©ployer et g√©rer* des applications conteneuris√©es tout en offrant la *meilleure utilisation possible* de la plate-forme de calcul.

--

- Il fait *l'abstraction* de l'infrastructure sous-jacente en *simplifiant le d√©veloppement* d'applications et la *gestion du mat√©riel*.

.debug[[kube/concepts-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/concepts-k8s_fr.md)]
---

## Benefices de Kubernetes

--

- Simplification du *d√©ploiement* d'applications.

--

- Am√©lioration de *l'utilisation* du syst√®me mat√©riel.

--

- *Passage √† l'√©chelle* automatique de l'application.

--

- *Simplification du d√©veloppement* d'applications

--

- *Tol√©rance aux pannes*, *haute disponibilit√©* et *auto-gu√©rison*

.debug[[kube/concepts-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/concepts-k8s_fr.md)]
---

## Choses de base que nous pouvons demander √† Kubernetes

--

- D√©marrer 5 conteneurs en utilisant l'image `atseashop/api:v1.3`

--

- Placer un 'load balancer' interne devant ces conteneurs

--

- D√©marrer 10 conteneurs en utilisant l'image `atseashop/webfront:v1.3`

--

- Placez un 'load balancer' public devant ces conteneurs

--

- C'est No√´l, beaucoup de trafic, augmenter notre cluster et ajouter des conteneurs

--

- Nouvelle version! Remplacer mes conteneurs avec la nouvelle image `atseashop/webfront:v1.4`

--

- Continuez √† traiter les demandes pendant la mise √† niveau; mettre √† jour mes conteneurs un √† la fois

.debug[[kube/concepts-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/concepts-k8s_fr.md)]
---

## D'autres choses que Kubernetes peut faire pour nous

- Autoscaling de base

- D√©ploiement bleu / vert, d√©ploiement canari

- Les services √† long terme, mais aussi les travaux par batch (ponctuels)

- Overcommit notre cluster et *expulser* les jobs de basse priorit√©

- Ex√©cuter des services avec des donn√©es * stateful * (bases de donn√©es, etc.)

- Contr√¥le d'acc√®s √† grain fin d√©finissant * ce qui * peut √™tre fait par * qui * sur * quelles * ressources

- Int√©gration de services tiers (* catalogue de services *)

- Automatiser des t√¢ches complexes (* op√©rateurs *)

.debug[[kube/concepts-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/concepts-k8s_fr.md)]
---

## Kubernetes architecture

.debug[[kube/concepts-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/concepts-k8s_fr.md)]
---

class: pic

![haha seulement blague](images/k8s-arch1.png)

.debug[[kube/concepts-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/concepts-k8s_fr.md)]
---

## Kubernetes architecture

- Ha ha ha ha

- OK, j'essayais de vous faire peur, c'est beaucoup plus simple que √ßa ‚ù§Ô∏è

.debug[[kube/concepts-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/concepts-k8s_fr.md)]
---

class: pic

![Celui-l√† ressemble plus √† la r√©alit√©](images/kube_archi_simple.png)

.debug[[kube/concepts-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/concepts-k8s_fr.md)]
---

class: pic

![Celui-l√† ressemble plus √† la r√©alit√©](images/k8s-arch2.png)

.debug[[kube/concepts-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/concepts-k8s_fr.md)]
---

## Cr√©dits

- Le premier sch√©ma est un cluster Kubernetes avec stockage soutenu par iSCSI "multi-path"

  (Source: [Yongbok Kim](https://www.yongbok.net/blog/))

- Le second est repris par le livre de Marko Luksa "Kubernetes in Action"

- Le troisieme est une repr√©sentation simplifi√©e d'un cluster Kubernetes

  (Source: [Imesh Gunaratne](https://medium.com/containermind/a-reference-architecture-for-deploying-wso2-middleware-on-kubernetes-d4dee7601e8e))

.debug[[kube/concepts-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/concepts-k8s_fr.md)]
---

## A savoir...

- Comment prononce-t-on kubernetes?
    - Mot venant du grecque Œ∫œÖŒ≤ŒµœÅŒΩŒÆœÑŒ∑œÇ, prononc√© "kivernitis"
    - En anglais : "coubernetis"
    - En fran√ßais : "cubernetesse" ou "cubernette"

- On peut abbr√©ger Kubernetes en k8s

- Kubernetes viens avec de l'autocompl√©tion √† int√©grer dans votre bash :

`source <(kubectl completion bash)`

√áa compl√®te les commandes mais aussi les noms des objets!

Commande d√©j√† √©ff√©ctu√©e dans vos VMs.


.debug[[kube/concepts-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/concepts-k8s_fr.md)]
---

## Architecture de Kubernetes: les noeuds

- Les n≈ìuds ex√©cutant nos conteneurs ex√©cutent une collection de services:

  - **Container Runtime** (typiquement Docker pour le deploiment de conteneurs)

  - **Kubelet** (l'agent de noeud, gere les conteneurs, communique avec l'API)

  - **Kube-proxy** (un composant r√©seau qui fait du "load-balancing")

- Les n≈ìuds √©taient autrefois appel√©s "minions"

  (Vous pourriez voir ce mot dans les anciens articles ou dans la documentation)

.debug[[kube/concepts-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/concepts-k8s_fr.md)]
---

## Architecture de Kubernetes: le "Control Plane"

- La logique de Kubernetes (ses "cerveaux") est une collection de services:

  - **API server**: notre point d'entr√©e pour tout!

  - **Scheduler**: affecte des n≈ìuds aux composants
  
  - **Controller Manager**: fonctions de niveau de cluster
  
  - **Etcd**: un key/value store fiable, la "base de donn√©es" de Kubernetes

- Ensemble, ces services forment le "Control Plane" de notre cluster

- Le "Control Plane" est aussi appel√© le "master"

.debug[[kube/concepts-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/concepts-k8s_fr.md)]
---

## Ex√©cution du "Control Plane" sur des noeuds sp√©ciaux

- Il est courant de r√©server un noeud d√©di√© au plan de contr√¥le

  (Sauf pour les clusters de d√©veloppement √† noeud unique, comme lorsque vous utilisez minikube)

- Ce noeud s'appelle alors un "master"

  (Oui, c'est ambigu: le "master" est-il un n≈ìud, ou tout le plan de contr√¥le?)

- Les applications normales sont limit√©es √† l'ex√©cution sur ce noeud

  (En utilisant un m√©canisme appel√© ["taints"](https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/))

- Lorsque la haute disponibilit√© est requise, chaque service du plan de contr√¥le doit √™tre r√©silient

- Le plan de contr√¥le est ensuite r√©pliqu√© sur plusieurs n≈ìuds

  (Ceci est parfois appel√© une configuration "multi-master")

.debug[[kube/concepts-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/concepts-k8s_fr.md)]
---

## Ex√©cution du "Control Plane" en dehors des conteneurs

- Les services du "Control Plane" peuvent fonctionner dans ou hors des conteneurs

- Par exemple: puisque `etcd` est un service critique, certaines personnes
  d√©ployer directement sur un cluster d√©di√© (sans conteneurs)

  (Ceci est illustr√© sur le premier sch√©ma "super compliqu√©")

- Dans certaines offres Kubernetes h√©berg√©es (par exemple, GKE), le plan de contr√¥le est invisible

  (Nous ne "voyons" qu'un point de terminaison API Kubernetes)

- Dans ce cas, il n'y a pas de "noeud master"

* Pour cette raison, il est plus juste de dire "Control Plane" plut√¥t que "master". *

.debug[[kube/concepts-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/concepts-k8s_fr.md)]
---

## Doit-on lancer Docker du tout?

Non!

--

- Par d√©faut, Kubernetes utilise Docker Engine pour ex√©cuter les conteneurs

- Nous pourrions aussi utiliser `rkt` ("Rocket") de CoreOS/Redhat

- Ou tirer parti d'autres runtimes connectables via l'interface *Container Runtime*

  (comme CRI-O, ou containerd)

.debug[[kube/concepts-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/concepts-k8s_fr.md)]
---

## Doit-on lancer Docker du tout?

Oui!

--

- Dans cette formation, nous ex√©cutons notre application sur un seul noeud en premier

- Nous aurons besoin de construire des images et de les exp√©dier

- Nous pouvons faire ces choses sans Docker mais

- Docker est toujours le moteur de conteneur le plus stable aujourd'hui
  <br/>
  (mais d'autres options m√ªrissent tr√®s rapidement)

.debug[[kube/concepts-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/concepts-k8s_fr.md)]
---

## Doit-on lancer Docker du tout?

- Sur nos environnements de d√©veloppement, les pipelines CI ...:

  *Oui, presque certainement*

- Sur nos serveurs de production:

  *Oui (aujourd'hui)*

  *Probablement pas (dans le futur)*

.footnote[Plus d'informations sur CRI [sur le blog Kubernetes](https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes)]

.debug[[kube/concepts-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/concepts-k8s_fr.md)]
---

## Ressources de Kubernetes

- L'API Kubernetes d√©finit beaucoup d'objets appel√©s *ressources*

- Ces ressources sont organis√©es par type, ou `Kind` (dans l'API)

- Quelques types de ressources communs sont:

  - noeud (une machine - physique ou virtuelle - dans notre cluster)
  - pod (groupe de conteneurs fonctionnant ensemble sur un noeud)
  - service (point de terminaison r√©seau stable pour se connecter √† un ou plusieurs conteneurs)
  - namespace (groupe de choses plus ou moins isol√©)
  - secret (paquet de donn√©es sensibles √† transmettre √† un conteneur)
 
  Et beaucoup plus! (Nous pouvons voir la liste compl√®te en ex√©cutant `kubectl get`)

.debug[[kube/concepts-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/concepts-k8s_fr.md)]
---

class: pic

![N≈ìud, pod, conteneur](images/k8s-arch3-thanks-weave.png)

.debug[[kube/concepts-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/concepts-k8s_fr.md)]
---

class: pic

![Un des meilleurs diagrammes d'architecture Kubernetes disponibles](images/k8s-arch4-thanks-luxas.png)

.debug[[kube/concepts-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/concepts-k8s_fr.md)]
---

## Cr√©dits

- Le premier diagramme est une gracieuset√© de Weave Works

  - un *pod* peut avoir plusieurs conteneurs travaillant ensemble

  - Les adresses IP sont associ√©es √† *pods*, pas avec des conteneurs individuels

- Le deuxi√®me diagramme est une gracieuset√© de Lucas K√§ldstr√∂m, dans [cette pr√©sentation](https://speakerdeck.com/luxas/kubeadm-cluster-creation-internals-from-self-hosting-to-upgradability-and-ha)

  - c'est l'un des meilleurs diagrammes d'architecture Kubernetes disponibles!


.debug[[kube/concepts-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/concepts-k8s_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/container-housing.jpg)]

---

name: toc-dclaratif-vs-impratif
class: title

D√©claratif vs imp√©ratif

.nav[
[Section pr√©c√©dente](#toc-introduction-de-kubernetes)
|
[Retour table des mati√®res](#toc-chapter-2)
|
[Section suivante](#toc-modle-de-rseau-de-kubernetes)
]

.debug[(automatically generated title slide)]

---
# D√©claratif vs imp√©ratif

- Notre orchestrateur de conteneurs met un accent tr√®s fort sur √™tre *d√©claratif*

- D√©claratif:

  *Je voudrais une tasse de th√©.*

- Imp√©ratif:

  *Faire bouillir de l'eau. Versez-le dans une th√©i√®re. Ajouter les feuilles de th√©. Raide pendant un moment. Servir dans une tasse. *

--

- D√©claratif semble plus simple au premier abord ...

--

- ... Tant qu'on sait comment faire du th√©

.debug[[common/declarative_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/declarative_fr.md)]
---

## D√©claratif vs imp√©ratif

- Quel d√©claratif serait vraiment:

  *Je veux une tasse de th√©, obtenue en versant une infusion¬π de feuilles de th√© dans une tasse.*

--

  *¬πUne perfusion est obtenue en laissant l'objet tremper quelques minutes dans de l'eau chaude¬≤.*

--

  *¬≤Le liquide chaud est obtenu en le versant dans un conteneur appropri√©¬≥ et en le pla√ßant sur une cuisini√®re.*

--

  *¬≥Ah, enfin, les conteneurs! Quelque chose que nous connaissons. Mettons-nous au travail, allons-nous?*

--

.footnote[Saviez-vous qu'il y avait une [norme ISO](https://en.wikipedia.org/wiki/ISO_3103) pr√©cisant comment brasser le th√©?]

.debug[[common/declarative_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/declarative_fr.md)]
---

## D√©claratif vs imp√©ratif

- Syst√®mes imp√©ratifs:

  - plus simple

  - Si une t√¢che est interrompue, nous devons recommencer √† z√©ro

- Syst√®mes d√©claratifs:

  - si une t√¢che est interrompue (ou si nous nous montrons √† mi-chemin),
    nous pouvons comprendre ce qui manque et ne faisons que ce qui est n√©cessaire

  - nous devons pouvoir *observer* le syst√®me

  - ... et calcule un "diff" entre *ce que nous avons* et *ce que nous voulons*

.debug[[common/declarative_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/declarative_fr.md)]
---
## D√©claratif vs imp√©ratif dans Kubernetes

- Pratiquement tout ce que nous cr√©ons dans Kubernetes est cr√©√© √† partir d'un *spec*

- Surveillez les champs `spec` dans les fichiers YAML plus tard!

- Le *spec* d√©crit *comment nous voulons que la chose soit*

- Kubernetes va *r√©concilier* l'√©tat actuel avec les sp√©cifications
  <br/> (techniquement, cela est fait par un certain nombre de *contr√¥leurs*)

- Quand on veut changer de ressource, on met √† jour la *spec*

- Kubernetes va ensuite *converger* cette ressource

.debug[[kube/declarative_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/declarative_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/containers-by-the-water.jpg)]

---

name: toc-modle-de-rseau-de-kubernetes
class: title

Mod√®le de r√©seau de Kubernetes

.nav[
[Section pr√©c√©dente](#toc-dclaratif-vs-impratif)
|
[Retour table des mati√®res](#toc-chapter-2)
|
[Section suivante](#toc-premier-contact-avec-kubectl)
]

.debug[(automatically generated title slide)]

---
# Mod√®le de r√©seau de Kubernetes

- TL,DR:

  *Notre cluster (n≈ìuds et pods) est un grand r√©seau IP plat.*

--

- En d√©tail:

 - tous les n≈ìuds doivent pouvoir se rejoindre, sans NAT

 - Tous les pods doivent pouvoir se rejoindre, sans NAT

 - Les pods et les n≈ìuds doivent pouvoir se rejoindre, sans NAT

 - chaque pod est au courant de son adresse IP (pas de NAT)

- Kubernetes n'impose aucune impl√©mentation particuli√®re

.debug[[kube/kubenet_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubenet_fr.md)]
---

## Mod√®le de r√©seau de Kubernetes: le bon

- Tout peut atteindre tout

- Pas de traduction d'adresse

- Pas de traduction de port

- Pas de nouveau protocole

- Les pods ne peuvent pas se d√©placer d'un noeud √† l'autre et conserver leur adresse IP

- Les adresses IP ne doivent pas √™tre "portables" d'un n≈ìud √† l'autre

  (Nous pouvons utiliser par exemple un sous-r√©seau par n≈ìud et utiliser une topologie rout√©e simple)

- La sp√©cification est assez simple pour permettre de nombreuses impl√©mentations diff√©rentes

.debug[[kube/kubenet_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubenet_fr.md)]
---

## Mod√®le de r√©seau Kubernetes: le moins bon

- Tout peut atteindre tout

  - Si vous voulez de la s√©curit√©, vous devez ajouter des r√®gles de r√©seau

  - l'impl√©mentation r√©seau dont vous avez besoin doit les prendre en charge

- Il y a litt√©ralement des dizaines d'impl√©mentations l√†-bas

  (15 sont r√©pertori√©s dans la documentation de Kubernetes)

- Les pods ont une connectivit√© de niveau 3 (IP), mais les *services* sont de niveau 4

  (Services mappent vers un seul port UDP ou TCP, aucune plage de ports ou paquets IP arbitraires)

- `kube-proxy` est sur le chemin de donn√©es lors de la connexion √† un pod ou un conteneur,
  <br/> et ce n'est pas particuli√®rement rapide (repose sur un proxy utilisateur ou sur iptables)

.debug[[kube/kubenet_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubenet_fr.md)]
---

## Mod√®le de r√©seau Kubernetes: en pratique

- Les n≈ìuds que nous utilisons ont √©t√© configur√©s pour utiliser [Weave](https://github.com/weaveworks/weave)

- Nous n'approuvons pas Weave d'une mani√®re particuli√®re, √ßa marche juste pour nous

- Ne vous inqui√©tez pas de l'avertissement concernant les performances de `kube-proxy`

- √Ä moins que vous:

  - saturer r√©guli√®rement les interfaces r√©seau 10G
  - compte les taux de paquets en millions par seconde
  - lancer des plateformes VOIP ou de jeu √† fort trafic
  - faire des choses bizarres qui impliquent des millions de connexions simultan√©es
    <br/> (auquel cas vous connaissez d√©j√† le r√©glage du noyau)

- Si n√©cessaire, il existe des alternatives √† `kube-proxy`, par exemple [kube-router](https://www.kube-router.io)

.debug[[kube/kubenet_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubenet_fr.md)]
---

## Le "Container Network Interface" (CNI)

- Le CNI a une [sp√©cification](https://github.com/containernetworking/cni/blob/master/SPEC.md#network-configuration) bien d√©finie pour les plugins r√©seau

- Lorsqu'un pod est cr√©√©, Kubernetes d√©l√®gue la configuration r√©seau aux plugins CNI

- Typiquement, un plugin CNI va:

  - allouer une adresse IP (en appelant un plugin IPAM)

  - ajouter une interface r√©seau dans l'espace de noms r√©seau du pod

  - configurer l'interface ainsi que les routes requises, etc.

- Utiliser plusieurs plugins peut √™tre fait avec des "m√©ta-plugins" comme CNI-Genie ou Multus

- Tous les plugins CNI ne sont pas √©gaux

  (par exemple, ils n'impl√©mentent pas tous les strat√©gies de r√©seau, qui sont n√©cessaires pour isoler les pods)

.debug[[kube/kubenet_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubenet_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/distillery-containers.jpg)]

---

name: toc-premier-contact-avec-kubectl
class: title

Premier contact avec `kubectl`

.nav[
[Section pr√©c√©dente](#toc-modle-de-rseau-de-kubernetes)
|
[Retour table des mati√®res](#toc-chapter-2)
|
[Section suivante](#toc-grer-nos-premiers-conteneurs-sur-kubernetes)
]

.debug[(automatically generated title slide)]

---
# Premier contact avec `kubectl`

- `kubectl` est (presque) le seul outil dont nous aurons besoin pour parler √† Kubernetes

- C'est un outil CLI riche autour de l'API Kubernetes

  (Tout ce que vous pouvez faire avec `kubectl`, vous pouvez le faire directement avec l'API)

- Sur nos machines, il y a un fichier `~/.kube/config` avec:

  - l'adresse de l'API Kubernetes

  - le chemin vers nos certificats TLS utilis√©s pour l'authentification

- Vous pouvez √©galement utiliser l'indicateur `--kubeconfig` pour passer un fichier de configuration

- Ou directement `--server`,` --user`, etc.

- ¬´kubectl¬ª peut √™tre prononc√© ¬´Cube C T L¬ª, ¬´Cube cuttle¬ª ...

.debug[[kube/kubectlget_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlget_fr.md)]
---

## `kubectl get`

- Regardons nos ressources `Node` avec` kubectl get`!

.exercise[

- Regardez la composition de notre cluster:
  ```bash
  kubectl get node
  ```

- Ces commandes sont √©quivalentes:
  ```bash
  kubectl get no
  kubectl get node
  kubectl get nodes
  ```

]

.debug[[kube/kubectlget_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlget_fr.md)]
---

## Obtention d'une sortie lisible par machine

- `kubectl get` peut sortir JSON, YAML, ou √™tre directement format√©

.exercise[

- Donnez-nous plus d'informations sur les n≈ìuds:
  ```bash
  kubectl get nodes -o wide
  ```

- Ayons du YAML:
  ```bash
  kubectl get no -o yaml
  ```
  Vous voyez ce `kind: List` √† la fin? C'est le type de notre r√©sultat!

]

.debug[[kube/kubectlget_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlget_fr.md)]
---

## Utilisation de `kubectl` et `jq`

- C'est super facile de construire des rapports personnalis√©s

.exercise[

- Afficher la capacit√© de tous nos n≈ìuds en tant que flux d'objets JSON:
  ```bash
    kubectl get nodes -o json |
            jq ".items [] | {nom: .metadata.name} + .status.capacity"
  ```

]

.debug[[kube/kubectlget_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlget_fr.md)]
---

## Qu'est-ce qui est disponible?

- `kubectl` a de tr√®s bonnes installations d'introspection

- Nous pouvons lister tous les types de ressources disponibles en ex√©cutant `kubectl get`

- Nous pouvons voir les d√©tails d'une ressource avec:
  ```bash
  kubectl describe type/name
  kubectl describe type name
  ```

- Nous pouvons voir la d√©finition d'un type de ressource avec:
  ```bash
  kubectl explain type
  ```

Chaque fois, `type` peut √™tre un nom de type singulier, pluriel ou abr√©g√©.

.debug[[kube/kubectlget_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlget_fr.md)]
---

## Services

- Un *service* est un point de terminaison stable pour se connecter √† "quelque chose"

  (Dans la proposition initiale, ils √©taient appel√©s "portals")

.exercise[

- Listez les services sur notre cluster avec l'une de ces commandes:
  ```bash
  kubectl get services
  kubectl get svc
  ```

]

--

Il y a d√©j√† un service sur notre cluster: l'API Kubernetes elle-m√™me.

.debug[[kube/kubectlget_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlget_fr.md)]
---

## Services ClusterIP

- Un service `ClusterIP` est interne, disponible uniquement √† partir du cluster

- Ceci est utile pour l'introspection √† l'int√©rieur des conteneurs

.exercise[

- Essayez de vous connecter √† l'API:
  ```bash
  curl -k https://`10.96.0.1`
  ```
  
  - `-k` est utilis√© pour ignorer la v√©rification du certificat

  - Assurez-vous de remplacer 10.96.0.1 avec le CLUSTER-IP montr√© par `kubectl get svc`

]

--

L'erreur que nous voyons est attendue: l'API Kubernetes n√©cessite une authentification.

.debug[[kube/kubectlget_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlget_fr.md)]
---

## Liste des conteneurs en cours d'ex√©cution

- Les conteneurs sont manipul√©s via *pods*

- Un pod est un groupe de conteneurs:

 - fonctionnant ensemble (sur le m√™me noeud)

 - partage des ressources (RAM, CPU, mais aussi r√©seau, volumes)

.exercise[

- Liste des pods sur notre cluster:
  ```bash
  kubectl get pods
  ```

]

--

*Ce ne sont pas les pods que vous cherchez.* Mais o√π sont-ils?!?

.debug[[kube/kubectlget_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlget_fr.md)]
---

## Namespaces

- Les namespaces nous permettent de s√©parer les ressources

.exercise[

- Liste les espaces de noms sur notre cluster avec l'une de ces commandes:
  ```bash
  kubectl get namespaces
  kubectl get namespace
  kubectl get ns
  ```

]

--

*Vous savez quoi ... Ce truc de "kube-system" semble suspect. *

.debug[[kube/kubectlget_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlget_fr.md)]
---

## Acc√®s aux namespaces

- Par d√©faut, `kubectl` utilise le namespace `default`

- Nous pouvons passer √† un namespace diff√©rent avec l'option `-n`

.exercise[

- Lister les pods dans l'espace de noms `kube-system`:
  ```bash
  kubectl -n kube-system get pods
  ```

]

--

* Ding ding ding ding! *

L'espace de noms `kube-system` est utilis√© pour le "Control Plane".

.debug[[kube/kubectlget_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlget_fr.md)]
---

## Quels sont tous ces pod du Control Plane?

- `etcd` est notre serveur etcd

- `kube-apiserver` est le serveur API

- `kube-controller-manager` et `kube-scheduler` sont d'autres composants principaux

- `kube-dns` (ou `coredns`) est un composant suppl√©mentaire (pas obligatoire mais super utile, donc c'est l√†)

- `kube-proxy` est le composant (par noeud) g√©rant les mappages de ports et tel

- `weave` est le composant (par noeud) g√©rant l'overlay du r√©seau

- la colonne `READY` indique le nombre de conteneurs dans chaque pod

- les pods dont le nom se termine par `-node1` sont les composants master
  <br/>
  (Ils ont √©t√© sp√©cifiquement "√©pingl√©" au n≈ìud master)

.debug[[kube/kubectlget_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlget_fr.md)]
---

## Qu'en est-il de `kube-public`?

.exercise[

- Lister les pods dans le namespace `kube-public`:
  ```bash
  kubectl -n kube-public get pods
  ```

]

--

- Peut-√™tre n'a-t-il pas de pods, mais quels sont les secrets du `kube-public`?

--

.exercise[

- Liste les secrets dans le namespace `kube-public`:
  ```bash
  kubectl -n kube-public get secrets
  ```

]
--

- `kube-public` est cr√©√© par kubeadm & [utilis√© pour le bootstrap de s√©curit√©](https://kubernetes.io/blog/2017/01/stronger-foundation-for-creating-and-managing-kubernetes-clusters)


.debug[[kube/kubectlget_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlget_fr.md)]
---

## Pour aller plus loin

- Les [composants de bases](https://kubernetes.io/docs/concepts/overview/components/) de Kubernetes.

- Comprendre [les objets](https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/) de Kubernetes.




.debug[[kube/kubectlget_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlget_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/lots-of-containers.jpg)]

---

name: toc-grer-nos-premiers-conteneurs-sur-kubernetes
class: title

G√©rer nos premiers conteneurs sur Kubernetes

.nav[
[Section pr√©c√©dente](#toc-premier-contact-avec-kubectl)
|
[Retour table des mati√®res](#toc-chapter-2)
|
[Section suivante](#toc-installation-de-kubernetes)
]

.debug[(automatically generated title slide)]

---
# G√©rer nos premiers conteneurs sur Kubernetes

- Premi√®res choses d'abord: nous ne pouvons pas executer un conteneur

--

- Nous allons lancer un pod, et dans ce pod il y aura un seul conteneur

--

- Dans ce conteneur dans le pod, nous allons lancer une simple commande `ping`

- Ensuite, nous allons commencer des copies suppl√©mentaires du pod

.debug[[kube/kubectlrun_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlrun_fr.md)]
---

## D√©marrer un pod simple avec `kubectl run`

- Nous devons sp√©cifier au moins un *name* et l'image que nous voulons utiliser

.exercise[

- Nous allong pinger `1.1.1.1`, Cloudflare's
  [DNS public resolver](https://blog.cloudflare.com/announcing-1111/):
  ```bash
  kubectl run pingpong --image alpine ping 1.1.1.1
  ```

]

--

OK, qu'est-ce qui vient de se passer?

.debug[[kube/kubectlrun_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlrun_fr.md)]
---

## Dans les coulisses de `kubectl run`

- Regardons les ressources qui ont √©t√© cr√©√©es par `kubectl run`

.exercise[

- Listez la plupart des types de ressources:
  ```bash
  kubectl get all
  ```

]

--

Nous devrions voir les choses suivantes:
- `deployment.apps/pingpong` (le *deployment* que nous venons de cr√©er)
- `replicaset.apps/pingpong-xxxxxxxxxx` (un *replica set* cr√©√© par le deployment)
- `pod/pingpong-xxxxxxxxxx-yyyyy` (un *pod* cr√©√© par le replica set)

Note: √† partir de 1.10.1, les types de ressources sont affich√©s plus en d√©tail.

.debug[[kube/kubectlrun_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlrun_fr.md)]
---

## Quelles sont ces diff√©rentes choses?

- Un *deployment* est une construction de haut niveau

  - permet le "scaling", "rolling updates", "rollbacks"

  - plusieurs d√©ploiements peuvent √™tre utilis√©s ensemble pour mettre en ≈ìuvre
    [canary deployment](https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#canary-deployments)

  - d√©l√®gue la gestion des pods aux *replica sets*

- Un *replica set* est une construction de bas niveau

  - s'assure qu'un nombre donn√© de pods identiques fonctionnent

  - permet le "scaling"

  - rarement utilis√© directement

.debug[[kube/kubectlrun_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlrun_fr.md)]
---

## Notre deployment `pingpong`

- `kubectl run` a cr√©√© un *deployment*,`deployment.apps/pingpong`

```
NAME                       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/pingpong   1         1         1            1           10m
```

- Ce deployment a cr√©√© un *replica set*, `replicaset.apps/pingpong-xxxxxxxxxx`

```
NAME                                  DESIRED   CURRENT   READY     AGE
replicaset.apps/pingpong-7c8bbcd9bc   1         1         1         10m
```

- Ce replica set a cr√©√© un *pod*, `pod/pingpong-xxxxxxxxxx-yyyyy`

```
NAME                            READY     STATUS    RESTARTS   AGE
pod/pingpong-7c8bbcd9bc-6c9qz   1/1       Running   0          10m
```

- Nous verrons plus tard comment ces trucs jouent ensemble pour:

    - scaling, high availability, rolling updates
.debug[[kube/kubectlrun_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlrun_fr.md)]
---

## Affichage de la sortie du conteneur

- Utilisons la commande `kubectl logs`

- Nous allons passer soit un *pod name*, soit un *type/name*

  (Par exemple, si nous sp√©cifions un deployment ou un replica set, il recevra le premier pod)

- Sauf indication contraire, il ne montrera que les logs du premier conteneur dans le pod

  (Bonne chose, il n'y en a qu'une dans la n√¥tre!)

.exercise[

- Voir le r√©sultat de notre commande `ping`:
  ```bash
  kubectl logs deploy/pingpong
  ```

]

.debug[[kube/kubectlrun_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlrun_fr.md)]
---

## Flux de logs en temps r√©el

- Tout comme `docker logs`, `kubectl logs` supporte les options pratiques:

  - `-f`/`--follow` pour streamer les logs en temps r√©el (√† la `tail -f`)

  - `--tail` pour indiquer combien de lignes vous voulez voir (√† partir de la fin)

  - `--since` pour obtenir les logs seulement apr√®s un timestamp donn√©

.exercise[

- Voir les derniers logs de notre commande `ping`:
  ```bash
  kubectl logs deploy/pingpong --tail 1 --follow
  ```

]

.debug[[kube/kubectlrun_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlrun_fr.md)]
---

## "Scaling" de notre application

- Nous pouvons cr√©er des copies suppl√©mentaires de notre conteneur (je veux dire, notre pod) avec ¬´kubectl scale¬ª

.exercise[

- Scale notre deployment `pingpong`:
  ```bash
  kubectl scale deploy/pingpong --replicas 8
  ```

]

Note: Et si nous essayions de scaler `replicaset.apps/pingpong-xxxxxxxxxx`?

Nous pourrions! Mais le *deployment* le remarquerait tout de suite et reviendrait au niveau initial.

.debug[[kube/kubectlrun_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlrun_fr.md)]
---

## R√©silience

- Le *deployment* `pingpong` regarde son *replica set*

- Le *replica set* assure que le bon nombre de *pods* sont en cours d'ex√©cution

- Que se passe-t-il si les pods disparaissent?

.exercise[

- Dans une fen√™tre s√©par√©e, affichez les pods et continuez √† les regarder:
  ```bash
  kubectl get pods -w
  ```

- D√©truire un pod:
  ```bash
  kubectl delete pod pingpong-xxxxxxxxxx-yyyyy
  ```
]

.debug[[kube/kubectlrun_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlrun_fr.md)]
---

## Et si on voulait quelque chose de diff√©rent?

- Et si nous voulions d√©marrer un conteneur "one-shot" qui ne red√©marre pas?

- Nous pourrions utiliser `kubectl run --restart=OnFailure` ou `kubectl run --restart=Never`

- Ces commandes cr√©eraient *jobs* ou *pods* au lieu de *deployments*

- Sous le tapis, `kubectl run` invoque des "generators" pour cr√©er des descriptions de ressources

- Nous pourrions aussi √©crire nous-m√™mes ces descriptions de ressources (typiquement en YAML), et les cr√©er sur le cluster avec `kubectl apply -f` (discut√© plus tard)

- Avec `kubectl run --schedule=...`, nous pouvons aussi cr√©er des *cronjobs*

.debug[[kube/kubectlrun_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlrun_fr.md)]
---

## Affichage des logs de plusieurs pods

- Lorsque nous sp√©cifions un nom de deployment, seuls les logs d'un seul pod sont affich√©s

- Nous pouvons voir les logs de plusieurs pods en sp√©cifiant un *selector*

- Un selector est une expression logique utilisant des *labels*

- Commod√©ment, quand vous "kubectl run somename", les objets associ√©s ont un label `run = somename`

.exercise[

- Regardez la derni√®re ligne du log de tous les pods avec le label `run = pingpong`:
  ```bash
  kubectl logs -l run=pingpong --tail 1
  ```

]

Malheureusement, `--follow` ne peut pas (encore) √™tre utilis√© pour diffuser les logs de plusieurs conteneurs.

.debug[[kube/kubectlrun_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlrun_fr.md)]
---

## Ne nous inondons pas 1.1.1.1?

- Si vous vous posez cette question, bonne question!

- Ne vous inqui√©tez pas, cependant:

  *Le groupe de recherche de l'APNIC d√©tenait les adresses IP 1.1.1.1 et 1.0.0.1. Alors que les adresses √©taient valides, tant de gens les avaient entr√©s dans divers syst√®mes al√©atoires qu'ils √©taient continuellement submerg√©s par un flot de d√©chets. L'APNIC voulait √©tudier ce trafic de d√©chets, mais chaque fois qu'il avait essay√© d'annoncer les IP, l'inondation submergerait tout r√©seau conventionnel.*

  (Source: https://blog.cloudflare.com/announcing-1111/)

- Il est tr√®s peu probable que nos pings concert√©s parviennent √† produire
  m√™me un modeste coup au Cloudflare!

.debug[[kube/kubectlrun_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlrun_fr.md)]
---

## Tout couper

.exercise[

- Arr√©tez le deployment:
  ```bash
  kubectl delete deploy/pingpong
  ```
- Quel est l'√©tat de l'application ?
  ```bash
  kubectl get all
  ```

]

- Pour aller plus loin:

  - [Lancer un deployement dupuis un fichier YAML](https://kubernetes.io/docs/tasks/run-application/run-stateless-application-deployment/)

  - [Lancer un *cronjob*](https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/)


.debug[[kube/kubectlrun_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlrun_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/plastic-containers.JPG)]

---

name: toc-installation-de-kubernetes
class: title

Installation de Kubernetes

.nav[
[Section pr√©c√©dente](#toc-grer-nos-premiers-conteneurs-sur-kubernetes)
|
[Retour table des mati√®res](#toc-chapter-3)
|
[Section suivante](#toc-le-dashboard-de-kubernetes)
]

.debug[(automatically generated title slide)]

---
# Installation de Kubernetes

- Comment avons-nous mis en place ces clusters Kubernetes que nous utilisons?

--

- Nous avons utilis√© `kubeadm` sur des instances VM fra√Æchement install√©es ex√©cutant Ubuntu 16.04 LTS

    1. Installez Docker

    2. Installer les paquets Kubernetes

    3. Ex√©cutez `kubeadm init` sur le noeud ma√Ætre

    4. Configurer Weave (le r√©seau de superposition)
       <br/>
       (cette √©tape est juste une commande "kubectl apply", discut√©e plus tard)

    5. Ex√©cutez `kubeadm join` sur les autres noeuds (avec le token produit par` kubeadm init`)

    6. Copiez le fichier de configuration g√©n√©r√© par `kubeadm init`

- V√©rifiez le [prepare VMs README](https://github.com/RyaxTech/kube.training/blob/master/prepare-vms/README.md) pour plus de d√©tails

.debug[[kube/setup-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/setup-k8s_fr.md)]
---

## "kubeadm" inconv√©nients

- Ne configure pas Docker ou tout autre moteur de conteneur

- Ne configure pas le r√©seau d'overlay

- Ne configure pas multi-master (pas de haute disponibilit√©)

--

  (Au moins pas encore!)

--

- "Il reste deux fois plus d'√©tapes que la mise en place d'un cluster Docker Swarm" 

.debug[[kube/setup-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/setup-k8s_fr.md)]
---

## Autres options de d√©ploiement

- Si vous √™tes sur Azure:
  [AKS](https://azure.microsoft.com/services/container-service/)

- Si vous √™tes sur Google Cloud:
  [GKE](https://cloud.google.com/kubernetes-engine/)

- Si vous √™tes sur AWS:
  [EKS](https://aws.amazon.com/eks/)
  ou
  [kops](https://github.com/kubernetes/kops)

- Sur une machine locale:
  [minikube](https://kubernetes.io/docs/getting-started-guides/minikube/),
  [kubespawn](https://github.com/kinvolk/kube-spawn),
  [Docker4Mac](https://docs.docker.com/docker-for-mac/kubernetes/)

- Si vous voulez quelque chose de personnalisable:
  [kubicorn](https://github.com/kubicorn/kubicorn)

  Probablement le plus proche d'une solution multi-cloud / hybride jusqu'√† pr√©sent, mais en d√©veloppement

.debug[[kube/setup-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/setup-k8s_fr.md)]
---

## Encore plus d'options de d√©ploiement

- Si vous aimez Ansible:
  [kubespray](https://github.com/kubernetes-incubator/kubespray)

- Si vous aimez Terraform:
  [typhon](https://github.com/poseidon/typhoon/)

- Vous pouvez √©galement apprendre √† installer chaque composant manuellement, avec
  l'excellent tutoriel [Kubernetes The Hard Way](https://github.com/kelseyhightower/kubernetes-the-hard-way)

  *Kubernetes The Hard Way est optimis√© pour l'apprentissage, ce qui signifie prendre le long chemin pour s'assurer que vous comprenez chaque t√¢che requise pour d√©marrer un cluster Kubernetes.*

- Il y a aussi beaucoup d'options commerciales disponibles!

- Pour une liste plus longue, consultez la documentation de Kubernetes:
  <br/>
  il a un excellent guide pour [choisir la bonne solution](https://kubernetes.io/docs/setup/pick-right-solution/) pour configurer Kubernetes.

.debug[[kube/setup-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/setup-k8s_fr.md)]
---

## Installation avec Kubeadm

- Nous n'allons pas faire l'installation de Kubernetes.

- Les slides suivantes vous permettes d'avoir les bases avec Kubeadm.

.debug[[kube/setup-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/setup-k8s_fr.md)]
---

## Installation avec Kubeadm


.exercise[

- Installation de paquets Docker si ils ne sont pas install√© sur chaque noeud du cluster:
  ```bash
    sudo su
    apt-get update
    apt-get install -y apt-transport-https ca-certificates curl software-properties-common
    curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
    add-apt-repository "deb https://download.docker.com/linux/$(. /etc/os-release; echo "$ID") $(lsb_release -cs) stable"
    apt-get update && apt-get install -y docker-ce docker-compose
    exit
    sudo groupadd docker
    sudo usermod -aG docker $USER
  ```
]

.debug[[kube/setup-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/setup-k8s_fr.md)]
---

## Installation avec Kubeadm suite


.exercise[

- Installation de paquets Kubernetes si ils ne sont pas install√© sur chaque noeud du cluster:
  ```bash
    sudo apt-get update && sudo apt-get install -y apt-transport-https curl
    curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
    sudo su
    cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
    deb http://apt.kubernetes.io/ kubernetes-xenial main
    EOF
    exit
    sudo apt-get update
    sudo apt-get install -y kubelet kubeadm kubectl
   ```
]

.debug[[kube/setup-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/setup-k8s_fr.md)]
---

## Installation avec Kubeadm suite

.exercise[

- Configuration de Kubernetes avec Kubeadm au premier noeud du cluster:
  ```bash
    sudo kubeadm init 
    sudo mkdir -p $HOME/.kube /home/docker/.kube
    sudo cp /etc/kubernetes/admin.conf $HOME/.kube/config
    sudo cp /etc/kubernetes/admin.conf /home/docker/.kube/config
    sudo chown -R $(id -u) $HOME/.kube
    kubever=$(kubectl version | base64 | tr -d '\n')
    kubectl apply -f https://cloud.weave.works/k8s/net?k8s-version=$kubever
  ```

- Configuration de Kubernetes avec Kubeadm aux autres noeuds du cluster:
- Appliquez la commande retourn√© par `kubeadm init` sur le master
- Testez si les noeuds sont bien configur√©s avec:
  ```bash
  kubectl get nodes
  ```
]





.debug[[kube/setup-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/setup-k8s_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/train-of-containers-1.jpg)]

---

name: toc-le-dashboard-de-kubernetes
class: title

Le dashboard de Kubernetes

.nav[
[Section pr√©c√©dente](#toc-installation-de-kubernetes)
|
[Retour table des mati√®res](#toc-chapter-3)
|
[Section suivante](#toc-les-implications-de-scurit-de-kubectl-apply)
]

.debug[(automatically generated title slide)]

---
# Le dashboard de Kubernetes

- Les ressources de Kubernetes peuvent √©galement √™tre visualis√©es avec un dashboard web

- Nous allons d√©ployer ce dashboard avec *trois commandes:*

  1) plutot *ex√©cuter* le dashboard

  2) contourner SSL pour le dashboard

  3) Ignorer l'authentification pour le dashboard

--

Il y a une √©tape suppl√©mentaire pour rendre le dashboard disponible de l'ext√©rieur (nous y reviendrons)

--

.footnote[.warning[Oui, cela ouvrira notre cluster √† toutes sortes de manigances. Ne fais pas √ßa √† la maison.]]

.debug[[kube/dashboard_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/dashboard_fr.md)]
---

## 1) Ex√©cution du dashboard

- Nous devons cr√©er un * d√©ploiement * et un * service * pour le dashboard

- Mais aussi un * secret *, un * compte de service *, un * r√¥le * et un * r√¥le contraignant *

- Toutes ces choses peuvent √™tre d√©finies dans un fichier YAML et cr√©√©es avec `kubectl apply -f`

.exercise[

- Cr√©er toutes les ressources du dashboard, avec la commande suivante:
  ```bash
  kubectl apply -f https://goo.gl/Qamqab
  ```

]

L'URL de goo.gl se d√©veloppe pour:
<br/>
.small[https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml]

.debug[[kube/dashboard_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/dashboard_fr.md)]
---


## 2) Contournement SSL pour le dashboard

- Le dashboard Kubernetes utilise HTTPS, mais nous n'avons pas de certificat

- Les versions r√©centes de Chrome (63 et versions ult√©rieures) et Edge refusent de se connecter

  (Vous n'aurez m√™me pas l'option d'ignorer un avertissement de s√©curit√©!)

- Nous pourrions (et devrions!) Obtenir un certificat, par ex. avec [Let's Encrypt](https://letsencrypt.org/)

- ... Mais pour plus de commodit√©, pour cet atelier, nous transmettrons HTTP √† HTTPS

.warning[Ne le faites pas √† la maison, ou pire, au travail!]

.debug[[kube/dashboard_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/dashboard_fr.md)]
---

## Ex√©cution du d√©balleur SSL

- Nous allons lancer [`socat`](http://www.dest-unreach.org/socat/doc/socat.html), en lui disant d'accepter les connexions TCP et de les relayer via SSL

- Ensuite, nous allons exposer cette instance `socat` avec un service `NodePort`

- Pour plus de commodit√©, ces √©tapes sont soigneusement encapsul√©es dans un autre fichier YAML

.exercise[

- Appliquez le fichier YAML pratique et annulez la protection SSL:
  ```bash
  kubectl apply -f https://goo.gl/tA7GLz
  ```

]

L'URL goo.gl se d√©veloppe pour:
<br/>
.small[.small[https://gist.githubusercontent.com/jpetazzo/c53a28b5b7fdae88bc3c5f0945552c04/raw/da13ef1bdd38cc0e90b7a4074be8d6a0215e1a65/socat.yaml]]

.warning[Tout notre trafic de dashboard est maintenant en texte clair, y compris les mots de passe!]

.debug[[kube/dashboard_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/dashboard_fr.md)]
---

## Connexion au dashboard

.exercise[

- V√©rifiez quel port est le dashboard:
  ```bash
  kubectl -n kube-system get svc socat
  ```

]

Vous aurez besoin du port `8080`.


.exercise[

- Connectez-vous √† http://oneofournodes:3xxxx/

]

Le dashboard vous demandera ensuite l'authentification que vous souhaitez utiliser.

.debug[[kube/dashboard_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/dashboard_fr.md)]
---

## Authentification de dashboard

- Nous avons trois options d'authentification √† ce stade:

  - token (associ√© √† un role disposant des autorisations appropri√©es)

  - kubeconfig (par exemple en utilisant le fichier `~/.kube/config` de `node1`)

  - "skip" (utilisez le dashboard "service account")

- Utilisons "skip": nous recevons un tas d'avertissements et ne voyons pas grand-chose

.debug[[kube/dashboard_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/dashboard_fr.md)]
---

## 3) Ignorer l'authentification pour le dashboard

- La documentation du dashboard [explique comment proc√©der](https://github.com/kubernetes/dashboard/wiki/Access-control#admin-privileges)

- Nous avons juste besoin de charger un autre fichier YAML!

.exercise[

- Accorder des privil√®ges d'administrateur au dashboard afin que nous puissions voir nos ressources:
  ```bash
  kubectl apply -f https://goo.gl/CHsLTA
  ```

- Rechargez le dashboard et profitez-en!

]

--

.warning[Au fait, nous venons d'ajouter une porte d√©rob√©e √† notre cluster Kubernetes!]

.debug[[kube/dashboard_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/dashboard_fr.md)]
---

## Exposer le dashboard sur HTTPS

- Nous avons pris un raccourci en transf√©rant HTTP √† HTTPS dans le cluster

- Exposons le dashboard sur HTTPS!

- Le dashboard est expos√© via un service `ClusterIP` (trafic interne uniquement)

- Nous allons changer cela en un service `NodePort` (acceptant le trafic ext√©rieur)

.exercise[

- Modifier le service:
  ```bash
  kubectl edit service kubernetes-dashboard
  ```

]

--

`NotFound`?!? Pourquoi ca ne marche pas?!?

.debug[[kube/dashboard_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/dashboard_fr.md)]
---

## Modification du service `kubernetes-dashboard`

- Si nous regardons le [YAML](https://goo.gl/Qamqab) que nous avons charg√© avant, nous aurons un indice

--

- Le dashboard a √©t√© cr√©√© dans le namespace `kube-system`

--

.exercise[

- Modifier le service:
  ```bash
  kubectl -n kube-system edit service kubernetes-dashboard
  ```

- Changez `type: ClusterIP` en `type: NodePort`, sauvegardez et quittez

- V√©rifiez le port qui a √©t√© assign√© avec `kubectl -n kube-system get services`

- Connectez-vous √† https://oneofournodes:3xxxx/ (oui, https)

]

.debug[[kube/dashboard_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/dashboard_fr.md)]
---

## Ex√©cution s√©curis√©e du dashboard Kubernetes

- Les √©tapes que nous venons de vous montrer sont *avec des buts √©ducatives seulement!*

- Si vous faites cela sur votre cluster de production, les gens [peuvent et vont en abuser](https://blog.redlock.io/cryptojacking-tesla)

- Pour une discussion approfondie sur la s√©curisation du dashboard,
  <br/>
  v√©rifier [cet excellent post sur le blog de Heptio](https://blog.heptio.com/on-securing-the-kubernetes-dashboard-16b09b1b7aca)

.debug[[kube/dashboard_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/dashboard_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/train-of-containers-2.jpg)]

---

name: toc-les-implications-de-scurit-de-kubectl-apply
class: title

Les implications de s√©curit√© de `kubectl apply`

.nav[
[Section pr√©c√©dente](#toc-le-dashboard-de-kubernetes)
|
[Retour table des mati√®res](#toc-chapter-3)
|
[Section suivante](#toc-)
]

.debug[(automatically generated title slide)]

---

# Les implications de s√©curit√© de `kubectl apply`

- Quand nous faisons `kubectl apply -f <URL>`, nous cr√©ons des ressources arbitraires

- Les ressources peuvent √™tre mauvaises; Imaginez un `deployment` ...

--

   - d√©marre des mineurs bitcoin sur l'ensemble du cluster

--

   - cache dans un espace de noms autre que le "default"

--

   - bind-mounts le syst√®me de fichiers de nos n≈ìuds

--

   - ins√®re les cl√©s SSH dans le compte root (sur le noeud)

--

   - crypte nos donn√©es et les garde en rancon

--

   - ‚ò†Ô∏è‚ò†Ô∏è‚ò†Ô∏è

.debug[[kube/dashboard_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/dashboard_fr.md)]
---

## `kubectl apply` est le nouveau `curl | sh`

- `curl | sh` est pratique

- Il est s√ªr si vous utilisez des URL HTTPS provenant de sources fiables

--

- `kubectl apply -f` est pratique

- Il est s√ªr si vous utilisez des URL HTTPS provenant de sources fiables

- Exemple: les instructions d'installation officielles pour la plupart des r√©seaux de pod

--

- Il introduit de nouveaux modes de d√©faillance (comme si vous essayez d'appliquer yaml √† partir d'un lien qui n'est plus valide)


.debug[[kube/dashboard_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/dashboard_fr.md)]
---

## Pour aller plus loin


.exercise[
- Relancez l'exemple pr√©√ß√©dent
  ```bash
  kubectl run pingpong --image alpine ping 1.1.1.1
  ```
- Observez le deployment et son pod. Trouvez-vous les logs du pod?

- Arr√©tez le deployment.

]

Vous pouvez stopper le dashboard ou le laisser. Comme le dashboard n'est pas s√©curis√©, nous conseillons de l'arr√©ter.
```bash
  kubectl delete -f https://goo.gl/CHsLTA
  ```



.debug[[kube/dashboard_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/dashboard_fr.md)]</textarea>
    <script src="remark.min.js" type="text/javascript">
    </script>
    <script type="text/javascript">
      var slideshow = remark.create({
        ratio: '16:9',
        highlightSpans: true,
        excludedClasses: ["self-paced"]
      });
    </script>
    
    <!-- 
    These two scripts will be available only when loading the
    content using the pub/sub server. Otherwise, they'll just
    404 and that's OK.
    -->
    <script src="/socket.io/socket.io.js">
    </script>
    <script src="/remote.js">
    </script>

  </body>
</html>
