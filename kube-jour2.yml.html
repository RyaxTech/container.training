<!DOCTYPE html>
<html>
  <head>
    <title>Kubernetes  Concepts de base, Services et Deploiement  </title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <link rel="stylesheet" href="workshop.css">
  </head>
  <body>
    <!--
    <div style="position: absolute; left: 20%; right: 20%; top: 30%;">
      <h1 style="font-size: 3em;">Loading ...</h1>
      The slides should show up here. If they don't, it might be
      because you are accessing this file directly from your filesystem.
      It needs to be served from a web server. You can try this:
      <pre>
        docker-compose up -d
        open http://localhost:8888/workshop.html # on MacOS
        xdg-open http://localhost:8888/workshop.html # on Linux
      </pre>
      Once the slides are loaded, this notice disappears when you
      go full screen (e.g. by hitting "f").
    </div>
    -->
    <textarea id="source">class: title, self-paced

Kubernetes <br/>Concepts de base, Services et Deploiement <br/>

.nav[*Self-paced version*]

.debug[
```
 M intro-fullday.yml.html
 M intro-selfpaced.yml.html
 M kube-fullday.yml.html
 M kube-halfday.yml.html
UU kube-jour1.yml.html
UU kube-jour2.yml.html
UU kube-jour3.yml.html
A  kube/advanced.md
M  kube/advanced_fr.md
A  kube/configs.md
M  kube/configs_fr.md
A  kube/stockage.md
M  kube/stockage_fr.md
?? .directory
?? common/.directory
?? intro/.directory
?? kube/.directory
?? slides/
?? swarm/.directory

```

These slides have been built from commit: 52d8949


[common/title_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/title_fr.md)]
---

class: title, in-person

Kubernetes <br/>Concepts de base, Services et Deploiement <br/><br/></br>


.debug[[common/title_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/title_fr.md)]
---

name: toc-chapter-1

## Chapter 1

- [Les objets de Kubernetes](#toc-les-objets-de-kubernetes)

- [Exposition des conteneurs](#toc-exposition-des-conteneurs)

- [D√©ployer un registre auto-h√©berg√©](#toc-dployer-un-registre-auto-hberg)

- [Exposant des services en interne](#toc-exposant-des-services-en-interne)

- [Exposant des services pour un acc√®s externe](#toc-exposant-des-services-pour-un-accs-externe)

- [Passage √† l'√©chelle (scaling) d'un d√©ploiement](#toc-passage--lchelle-scaling-dun-dploiement)

.debug[(auto-generated TOC)]
---
name: toc-chapter-2

## Chapter 2

- [Daemon sets](#toc-daemon-sets)

- [Mise √† jour d'un service via des labels et des selectors](#toc-mise--jour-dun-service-via-des-labels-et-des-selectors)

- [Rolling updates](#toc-rolling-updates)

- [Acc√®s aux logs depuis le CLI](#toc-accs-aux-logs-depuis-le-cli)

- [Logs centralis√©e](#toc-logs-centralise)

.debug[(auto-generated TOC)]
---
name: toc-chapter-3

## Chapter 3

- [Gestion des stacks avec Helm](#toc-gestion-des-stacks-avec-helm)

- [Namespaces](#toc-namespaces)

.debug[(auto-generated TOC)]



.debug[[common/toc.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/toc.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/Container-Ship-Freighter-Navigation-Elbe-Romance-1782991.jpg)]

---

name: toc-les-objets-de-kubernetes
class: title

Les objets de Kubernetes

.nav[
[Section pr√©c√©dente](#toc-)
|
[Retour table des mati√®res](#toc-chapter-1)
|
[Section suivante](#toc-exposition-des-conteneurs)
]

.debug[(automatically generated title slide)]

---
# Les objets de Kubernetes
--

- **Pods**
  - Repr√©sente une unit√© de d√©ploiement compos√©e d'un ou de plusieurs conteneurs √©troitement li√©s partageant des ressources.
  - Les conteneurs d'un Pod peuvent communiquer entre eux via localhost.
  - Tous les pods r√©sident dans un seul espace d'adressage r√©seau partag√© et plat, aucune passerelle NAT n'existe entre eux. Les pods acc√®dent les uns aux autres sur leur adresse IP unique.

--

- **Controllers**
  - Cr√©ent et g√©rent plusieurs pods g√©rant la r√©plication, le d√©ploiement et l'auto-r√©paration.

.debug[[kube/kubeobjects_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubeobjects_fr.md)]
---

## Pods et Nodes

![Un des meilleurs diagrammes d'architecture Kubernetes disponibles](images/pods.png)


.debug[[kube/kubeobjects_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubeobjects_fr.md)]
---

## Pods et Reseau


![Un des meilleurs diagrammes d'architecture Kubernetes disponibles](images/pods2.png)

.debug[[kube/kubeobjects_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubeobjects_fr.md)]
---

## Pods et Conteneurs 

- On doit avoir un processus par conteneur. 

- Si les conteneurs n'ont pas besoin d‚Äô√™tre sur le m√™me node il vaut mieux les mettre dans des pods diff√©rents.

![Un des meilleurs diagrammes d'architecture Kubernetes disponibles](images/pods3.png)


.debug[[kube/kubeobjects_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubeobjects_fr.md)]
---


## Les objets de Kubernetes (suite)
--

- **Service**
  - Repr√©sente un seul point d'entr√©e constant √† un groupe de pods fournissant le m√™me service. Chaque service a une adresse IP et un port qui ne changent jamais tant que le service existe.
--

- **Volumes**
  - Des repertoires accessibles aux conteneurs d'un pod. Li√©s au cycle de vie des pods.

--

- **Namespaces**
  - Ils fournissent une abstraction permettant l'utilisation de plusieurs clusters virtuels soutenus par le m√™me cluster physique.
--

- **Nodes**
  - Ils peuvent √™tre des machines virtuelles ou des machines physiques, ils fournissent les services n√©cessaires pour ex√©cuter des pods et sont g√©r√©s par les composants principaux.

.debug[[kube/kubeobjects_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubeobjects_fr.md)]
---

class: pic

![Un des meilleurs diagrammes d'architecture Kubernetes disponibles](images/k8s-arch4-thanks-luxas.png)

.debug[[kube/kubeobjects_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubeobjects_fr.md)]
---

class: pic

![Un des meilleurs diagrammes d'architecture Kubernetes disponibles](images/kube_archi_simple.png)

.debug[[kube/kubeobjects_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubeobjects_fr.md)]
---

## L'architecture de Kubernetes

--

- **Communication et ex√©cution de composants**
  - Tous les composants passent par le serveur API pour communiquer entre eux
  - Seul kubelet s'ex√©cute en tant que composant syst√®me normal et peut ex√©cuter les autres composants en tant que pods
  - Les composants sur les n≈ìuds de travail doivent s'ex√©cuter sur le m√™me n≈ìud, mais les composants de ma√Ætre peuvent √™tre r√©partis sur plusieurs n≈ìuds

--

- **Etcd**
  - Key/Value Store distribu√© et coh√©rent.
  - Seulement le serveur API parle directement avec etcd. Tous les autres composants communiquent avec etcd indirectement via API-Server sur la base du "Contr√¥le de concurrence optimiste"
  - Utilise l'algorithme de consensus Raft pour d√©cider de l'√©tat actuel en fonction du quorum (majorit√©).

.debug[[kube/kubeobjects_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubeobjects_fr.md)]
---

## L'architecture de Kubernetes (suite)

--

- **API-Server**
  - Il fournit une interface CRUD (Creat, Read, Update Delete) pour interroger et modifier l'√©tat du cluster sur une API RESTful.
  - Effectue l'authentification, l'autorisation et le contr√¥le d'admission via diff√©rents plugins avant d'acc√©der √† l'√©tat dans etcd
  - Surveille le m√©canisme pour informer les clients des modifications sur les objets.

--

- **Controller manager**
  - Il combine une multitude de contr√¥leurs effectuant diverses t√¢ches de reconciliation d'etat.
  - Chaque contr√¥leur surveille le serveur API pour les modifications apport√©es aux ressources (D√©ploiements, Services, etc.) et effectue des op√©rations pour chaque modification
  - Il r√©concilie l'√©tat actuel avec l'√©tat souhait√© (sp√©cifi√© dans la section des sp√©cifications de la ressource)

.debug[[kube/kubeobjects_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubeobjects_fr.md)]
---


## L'architecture de Kubernetes (suite)

--

- **Scheduler**
  - Il met √† jour la d√©finition des pods et, via le m√©canisme de surveillance du serveur API, le kubelet est averti pour ex√©cuter un pod.
  - L'algorithme de planification par d√©faut d√©termine les n≈ìuds acceptables et s√©lectionne le meilleur pour le pod en fonction de divers param√®tres configurables.
  - Plusieurs schedulers peuvent s'ex√©cuter simultan√©ment dans le cluster et un module peut utiliser celui qui est le plus adapt√©.

.debug[[kube/kubeobjects_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubeobjects_fr.md)]
---
class: pic

![Exemple de Kubectl](images/kubectl_ex.png)

.debug[[kube/kubeobjects_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubeobjects_fr.md)]
---


.debug[[kube/kubeobjects_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubeobjects_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/ShippingContainerSFBay.jpg)]

---

name: toc-exposition-des-conteneurs
class: title

Exposition des conteneurs

.nav[
[Section pr√©c√©dente](#toc-les-objets-de-kubernetes)
|
[Retour table des mati√®res](#toc-chapter-1)
|
[Section suivante](#toc-dployer-un-registre-auto-hberg)
]

.debug[(automatically generated title slide)]

---
# Exposition des conteneurs

- `kubectl expose` cr√©e un *service* pour les pods existants

- Un *service* est une adresse stable pour un pod (ou un groupe de pods)

- Si nous voulons nous connecter √† notre (nos) pod(s), nous devons cr√©er un *service*

- Une fois qu'un service est cr√©√©, `kube-dns` nous permettra de l'acceder par son nom

  (c'est-√†-dire apr√®s avoir cr√©√© le service "hello", le nom "hello" va se lier √† quelque chose)

- Il existe diff√©rents types de services, d√©taill√©s sur les diapositives suivantes:

  `ClusterIP`,` NodePort`, `LoadBalancer`,` ExternalName`

.debug[[kube/kubectlexpose_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlexpose_fr.md)]
---

## Types de service de base

- `ClusterIP` (type par d√©faut)

  - une adresse IP virtuelle est allou√©e au service (dans une plage interne, priv√©e)
  - cette adresse IP est accessible uniquement depuis le cluster (n≈ìuds et pods)
  - notre code peut se connecter au service en utilisant le num√©ro de port d'origine

- `NodePort`

  - un port est attribu√© au service (par d√©faut, dans la plage 30000-32768)
  - ce port est disponible *sur tous nos n≈ìuds* et n'importe qui peut s'y connecter
  - notre code doit √™tre chang√© pour se connecter √† ce nouveau num√©ro de port

Ces types de service sont toujours disponibles.

Sous le tapis: `kube-proxy` utilise un proxy userland et un tas de r√®gles `iptables`.

.debug[[kube/kubectlexpose_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlexpose_fr.md)]
---

## Plus de types de services

- `LoadBalancer`

  - un √©quilibreur de charge externe est affect√© au service
  - l'√©quilibreur de charge est configur√© en cons√©quence
    <br/>(par exemple: un service `NodePort` est cr√©√© et l'√©quilibreur de charge envoie le trafic vers ce port)

- `ExternalName`

  - l'entr√©e DNS g√©r√©e par `kube-dns` sera juste un `CNAME` dans un enregistrement fourni
  - pas de port, pas d'adresse IP, rien d'autre n'est attribu√©

Le type `LoadBalancer` est actuellement disponible uniquement sur AWS, Azure et GCE.

.debug[[kube/kubectlexpose_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlexpose_fr.md)]
---

## Ex√©cuter des conteneurs avec des ports ouverts

- Puisque `ping` n'a rien √† connecter, nous devrons ex√©cuter autre chose

.exercise[

- D√©marrer un tas de conteneurs ElasticSearch:
  ```bash
  kubectl run elastic --image=elasticsearch:2 --replicas=7
  ```

- Regardez-les commencer:
  ```bash
  kubectl get pods -w
  ```

]

L'option `-w` "watches" les √©v√©nements se produisant sur les ressources sp√©cifi√©es.

Note: veuillez NE PAS appeler le service `search`. Cela entrerait en conflit avec le TLD.

.debug[[kube/kubectlexpose_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlexpose_fr.md)]
---

## Exposant notre d√©ploiement

- Nous allons cr√©er un service `ClusterIP` par d√©faut

.exercise[

- Exposez le port de l'API HTTP ElasticSearch:
  ```bash
  kubectl expose deploy/elastic --port 9200
  ```

- Rechercher quelle adresse IP a √©t√© attribu√©e:
  ```bash
  kubectl get svc
  ```

]

.debug[[kube/kubectlexpose_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlexpose_fr.md)]
---

## Les services sont des constructions de couche 4

- Vous pouvez attribuer des adresses IP aux services, mais ils sont toujours *couche 4*

  (c'est-√†-dire qu'un service n'est pas une adresse IP, c'est une adresse IP + un protocole + un port)

- Ceci est caus√© par l'impl√©mentation actuelle de `kube-proxy`

  (il repose sur des m√©canismes qui ne supportent pas la couche 3)

- En cons√©quence: vous *devez* indiquer le num√©ro de port pour votre service
    
- L'ex√©cution de services avec un port arbitraire (ou des plages de ports) n√©cessite des hacks

  (par exemple, host networking mode)

.debug[[kube/kubectlexpose_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlexpose_fr.md)]
---

## Test de notre service

- Nous allons maintenant envoyer quelques requ√™tes HTTP √† nos pods ElasticSearch

.exercise[

- Obtenons l'adresse IP qui a √©t√© allou√©e pour notre service, *par programmation:*
  ```bash
  IP=$(kubectl get svc elastic -o go-template --template '{{.spec.clusterIP}}')
  ```

- Envoyez quelques demandes:
  ```bash
  curl http://$IP:9200/
  ```

]

--

Nous pouvons voir `curl: (7) Failed to connect to _IP_ port 9200: Connection refused`.

C'est normal pendant que le service d√©marre.

--

Une fois qu'il est en cours d'ex√©cution, nos demandes sont √©quilibr√©es en charge sur plusieurs pods.

.debug[[kube/kubectlexpose_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlexpose_fr.md)]
---

class: extra-details

## Si nous n'avons pas besoin d'un √©quilibreur de charge

- Parfois, nous voulons acc√©der directement √† nos services qui passent √† l'√©chelle:

  - Si nous voulons sauver un petit peu de latence (typiquement moins de 1ms)

  - si nous devons nous connecter sur des ports arbitraires (au lieu de quelques uns fixes)

  - si nous avons besoin de communiquer sur un autre protocole que UDP ou TCP

  - si nous voulons d√©cider comment √©quilibrer les demandes c√¥t√© client

  - ...

- Dans ce cas, nous pouvons utiliser un "headless service"

.debug[[kube/kubectlexpose_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlexpose_fr.md)]
---

class: extra-details

## Headless Services

- Un "headless service" est obtenu en d√©finissant le champ `clusterIP` sur `None`

  (Soit avec `--cluster-ip = None`, soit en fournissant un YAML personnalis√©)

- Par cons√©quent, le service n'a pas d'adresse IP virtuelle

- Comme il n'y a pas d'adresse IP virtuelle, il n'y a pas non plus d'√©quilibreur de charge

- `kube-dns` retournera les adresses IP des pods en plusieurs enregistrements `A`

- Cela nous donne un moyen facile de d√©couvrir toutes les r√©pliques pour un d√©ploiement

.debug[[kube/kubectlexpose_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlexpose_fr.md)]
---

class: extra-details

## Services et endpoints

- Un service a un certain nombre de "endpoints"

- Chaque endpoint est un h√¥te + port o√π le service est disponible

- Les endpoints sont maintenus et mis √† jour automatiquement par Kubernetes

.exercise[

- V√©rifiez les points de terminaison que Kubernetes a associ√©s √† notre service `elastic`:
  ```bash
  kubectl describe service elastic
  ```

]

Dans la sortie, il y aura une ligne commen√ßant par `Endpoints:`.

Cette ligne listera un tas d'adresses au format `host:port`.

.debug[[kube/kubectlexpose_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlexpose_fr.md)]
---

class: extra-details

## Affichage des d√©tails du point de terminaison

- Lorsque nous avons beaucoup de endpoints, nos commandes d'affichage tronquent la liste
  ```bash
  kubectl get endpoints
  ```

- Si nous voulons voir la liste compl√®te, nous pouvons utiliser l'une des commandes suivantes:
  ```bash
  kubectl describe endpoints elastic
  kubectl get endpoints elastic -o yaml
  ```

- Ces commandes vont nous montrer une liste d'adresses IP

- Ces adresses IP doivent correspondre aux adresses des pods correspondants:
  ```bash
  kubectl get pods -l run=elastic -o wide
  ```

.debug[[kube/kubectlexpose_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlexpose_fr.md)]
---

class: extra-details

## "endpoints" pas "endpoint"

- `endpoints` est la seule ressource qui ne peut pas √™tre singuli√®re

```bash
$ kubectl get endpoint
error: the server doesn't have a resource type "endpoint"
```

- C'est parce que le type lui-m√™me est pluriel (contrairement √† toutes les autres ressources)

- Il n'y a pas d'objet `endpoint` object: `type Endpoints struct`

- Le type ne repr√©sente pas un seul endpoint, mais une liste de endpoints



.debug[[kube/kubectlexpose_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlexpose_fr.md)]
---
class: title

Notre application sur Kubernetes

.debug[[kube/ourapponkube_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/ourapponkube_fr.md)]
---

## Qu'est-ce qu'il y'a au menu?

Dans cette partie, nous allons:

- **construire** des images pour notre application,

- **exp√©dier** ces images avec un registre,

- **d√©ployer** des d√©ploiements utilisant ces images,

- exposer ces d√©ploiements pour qu'ils puissent communiquer entre eux,

- exposer l'interface web afin que nous puissions y acc√©der de l'ext√©rieur.

.debug[[kube/ourapponkube_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/ourapponkube_fr.md)]
---

## Le plan

- Construction sur notre noeud de contr√¥le (`node1`)

- Marquage des images pour qu'elles soient nomm√©es `$REGISTRY/servicename`

- T√©l√©chargement sur un registre

- Creation des d√©ploiements en utilisant les images

- Exposition (avec un ClusterIP) des services qui ont besoin de communiquer

- Exposition (avec un NodePort) de l'interface Web

.debug[[kube/ourapponkube_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/ourapponkube_fr.md)]
---

## Quel registre voulons-nous utiliser?

- Nous pourrions utiliser le Docker Hub

- Ou un service offert par notre fournisseur de cloud (ACR, GCR, ECR ...)

- Ou nous pourrions simplement auto-h√©berger ce registre

*Nous h√©bergerons automatiquement le registre car c'est la solution la plus g√©n√©rique pour cet atelier.*

.debug[[kube/ourapponkube_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/ourapponkube_fr.md)]
---

## Utiliser le registre open source

- Nous devons lancer un conteneur `registry:2`
  <br/>(assurez-vous de sp√©cifier la balise `:2` pour ex√©cuter la nouvelle version!)

- Il va stocker des images et des couches dans le syst√®me de fichiers local
  <br/>(mais vous pouvez ajouter un fichier de configuration pour utiliser S3, Swift, etc.)

- Docker *n√©cessite* TLS lors de la communication avec le registre

  - sauf pour les registres sur ¬´127.0.0.0/8¬ª (c'est-√†-dire `localhost`)

  - ou avec le flag Engine `--insecure-registry`

- Notre strat√©gie: publier le conteneur de registre sur un NodePort,
  <br/> pour qu'il soit disponible via `127.0.0.1:xxxxx` sur chaque noeud

.debug[[kube/ourapponkube_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/ourapponkube_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/aerial-view-of-containers.jpg)]

---

name: toc-dployer-un-registre-auto-hberg
class: title

D√©ployer un registre auto-h√©berg√©

.nav[
[Section pr√©c√©dente](#toc-exposition-des-conteneurs)
|
[Retour table des mati√®res](#toc-chapter-1)
|
[Section suivante](#toc-exposant-des-services-en-interne)
]

.debug[(automatically generated title slide)]

---

# D√©ployer un registre auto-h√©berg√©

- Nous allons d√©ployer un conteneur de registre et l'exposer avec un NodePort

.exercise[

- Cr√©ez le service de registre:
  ```bash
  kubectl run registry --image=registry:2
  ```

- Exposez-le sur un NodePort:
  ```bash
  kubectl expose deploy/registry --port=5000 --type=NodePort
  ```

]

.debug[[kube/ourapponkube_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/ourapponkube_fr.md)]
---

## Connexion √† notre registre

- Nous devons trouver quel port a √©t√© attribu√©

.exercise[

- Voir les d√©tails du service:
  ```bash
  kubectl describe svc/registry
  ```

- Obtenez le num√©ro de port par programme:
  ```bash
  NODEPORT=$(kubectl get svc/registry -o json | jq .spec.ports[0].nodePort)
  REGISTRY=127.0.0.1:$NODEPORT
  ```

]

.debug[[kube/ourapponkube_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/ourapponkube_fr.md)]
---

## Test de notre registre

- Une route API de registre Docker pratique √† retenir est `/v2/_catalog`

.exercise[

- Voir les d√©p√¥ts actuellement d√©tenus dans notre registre:
  ```bash
  curl $REGISTRY/v2/_catalog
  ```

]

--

Nous devrions voir:
```json
{"repositories": []}
```

.debug[[kube/ourapponkube_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/ourapponkube_fr.md)]
---

## Test de notre registre local

- Nous pouvons retagger une petite image, et la pousser vers le registre

.exercise[

- Assurez-vous que nous avons l'image busybox, et retaggez la:
  ```bash
  docker pull busybox
  docker tag busybox $REGISTRY/busybox
  ```

- Poussez-le:
  ```bash
  docker push $REGISTRY/busybox
  ```

]

.debug[[kube/ourapponkube_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/ourapponkube_fr.md)]
---

## V√©rifier √† nouveau ce qu'il y a dans notre registre local

- Utilisons le m√™me point final que pr√©c√©demment

.exercise[

- Assurez-vous que notre image busybox est maintenant dans le registre local:
  ```bash
  curl $ REGISTRY / v2 / _catalog
  ```

]

La commande curl devrait maintenant sortir:
```json
{"repositories": ["busybox"]}
```

.debug[[kube/ourapponkube_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/ourapponkube_fr.md)]
---

## Construire et pousser nos images

- Nous allons utiliser une fonctionnalit√© pratique de Docker Compose

.exercise[

- Allez dans le r√©pertoire `stacks`:
  ```bash
  cd ~/container.training/stacks
  ```

- Construire et pousser les images:
  ```bash
  export REGISTRY
  export TAG=v0.1
  docker-compose -f dockercoins.yml build
  docker-compose -f dockercoins.yml push
  ```

]

Jetons un coup d'≈ìil au fichier `dockercoins.yml` pendant que ce dernier construit et pousse.

.debug[[kube/ourapponkube_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/ourapponkube_fr.md)]
---

```yaml
version: "3"

services:
  rng:
    build: dockercoins/rng
    image: ${REGISTRY-127.0.0.1:5000}/rng:${TAG-latest}
    deploy:
      mode: global
  ...
  redis:
    image: redis
  ...
  worker:
    build: dockercoins/worker
    image: ${REGISTRY-127.0.0.1:5000}/worker:${TAG-latest}
    ...
    deploy:
      replicas: 10
```

.warning[Juste au cas o√π vous vous poseriez la question ... Les "services" de Docker ne sont pas des "services" de Kubernetes.]

.debug[[kube/ourapponkube_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/ourapponkube_fr.md)]
---

class: extra-details

## √âviter le tag `latest`

.warning[Assurez-vous d'avoir bien d√©fini la variable `TAG`!]

- Si vous ne le faites pas, le tag sera par d√©faut `latest`

- Le probl√®me avec `latest`: personne ne sait √† quoi √ßa veut dire!

  - Le dernier commit dans le repo?

  - Le dernier commit dans une branche? (Laquelle?)

  - Le dernier tag?

  - Une version al√©atoire pouss√©e par un membre de l'√©quipe al√©atoire?

- Si vous continuez √† appuyer sur la balise `latest`, comment faire de rollback?

- Les tags de "Images" doivent √™tre significatives, c'est-√†-dire correspondre √† des branches, tags, ou hashes

.debug[[kube/ourapponkube_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/ourapponkube_fr.md)]
---

## D√©ploiement de toutes les choses

- Nous pouvons maintenant d√©ployer notre code (ainsi qu'une instance redis)

.exercise[

- D√©ployer `redis`:
  ```bash
  kubectl run redis --image=redis
  ```

- D√©ployer tout le reste:
  ```bash
   for SERVICE in hasher rng webui worker; do
      kubectl run $SERVICE --image=$REGISTRY/$SERVICE:$TAG
    done
  ```

]

.debug[[kube/ourapponkube_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/ourapponkube_fr.md)]
---

## Est-ce que √ßa marche?

- Apr√®s avoir attendu la fin du d√©ploiement, regardons les logs!

  (Indice: utilisez `kubectl get deploy -w` pour regarder les √©v√©nements de d√©ploiement)

.exercise[

- Regardez quelques logs:
  ```bash
  kubectl logs deploy/rng
  kubectl logs deploy/worker
  ```

]

--

ü§î `rng` va bien ... Mais pas `worker`.

--

üí° Oh, c'est vrai! Nous avons oubli√© de "exposer".

.debug[[kube/ourapponkube_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/ourapponkube_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/blue-containers.jpg)]

---

name: toc-exposant-des-services-en-interne
class: title

Exposant des services en interne

.nav[
[Section pr√©c√©dente](#toc-dployer-un-registre-auto-hberg)
|
[Retour table des mati√®res](#toc-chapter-1)
|
[Section suivante](#toc-exposant-des-services-pour-un-accs-externe)
]

.debug[(automatically generated title slide)]

---

# Exposant des services en interne

- Trois d√©ploiements doivent √™tre accessibles par d'autres: `hasher`,` redis`, `rng`

- `worker` n'a pas besoin d'√™tre expos√©

- `webui` sera trait√© plus tard

.exercise[

- Exposez chaque d√©ploiement, en sp√©cifiant le bon port:
  ```bash
  kubectl expose deployment redis --port 6379
  kubectl expose deployment rng --port 80
  kubectl expose deployment hasher --port 80
  ```

]

.debug[[kube/ourapponkube_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/ourapponkube_fr.md)]
---

## Est-ce que √ßa marche encore?

- Le `worker` a une boucle infinie, qui r√©essaie 10 secondes apr√®s une erreur

.exercise[

- Diffuser les logs du worker:
  ```bash
  kubectl logs deploy/worker --follow
  ```

  (Donnez-lui environ 10 secondes pour r√©cup√©rer)

]

--

Nous devrions maintenant voir le ¬´travailleur¬ª, bien, travaillant heureusement.

.debug[[kube/ourapponkube_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/ourapponkube_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/chinook-helicopter-container.jpg)]

---

name: toc-exposant-des-services-pour-un-accs-externe
class: title

Exposant des services pour un acc√®s externe

.nav[
[Section pr√©c√©dente](#toc-exposant-des-services-en-interne)
|
[Retour table des mati√®res](#toc-chapter-1)
|
[Section suivante](#toc-passage--lchelle-scaling-dun-dploiement)
]

.debug[(automatically generated title slide)]

---

# Exposant des services pour un acc√®s externe

- Maintenant, nous aimerions acc√©der √† l'interface Web

- Nous l'exposerons avec un `NodePort`

  (Juste comme nous l'avons fait pour le registre)

.exercise[

- Cr√©ez un service `NodePort` pour l'interface Web:
  ```bash
  kubectl expose deploy/webui --type=NodePort --port=8080
  ```

- V√©rifiez le port qui a √©t√© allou√©:
  ```bash
  kubectl obtenir svc
  ```

]

.debug[[kube/ourapponkube_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/ourapponkube_fr.md)]
---

## Acc√®s √† l'interface utilisateur Web

- Nous pouvons maintenant nous connecter √† *n'importe quel noeud*, sur le port de noeud allou√©, pour voir l'interface web

.exercise[

- Ouvrez l'interface web dans votre navigateur (http://node-ip-address:3xxxx/)
]

--

*D'accord, nous sommes de retour l√† o√π nous avons commenc√©, quand nous utilisions un seul n≈ìud!*



.debug[[kube/ourapponkube_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/ourapponkube_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/container-cranes.jpg)]

---

name: toc-passage--lchelle-scaling-dun-dploiement
class: title

Passage √† l'√©chelle (scaling) d'un d√©ploiement

.nav[
[Section pr√©c√©dente](#toc-exposant-des-services-pour-un-accs-externe)
|
[Retour table des mati√®res](#toc-chapter-1)
|
[Section suivante](#toc-daemon-sets)
]

.debug[(automatically generated title slide)]

---
# Passage √† l'√©chelle (scaling) d'un d√©ploiement

- Nous allons commencer par un facile: le d√©ploiement du `worker`

.exercise[

- Ouvrez deux nouveaux terminaux pour v√©rifier ce qui se passe avec les pods et les d√©ploiements:
  ```bash
  kubectl get pods -w
  kubectl get deployments -w
  ```


- Maintenant, cr√©ez plus de replicas `worker`:
  ```bash
  kubectl scale deploy/worker --replicas=10
  ```

]

Apr√®s quelques secondes, le graphique dans l'interface utilisateur Web devrait appara√Ætre.
<br/>
(Et aller jusqu'√† 10 hashes/seconde)



.debug[[kube/kubectlscale_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/kubectlscale_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/container-housing.jpg)]

---

name: toc-daemon-sets
class: title

Daemon sets

.nav[
[Section pr√©c√©dente](#toc-passage--lchelle-scaling-dun-dploiement)
|
[Retour table des mati√®res](#toc-chapter-2)
|
[Section suivante](#toc-mise--jour-dun-service-via-des-labels-et-des-selectors)
]

.debug[(automatically generated title slide)]

---
# Daemon sets

- Nous voulons passer √† l'√©chelle `rng` d'une mani√®re qui est diff√©rente de la fa√ßon dont nous avons passer √† l'√©chelle `worker`

- Nous voulons une (et exactement une) instance de `rng` par noeud

- Et si nous d√©ployions simplement `deploy/rng` sur le nombre de n≈ìuds?

  - rien ne garantit que les conteneurs `rng` seront distribu√©s uniform√©ment

  - Si nous ajoutons des n≈ìuds plus tard, ils ne lanceront pas automatiquement une copie de `rng`

  - Si nous supprimons (ou red√©marrons) un n≈ìud, un conteneur `rng` va red√©marrer ailleurs

- Au lieu d'un `deploiyment`, nous utiliserons un `daemonset`

.debug[[kube/daemonset_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/daemonset_fr.md)]
---

## Daemon set en pratique

- Les daemons sets sont parfaits pour les processus par n≈ìud √† l'√©chelle du cluster:

  - `kube-proxy`

  - `weave` (notre overlay de r√©seau)

  - agents de surveillance

  - des outils de gestion du mat√©riel (par exemple des agents SCSI / FC HBA)

  - etc.

- Ils peuvent √©galement √™tre limit√©s √† l'ex√©cution [uniquement sur certains n≈ìuds](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#running-pods-on-only-some-nodes)

.debug[[kube/daemonset_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/daemonset_fr.md)]
---

## Cr√©ation d'un daemon set

- Malheureusement, √† partir de Kubernetes 1.10, l'interface de ligne de commande ne peut pas cr√©er de jeux de d√©mons

--

- Plus pr√©cis√©ment: il n'a pas de sous-commande pour cr√©er un daemon set

--

- Mais n'importe quel type de ressource peut toujours √™tre cr√©√© en fournissant une description YAML:
  ```bash
  kubectl apply -f foo.yaml
  ```

--

- Comment cr√©ons-nous le fichier YAML pour notre daemon set?

--

  - option 1: [lire les docs](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#create-a-daemonset)

--

  - option 2: `vi` notre moyen de sortir de celui-ci

.debug[[kube/daemonset_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/daemonset_fr.md)]
---

## Cr√©ation du fichier YAML pour notre daemon set

- Commen√ßons par le fichier YAML pour la ressource `rng` courante

.exercise[

- Faites un dump du ressource `rng` dans YAML:
  ```bash
  kubectl get deploy/rng -o yaml --export> rng.yml
  ```

- Modifier `rng.yml`

]

Note: `--export` supprimera les informations sp√©cifiques au cluster, c'est-√†-dire:
- namespace (pour que la ressource ne soit pas li√©e √† un espace de noms sp√©cifique)
- status et creation timestamp (inutile lors de la cr√©ation d'une nouvelle ressource)
- resourceVersion et uid (ils causeraient des probl√®mes ... *int√©ressants*)

.debug[[kube/daemonset_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/daemonset_fr.md)]
---

## "Casting" d'une ressource √† un autre

- Et si on changeait juste le champ `kind`?

  (√áa ne peut pas √™tre aussi simple, non?)

.exercise[

- Changez `kind: Deployment` en` kind: DaemonSet`

- Enregistrer, quitter

- Essayez de cr√©er notre nouvelle ressource:
  ```bash
  kubectl apply -f rng.yml
  ```

]

--

Nous savions tous que cela ne pourrait pas √™tre aussi facile, non!

.debug[[kube/daemonset_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/daemonset_fr.md)]
---

## Comprendre le probl√®me

- Le coeur de l'erreur est:
  ```
  error validating data:
  [ValidationError(DaemonSet.spec):
  unknown field "replicas" in io.k8s.api.extensions.v1beta1.DaemonSetSpec,
  ...
  ```
--

- *De toute √©vidence,* cela n'a pas de sens de sp√©cifier un nombre de r√©plicas pour un ensemble de d√©mons

--

- Solution de contournement: r√©solvez le probl√®me du YAML

  - supprimer le champ `replicas`
  - supprimer le champ `strategy` (qui d√©finit le m√©canisme de d√©ploiement pour un d√©ploiement)
  - enlever la ligne `status: {}` √† la fin

--

- Ou, on pourrait aussi ...

.debug[[kube/daemonset_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/daemonset_fr.md)]
---

## Utilise le `--force`, Luke

- Nous pourrions aussi dire √† Kubernetes d'ignorer ces erreurs et essayer quand m√™me

- Le nom actuel du flag `--force` est` --validate = false`

.exercise[

- Essayez de charger notre fichier YAML et ignorez les erreurs:
  ```bash
  kubectl apply -f rng.yml --validate=false
  ```

]

--

üé©‚ú®üêá

--

Attendez ... Maintenant, cela peut-il √™tre * aussi facile?

.debug[[kube/daemonset_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/daemonset_fr.md)]
---

## V√©rification de ce que nous avons fait

- Avons-nous transform√© notre ¬´deployment¬ª en un ¬´daemonset¬ª?

.exercise[

- Regardez les ressources que nous avons maintenant:
  ```bash
  Kubectl get all
  ```

]

--

Nous avons deux ressources appel√©es `rng`:

- le *deployment* existant avant

- le *daemon set* que nous venons de cr√©er

Nous avons aussi un trop grand nombre de pods.
<br/>
(Le module correspondant au *deployment* existe toujours.)

.debug[[kube/daemonset_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/daemonset_fr.md)]
---

## `deploy / rng` et` ds / rng`

- Vous pouvez avoir diff√©rents types de ressources avec le m√™me nom

  (c'est-√†-dire un *deploiement* et un *daemon set* tous deux nomm√©s "rng")

- Nous avons toujours l'ancien d√©ploiement `rng` * *

  ```
NAME                       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/rng        1         1         1            1           18m
  ```


- Mais maintenant nous avons le nouveau *daemon set* `rng`

  ```
NAME                DESIRED  CURRENT  READY  UP-TO-DATE  AVAILABLE  NODE SELECTOR  AGE
daemonset.apps/rng  2        2        2      2           2          <none>         9s
  ```

.debug[[kube/daemonset_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/daemonset_fr.md)]
---

## Trop de pods

- Si nous v√©rifions avec `kubectl get pods`, nous voyons:

  - *un pod* pour le deployment (nomm√© `rng-xxxxxxxxxx-yyyyy`)

  - *un pod par n≈ìud* pour le daemon set (nomm√© `rng-zzzzz`)

  ```
  NAME                        READY     STATUS    RESTARTS   AGE
  rng-54f57d4d49-7pt82        1/1       Running   0          11m
  rng-b85tm                   1/1       Running   0          25s
  rng-hfbrr                   1/1       Running   0          25s
  [...]
  ```
--

Le daemon set a cr√©√© un pod par n≈ìud, sauf sur le n≈ìud master.

Le n≈ìud principal a [taints](https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/) emp√™chant les pods de s'ex√©cuter.

(Pour planifier un pod sur ce n≈ìud de toute fa√ßon, le pod requiert les [tolerations](https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/).) appropri√©es

.footnote [(D√©sactiv√© par un? Nous n'ex√©cutons pas ces pods sur le n≈ìud h√©bergeant le Control Plane.)]

.debug[[kube/daemonset_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/daemonset_fr.md)]
---

## Que font tous ces pods?

- V√©rifions les logs de tous ces pods `rng`

- Tous ces pods ont un label `run=rng`:

  - le premier pod, parce que c'est ce que `kubectl run` fait
  - les autres (dans le daemon set), parce que nous avons
    *copi√© la sp√©cification du premier*

- Par cons√©quent, nous pouvons interroger les logs de tout le monde en utilisant ce selector `run=rng`

.exercise[

- V√©rifier les logs de tous les pods ayant un label `run=rng`:
  ```bash
  kubectl logs -l run=rng --tail 1
  ```

]

--

Il semble que *tous les pods* servent des requ√™tes pour le moment.

.debug[[kube/daemonset_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/daemonset_fr.md)]
---

## La magie des selectors

- Le *service* `rng` fait l'√©quilibrage de charge de requ√™tes vers un ensemble de pods

- Cet ensemble de pods est d√©fini comme "pods ayant le label `run=rng`"

.exercise[

- V√©rifiez le *selector* dans la d√©finition du service `rng`:
  ```bash
  kubectl describe service rng
  ```

]

Lorsque nous avons cr√©√© des pods suppl√©mentaires avec ce label, ils √©taient
automatiquement d√©tect√© par `svc/rng` et ajout√© en tant que *endpoints*
√† l'√©quilibreur de charge associ√©.

.debug[[kube/daemonset_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/daemonset_fr.md)]
---

## Retrait du premier pod de l'√©quilibreur de charge

- Que se passerait-il si on supprimait ce pod, avec `kubectl delete pod ...`?

--

  Le `replica set` le recr√©erait imm√©diatement.

--

- Que se passerait-il si nous enlevions le label `run=rng` de ce pod?

--

  Le `replica set` le recr√©erait imm√©diatement.

--

  ... Parce que ce qui compte pour le `replica set`, c'est le nombre de pods *correspondant √† ce selector.*

--

- Mais mais mais ... N'avons-nous pas plus d'un pod avec `run=rng` maintenant?

--

  La r√©ponse r√©side dans le selector exact utilis√© par le `replica set` ...

.debug[[kube/daemonset_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/daemonset_fr.md)]
---

## Plong√©e profonde dans les selectors

- Regardons les selectors pour le *deployment* `rng` et le *replica set* associ√©

.exercise[

- Afficher des informations d√©taill√©es sur le deployment `rng`:
  ```bash
  kubectl describe deploy rng
  ```

- Afficher des informations d√©taill√©es sur le r√©plica `rng`:
  <br/> (La deuxi√®me commande ne n√©cessite pas que vous obteniez le nom exact du replica set)
  ```bash
  kubectl describe rs rng-yyyy
  kubectl describe rs -l run=rng
  ```

]

--

Le selector du replica set poss√®de √©galement un `pod-template-hash`, contrairement aux pods de notre daemon set.

.debug[[kube/daemonset_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/daemonset_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/containers-by-the-water.jpg)]

---

name: toc-mise--jour-dun-service-via-des-labels-et-des-selectors
class: title

Mise √† jour d'un service via des labels et des selectors

.nav[
[Section pr√©c√©dente](#toc-daemon-sets)
|
[Retour table des mati√®res](#toc-chapter-2)
|
[Section suivante](#toc-rolling-updates)
]

.debug[(automatically generated title slide)]

---

# Mise √† jour d'un service via des labels et des selectors

- Et si nous voulons supprimer le deployment `rng` de l'√©quilibreur de charge?

- Option 1:

  - Detruis-le

- Option 2:

  - ajouter un *label* suppl√©mentaire au daemon set

  - mettre √† jour le service *selector* pour faire r√©f√©rence √† ce *label*

--

Bien s√ªr, l'option 2 offre plus d'opportunit√©s d'apprentissage. Non?

.debug[[kube/daemonset_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/daemonset_fr.md)]
---

## Ajouter un label suppl√©mentaire au daemon set

- Nous mettrons √† jour le daemon set "spec"

- Option 1:

  - √©ditez le fichier `rng.yml` que nous avons utilis√© plus t√¥t

  - chargez la nouvelle d√©finition avec `kubectl apply`

- Option 2:

  - utilisez `kubectl edit`

--

*Nous avons inclus quelques conseils sur les prochaines diapositives pour votre facilitation!*

.debug[[kube/daemonset_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/daemonset_fr.md)]
---

## Nous avons mis des ressources dans vos ressources

- Rappel: un daemon set est une ressource qui cr√©e plus de ressources!

- Il y a une diff√©rence entre:

  - le(s) label(s) d'une ressource (dans le bloc `metadata` au d√©but)

  - le selector d'une ressource (dans le bloc `spec`)

  - le(s) label(s) de la (des) ressource(s) cr√©√©e(s) par la premi√®re ressource (dans le bloc `template`)

- Vous devez mettre √† jour le selector et le template (les labels de metadata ne sont pas obligatoires)

- Le template doit correspondre au selector

  (c'est-√†-dire que la ressource refusera de cr√©er des ressources qu'elle ne s√©lectionnera pas)

.debug[[kube/daemonset_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/daemonset_fr.md)]
---

## Ajout de notre label

- Ajoutons un label `isactive: yes`

- En YAML, `yes` doit √™tre cit√©; c'est-√†-dire `isactive: "yes"`

.exercise[

- Mettre √† jour le daemon set pour ajouter `isactive:" yes "` au label du selector et du template:
  ```bash
  kubectl edit daemonset rng
  ```

- Mettre √† jour le service pour ajouter `isactive:" yes "` √† son selector:
  ```bash
  kubectl edit service rng
  ```

]

.debug[[kube/daemonset_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/daemonset_fr.md)]
---

## V√©rification de ce que nous avons fait

.exercise[

- V√©rifiez la ligne du log la plus r√©cente de tous les pods `run=rng` pour confirmer que exactement un par n≈ìud est maintenant actif:
  ```bash
  kubectl logs -l run=rng --tail 1
  ```

]

Les timestamps devraient nous donner un indice sur le nombre de pods qui re√ßoivent actuellement du trafic.

.exercise[

- Regardez les pods que nous avons en ce moment:
  ```bash
  kubectl get pods
  ```

]

.debug[[kube/daemonset_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/daemonset_fr.md)]
---

## Nettoyage

- Les pods du deployment et le "vieux" daemon set sont toujours en cours d'ex√©cution

- Nous allons les identifier par programme

.exercise[

- Listez les pods avec `run=rng` mais sans `isactive=yes`:
  ```bash
  kubectl get pods -l run=rng,isactive!=yes
  ```

- Enlevez ces pods:
  ```bash
  kubectl delete pods -l run=rng,isactive!=yes
  ```

]

.debug[[kube/daemonset_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/daemonset_fr.md)]
---

## Nettoyage des pods mortes

```
$ kubectl get pods
NAME                        READY     STATUS        RESTARTS   AGE
rng-54f57d4d49-7pt82        1/1       Terminating   0          51m
rng-54f57d4d49-vgz9h        1/1       Running       0          22s
rng-b85tm                   1/1       Terminating   0          39m
rng-hfbrr                   1/1       Terminating   0          39m
rng-vplmj                   1/1       Running       0          7m
rng-xbpvg                   1/1       Running       0          7m
[...]
```

- Les pods suppl√©mentaires (not√©es `Terminating` ci-dessus) vont dispara√Ætre

- ... Mais un nouveau (`rng-54f57d4d49-vgz9h` ci-dessus) a √©t√© red√©marr√© imm√©diatement!

--

- Rappelez-vous, le *deployment* existe toujours et v√©rifie qu'un pod est op√©rationnel

- Si on supprime le pod associ√© au deployment, il est recr√©√© automatiquement

.debug[[kube/daemonset_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/daemonset_fr.md)]
---

## Suppression d'un d√©ploiement

.exercise[

- Supprimez le d√©ploiement `rng`:
  ```bash
  kubectl delete deployment rng
  ```
]

-

- Le pod cr√©√© par le d√©ploiement est en cours de finalisation:

```
$ kubectl get pods
NAME                        READY     STATUS        RESTARTS   AGE
rng-54f57d4d49-vgz9h        1/1       Terminating   0          4m
rng-vplmj                   1/1       Running       0          11m
rng-xbpvg                   1/1       Running       0          11m
[...]
```

Ding, dong, le d√©ploiement est mort! Et le daemon set continue √† vivre.

.debug[[kube/daemonset_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/daemonset_fr.md)]
---

## √âviter les pods suppl√©mentaires

- Lorsque nous avons chang√© la d√©finition de daemon set, il a imm√©diatement cr√©√© de nouveaux pods. Nous avons d√ª enlever les anciens manuellement.

- Comment aurions-nous pu √©viter cela?

-

- En ajoutant le label `isactive:"yes"` aux pods avant de changer le daemon set!

- Cela peut √™tre fait par programme avec `kubectl patch`:

  ```bash
    PATCH='
    metadata:
      labels:
        isactive: "yes"
    '
    kubectl get pods -l run=rng -l controller-revision-hash -o name |
      xargs kubectl patch -p "$PATCH" 
  ```


.debug[[kube/daemonset_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/daemonset_fr.md)]
---

## Labels et d√©bogage

- Quand un pod se comporte mal, on peut le supprimer: un autre sera recr√©√©

- Mais on peut aussi changer ses labels

- Il sera supprim√© de l'√©quilibreur de charge (load balancerr) (il ne recevra plus de trafic)

- Un autre pod sera recr√©√© imm√©diatement

- Mais le pod probl√©matique est toujours l√†, et nous pouvons l'inspecter et le d√©boguer

- Nous pouvons m√™me le rajouter √† la rotation si n√©cessaire

  (Tr√®s utile pour r√©soudre les bugs intermittents et insaisissables)

.debug[[kube/daemonset_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/daemonset_fr.md)]
---

## Labels et contr√¥le de rollout avanc√©

- Inversement, nous pouvons ajouter des pods correspondant au selector d'un service

- Ces pods recevront alors des requ√™tes et serviront le trafic

- Exemples:

  - pod One-shot avec tous les flags de d√©bogage activ√©, pour collecter des logs

  - pods cr√©√©s automatiquement, mais ajout√©s √† la rotation dans une seconde √©tape
    <br/>
    (en d√©finissant leur label en cons√©quence)

- Cela nous donne des blocs de construction pour les "canary" et "blue/green deployments"



.debug[[kube/daemonset_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/daemonset_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/distillery-containers.jpg)]

---

name: toc-rolling-updates
class: title

Rolling updates

.nav[
[Section pr√©c√©dente](#toc-mise--jour-dun-service-via-des-labels-et-des-selectors)
|
[Retour table des mati√®res](#toc-chapter-2)
|
[Section suivante](#toc-accs-aux-logs-depuis-le-cli)
]

.debug[(automatically generated title slide)]

---
# Rolling updates

- Rolling update est la mise √† jour continue

- Par d√©faut (sans le rolling update), lorsqu'une ressource qui passe l'echelle est mise √† jour:

  - De nouveaux pods sont cr√©√©s

  - les anciennes pods sont termin√©es
  
  - ... Tout en m√™me temps
  
  - Si quelque chose ne va pas, ¬Ø\\\_(„ÉÑ)\_/¬Ø

.debug[[kube/rollout_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/rollout_fr.md)]
---

## Rolling updates

- Avec les 'rolling updates', lorsqu'une ressource est mise √† jour, ca se fait progressivement

- Deux param√®tres d√©terminent le rythme du rollout: `maxUnavailable` et `maxSurge`

- Ils peuvent √™tre sp√©cifi√©s en nombre absolu de pods, ou en pourcentage du nombre de replicas

- N'importe quand ...

  - il y aura toujours au moins des `replicas`-`maxUnavailable` pods disponibles

  - il n'y aura jamais plus que des `replicas`+`maxSurge` pods au total

  - il y aura donc jusqu'√†  `maxUnavailable`+`maxSurge` pods qui sont mises a jour

- Nous avons la possibilit√© de revenir √† la version pr√©c√©dente (rollback)
  <br/> (si la mise √† jour √©choue ou est insatisfaisante)

.debug[[kube/rollout_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/rollout_fr.md)]
---

## V√©rification des param√®tres de d√©ploiement actuels

- Rappelez-vous comment nous construisons des rapports personnalis√©s avec `kubectl` et `jq`:

.exercise[

- Afficher le plan de rollout pour nos deployments:
  ```bash
    kubectl get deploy -o json |
            jq ".items[] | {name:.metadata.name} + .spec.strategy.rollingUpdate"
  ```

]

.debug[[kube/rollout_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/rollout_fr.md)]
---


## Rolling update en pratique

- √Ä partir de Kubernetes 1.8, nous pouvons faire des rolling update avec:

  `deployments`, `daemonsets`, `statefulsets`

- La modification de l'une de ces ressources entra√Ænera automatiquement une mise √† jour progressive

- Les rolling updates peuvent √™tre surveill√©es avec la sous-commande `kubectl rollout`

.debug[[kube/rollout_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/rollout_fr.md)]
---

## Construire une nouvelle version du service `worker`

.exercise[

- Allez dans le r√©pertoire `stacks`:
  ```bash
  cd ~/container.training/stacks
  ```

- Editez `dockercoins/worker/worker.py`, mettez √† jour la ligne `sleep` pour dormir 1 seconde

- Construire un nouveau tag et le pousser dans le registry:
  ```bash
  #export REGISTRY=localhost:3xxxx
  export TAG=v0.2
  docker-compose -f dockercoins.yml build
  docker-compose -f dockercoins.yml push
  ```

]

.debug[[kube/rollout_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/rollout_fr.md)]
---

## Rolling out du nouveau service `worker`

.exercise[

- Surveillons ce qui se passe en ouvrant quelques terminaux, et ex√©cutons:
  ```bash
  kubectl get pods -w
  kubectl get replicasets -w
  kubectl get deployments -w
  ```

- Mettre √† jour `worker` soit avec `kubectl edit`, soit en ex√©cutant:
  ```bash
  kubectl set image deploy worker worker=$REGISTRY/worker:$TAG
  ```

]

--

Ce rollout devrait √™tre assez rapide. Que montre l'interface web?

.debug[[kube/rollout_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/rollout_fr.md)]
---

## Donnez-lui du temps

- Au d√©but, il semble que rien ne se passe (le graphique reste au m√™me niveau)

- Selon `kubectl get deploy -w`, le `deployment` a √©t√© mis √† jour tr√®s rapidement

- Mais `kubectl get pods -w` raconte une histoire diff√©rente

- Les anciens `pods` sont toujours l√†, et ils restent dans l'√©tat `Terminating` pendant un certain temps

- Finalement, ils sont termin√©s; puis le graphique diminue de mani√®re significative

- Ce retard est d√ª au fait que notre worker ne g√®re pas les signaux

- Kubernetes envoie une requ√™te d'arr√™t "polie" au worker, qui l'ignore

- Apr√®s une p√©riode de gr√¢ce, Kubernetes s'impatiente et tue le conteneur

  (La p√©riode de gr√¢ce est de 30 secondes, mais [peut √™tre modifi√©e](https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods) si n√©cessaire)

.debug[[kube/rollout_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/rollout_fr.md)]
---

## Le cas d'une erreur

- Que se passe-t-il si nous faisons une erreur?

.exercise[

- Mettre √† jour `worker` en sp√©cifiant une image inexistante:
  ```bash
  export TAG=v0.3
  kubectl set image deploy worker worker=$REGISTRY/worker:$TAG
  ```

- V√©rifiez ce qui se passe:
  ```bash
  kubectl rollout status deploy worker
  ```

]

--

Notre d√©ploiement est bloqu√©. Cependant, l'application n'est pas morte (seulement 10% plus lent).

.debug[[kube/rollout_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/rollout_fr.md)]
---

## Que se passe-t-il avec notre d√©ploiement?

- Pourquoi notre application est-elle 10% plus lente?

- Parce que `MaxUnavailable=1`, le deployment s'est termin√© 1 replica sur 10 disponible

- D'accord, mais pourquoi voyons-nous 2 nouvelles replicas en cours de deployment?

- Parce que `MaxSurge=1`, donc en plus de remplacer le "terminated", le d√©ploiement demarre un de plus

.debug[[kube/rollout_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/rollout_fr.md)]
---

class: extra-details

## Quelques d√©tails

- Nous commen√ßons avec 10 pods en cours d'ex√©cution pour le d√©ploiement `worker`

- Param√®tres actuels: MaxUnavailable=1 et MaxSurge=1

- Quand nous commen√ßons le d√©ploiement:

  - un replicas est supprim√©e (selon MaxUnavailable = 1)
  - un autre est cr√©√© (avec la nouvelle version) pour le remplacer
  - un autre est cr√©√© (avec la nouvelle version) par MaxSurge = 1

- Nous avons maintenant 9 r√©plicas en service et 2 en cours de d√©ploiement

- Notre d√©ploiement est bloqu√© √† ce stade!

.debug[[kube/rollout_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/rollout_fr.md)]
---

## R√©cup√©ration √† partir d'un mauvais d√©ploiement

- Nous pourrions pousser une image `v0.3`

  (la logique de r√©essai de pod l'attrapera finalement et le d√©ploiement se poursuivra)

- Ou nous pourrions invoquer une restauration manuelle

.exercise[

- Annuler le d√©ploiement et attendre que la poussi√®re s'installe:
  ```bash
  kubectl rollout undo deploy worker
  kubectl rollout status deploy worker
  ```

]

.debug[[kube/rollout_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/rollout_fr.md)]
---

## Modification des param√®tres de rollout

- Nous voulons:

  - revenir √† `v0.1`
  - √™tre prudent sur la disponibilit√© (toujours avoir le nombre d√©sir√© de workers disponibles)
  - √™tre agressif sur la vitesse de rollout (mettre √† jour plus d'un pod √† la fois)
  - donner un peu de temps √† nos workers pour "se r√©chauffer" avant de commencer plus

Les modifications correspondantes peuvent √™tre exprim√©es dans l'extrait YAML suivant:

.small[
```yaml
spec:
  template:
    spec:
      containers:
      - name: worker
        image: $REGISTRY/worker:v0.1
  strategy:
    rollingUpdate:
      maxUnavailable: 0
      maxSurge: 3
  minReadySeconds: 10
```
]


.debug[[kube/rollout_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/rollout_fr.md)]
---

## Application de modifications via un patch YAML

- Nous pourrions utiliser `kubectl edit deployment worker`

.debug[[kube/rollout_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/rollout_fr.md)]
---

## Application de modifications via un patch YAML

- Mais nous pourrions √©galement utiliser `kubectl patch` avec le YAML exact montr√© avant

.exercise[

.small[

- Appliquez tous nos changements et attendez qu'ils prennent effet:
  ```bash
  kubectl patch deployment worker -p "
    spec:
      template:
        spec:
          containers:
          - name: worker
            image: $REGISTRY/worker:v0.1
      strategy:
        rollingUpdate:
          maxUnavailable: 0
          maxSurge: 3
      minReadySeconds: 10
    "
  kubectl rollout status deployment worker
  kubectl get deploy -o json worker |
          jq "{name:.metadata.name} + .spec.strategy.rollingUpdate"
  ```
  ] 

]



.debug[[kube/rollout_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/rollout_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/lots-of-containers.jpg)]

---

name: toc-accs-aux-logs-depuis-le-cli
class: title

Acc√®s aux logs depuis le CLI

.nav[
[Section pr√©c√©dente](#toc-rolling-updates)
|
[Retour table des mati√®res](#toc-chapter-2)
|
[Section suivante](#toc-logs-centralise)
]

.debug[(automatically generated title slide)]

---
# Acc√®s aux logs depuis le CLI

- Les commandes `kubectl logs` ont des limitations:

  - il ne peut pas diffuser les logs de plusieurs pods √† la fois

  - lors de l'affichage des logs de plusieurs pods, il les m√©lange tous ensemble

- Nous allons voir comment le faire mieux

.debug[[kube/logs-cli_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/logs-cli_fr.md)]
---

## Le faire manuellement

- Nous *pourrions* (si nous √©tions si motiv√©), √©crire un programme ou un script qui:

  - prendre un selector comme argument

  - √©num√©rer tous les pods correspondant √† ce selector (avec `kubectl get -l ...`)

  - fork un `kubectl logs --follow ...` commande par conteneur

  - annoter les logs (le r√©sultat de chaque processus `kubectl logs ...`) avec leur origine

  - conserver la commande en utilisant `kubectl logs --timestamps ...` et fusionner la sortie

--

- Nous *pourrions* le faire, mais heureusement, d'autres l'ont d√©j√† fait pour nous!

.debug[[kube/logs-cli_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/logs-cli_fr.md)]
---

## Stern

[Stern](https://github.com/wercker/stern) est un projet open source
par [Wercker](http://www.wercker.com/).

√Ä partir du fichier README:

*Stern vous permet d'empiler plusieurs pods sur Kubernetes et plusieurs conteneurs dans le pod. Chaque r√©sultat est cod√© en couleur pour un d√©bogage plus rapide.*

*La requ√™te est une expression r√©guli√®re, de sorte que le nom du pod peut facilement √™tre filtr√© et que vous n'avez pas besoin de sp√©cifier l'identifiant exact (par exemple en omettant l'ID de d√©ploiement). Si un pod est supprim√©, il est retir√© de la queue et si un nouveau pod est ajout√©, il est automatiquement suivi.*

Exactement ce dont nous avons besoin!

.debug[[kube/logs-cli_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/logs-cli_fr.md)]
---

## Installation de Stern

- Pour simplifier, prenons simplement une version binaire

.exercise[

- T√©l√©charger une version binaire de GitHub:
  ```bash
  sudo curl -L -o /usr/local/bin/stern \
       https://github.com/wercker/stern/releases/download/1.6.0/stern_linux_amd64
  sudo chmod +x /usr/local/bin/stern
  ```


]

Ces instructions d'installation fonctionneront sur nos clusters, car ce sont des VM Linux amd64.

.debug[[kube/logs-cli_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/logs-cli_fr.md)]
---

## Utilisation de Stern

- Il existe deux fa√ßons de sp√©cifier les pods pour lesquels nous voulons voir les logs:

  - `-l` suivi d'une expression de s√©lection (comme avec de nombreuses commandes `kubectl`)

  - avec une "requ√™te de pod", c'est-√†-dire une expression rationnelle (regex) utilis√©e pour faire correspondre les noms de pod

- Ces deux voies peuvent √™tre combin√©es si n√©cessaire

.exercise[

- Voir les logs pour tous les conteneurs rng:
  ```bash
  stern rng
  ```

]

.debug[[kube/logs-cli_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/logs-cli_fr.md)]
---

## Options pratiques Stern

- Le flag `--tail N` montre les derni√®res lignes `N` pour chaque conteneur

  (Au lieu d'afficher les logs depuis la cr√©ation du conteneur)

- Le flag  `-t`/`--timestamps` montre les timestamps

- Le flag `--all-namespaces` est explicite

.exercise[

- Voir ce qui se passe avec les conteneurs du syst√®me `weave`:
  ```bash
  stern --tail 1 --timestamps --all-namespaces weave
  ```
]

.debug[[kube/logs-cli_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/logs-cli_fr.md)]
---

## Utiliser Stern avec un selector

- Lorsque vous sp√©cifiez un selector, nous pouvons omettre la valeur d'un label

- Cela va correspondre √† tous les objets ayant cette label (quelle que soit la valeur)

- Tout ce qui a √©t√© cr√©√© avec `kubectl run` a une √©tiquette `run`

- Nous pouvons utiliser cette propri√©t√© pour voir les logs de tous les pod cr√©√©s avec `kubectl run`

.exercise[

- Voir les logs pour toutes les choses commenc√©es avec `kubectl run`:
  ```bash
  stern -l run
  ```

.debug[[kube/logs-cli_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/logs-cli_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/plastic-containers.JPG)]

---

name: toc-logs-centralise
class: title

Logs centralis√©e

.nav[
[Section pr√©c√©dente](#toc-accs-aux-logs-depuis-le-cli)
|
[Retour table des mati√®res](#toc-chapter-2)
|
[Section suivante](#toc-gestion-des-stacks-avec-helm)
]

.debug[(automatically generated title slide)]

---
# Logs centralis√©e

- L'utilisation de `kubectl` ou` stern` est simple; mais il a des inconv√©nients:

  - quand un n≈ìud tombe en panne, ses logs ne sont plus disponibles

  - nous ne pouvons que vider ou diffuser des logs; nous voulons rechercher / indexer / compter ...

- Nous voulons envoyer tous nos logs √† un seul endroit

- Nous voulons les analyser (par exemple pour les logs HTTP) et les indexer

- Nous voulons un bon dashboard web

--

- Nous allons d√©ployer une stack EFK

.debug[[kube/logs-centralized_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/logs-centralized_fr.md)]
---

## Qu'est-ce que EFK?

- EFK est trois composants:

  - ElasticSearch (pour stocker et indexer les entr√©es de log)

  - Fluentd (pour obtenir les logs de conteneur, les traiter et les placer dans ElasticSearch)

  - Kibana (pour voir/rechercher les entr√©es de log avec une belle interface)

- Le seul composant auquel nous devons acc√©der depuis l'ext√©rieur du cluster sera Kibana

.debug[[kube/logs-centralized_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/logs-centralized_fr.md)]
---

## D√©ploiement de EFK sur notre cluster

- Nous allons utiliser un fichier YAML d√©crivant toutes les ressources n√©cessaires

.exercise[

- Chargez le fichier YAML dans notre cluster:
  ```bash
  kubectl apply -f https://goo.gl/MUZhE4
  ```


]

Si nous [regardons le fichier YAML](https://goo.gl/MUZhE4), nous voyons que
il cr√©e 1 daemon set, 2 deployments, 2 services,
et quelques roles et roles bindings (pour donner √† fluentd les permissions requises).

.debug[[kube/logs-centralized_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/logs-centralized_fr.md)]
---

## L'itin√©raire d'une ligne de log (avant Fluentd)

- Un conteneur √©crit une ligne sur stdout ou stderr

- Les deux sont g√©n√©ralement redirig√©s vers le moteur du conteneur (Docker ou autre)

- Le moteur de conteneur lit la ligne et l'envoie √† un pilote de logs

- Le timestamp et le flux (stdout ou stderr) sont ajout√©s √† la ligne du log

- Avec la configuration par d√©faut pour Kubernetes, la ligne est √©crite dans un fichier JSON

  (`/var/log/containers/pod-name_namespace_container-id.log`)  

- Ce fichier est lu lorsque nous invoquons `kubectl logs`; nous pouvons y acc√©der directement aussi

.debug[[kube/logs-centralized_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/logs-centralized_fr.md)]
---

## L'itin√©raire d'une ligne de log (avec Fluentd)

- Fluentd s'ex√©cute sur chaque noeud (gr√¢ce √† un daemon set)

- Il bind-mount `/var/log/containers` de l'h√¥te (pour acc√©der √† ces fichiers)

- Il scanne en permanence ce r√©pertoire pour les nouveaux fichiers; les lit; les analyse

- Chaque ligne de log devient un objet JSON, enti√®rement annot√© avec des informations suppl√©mentaires:
  <br/> container id, pod name, Kubernetes labels ...

- Ces objets JSON sont stock√©s dans ElasticSearch

- ElasticSearch indexe les objets JSON

- Nous pouvons acc√©der aux logs via Kibana (et effectuer des recherches, des comptes, etc.)

.debug[[kube/logs-centralized_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/logs-centralized_fr.md)]
---

## Acc√®s √† Kibana

- Kibana offre une interface web relativement simple

- Regardons √ßa!

.exercise[

- V√©rifiez quel `NodePort` a √©t√© allou√© √† Kibana:
  ```bash
  kubectl get svc kibana
  ```

- Avec notre navigateur Web, connectez-vous √† Kibana

]

.debug[[kube/logs-centralized_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/logs-centralized_fr.md)]
---

## Utiliser Kibana

* Remarque: ce n'est pas un atelier Kibana! Donc cette section est d√©lib√©r√©ment tr√®s laconique. *

- La premi√®re fois que vous vous connectez √† Kibana, vous devez "configure an index pattern"

- Il suffit d'utiliser celui qui est sugg√©r√©, `@timestamp`

- Puis cliquez sur "Discover" (dans le coin sup√©rieur gauche)

- Vous devriez voir les logs de conteneurs

- Conseil: dans la colonne de gauche, s√©lectionnez quelques champs √† afficher, par exemple:

  `kubernetes.host`,` kubernetes.pod_name`, `stream`,` log`

.debug[[kube/logs-centralized_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/logs-centralized_fr.md)]
---

## Caveat emptor

Nous utilisons EFK parce que c'est relativement simple
d√©ployer sur Kubernetes, sans devoir red√©ployer ou reconfigurer
notre cluster. Mais cela ne signifie pas que ce sera toujours le meilleur
option pour votre cas d'utilisation. Si vous utilisez Kubernetes dans le
cloud, vous pouvez envisager d'utiliser la journalisation du fournisseur de cloud
infrastructure (si elle peut √™tre int√©gr√©e avec Kubernetes).

La m√©thode de d√©ploiement que nous allons utiliser ici a √©t√© simplifi√©e:
il n'y a qu'un seul noeud ElasticSearch. Dans un vrai d√©ploiement, vous
pourrait utiliser un cluster, √† la fois pour des raisons de performance et de fiabilit√©.
Mais ceci sort du cadre de ce chapitre.

Le fichier YAML que nous avons utilis√© cr√©e toutes les ressources dans le
espace de noms `default`, pour plus de simplicit√©. Dans un sc√©nario r√©el, vous allez
cr√©ez les ressources dans l'espace de noms `kube-system` ou dans un espace de noms d√©di√©.

.debug[[kube/logs-centralized_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/logs-centralized_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/train-of-containers-1.jpg)]

---

name: toc-gestion-des-stacks-avec-helm
class: title

Gestion des stacks avec Helm

.nav[
[Section pr√©c√©dente](#toc-logs-centralise)
|
[Retour table des mati√®res](#toc-chapter-3)
|
[Section suivante](#toc-namespaces)
]

.debug[(automatically generated title slide)]

---
# Gestion des stacks avec Helm

- Nous avons cr√©√© nos premi√®res ressources avec `kubectl run`,` kubectl expose` ...

- Nous avons √©galement cr√©√© des ressources en chargeant les fichiers YAML avec `kubectl apply -f`

- Pour les plus grosses stacks, g√©rer des milliers de lignes de YAML est d√©raisonnable

- Ces bundles YAML doivent √™tre personnalis√©s avec des param√®tres variables

  (Par exemple: nombre de replicas, version de l'image √† utiliser ...)

- Ce serait bien d'avoir une collection de bundles (paquets) organis√©e et versionn√©e

- Ce serait bien de pouvoir upgrade/rollback ces bundles avec soin

- [Helm](https://helm.sh/) est un projet open source offrant toutes ces choses!

.debug[[kube/helm_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/helm_fr.md)]
---

## Concepts de Helm

- `helm` est un outil CLI

- `tiller` est son composant c√¥t√© serveur

- Un "chart" est une archive contenant des paquets YAML mod√©lis√©s

- Les charts sont versionn√©s

- Les charts peuvent √™tre stock√©s sur des repositories priv√©s ou publics

.debug[[kube/helm_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/helm_fr.md)]
---

## Installation de Helm

- Nous devons installer la CLI `helm`; puis utilisez-le pour d√©ployer  `tiller`

.exercise[

- Installez la CLI `helm`:
  ```bash
  curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get > get_helm.sh
  chmod 700 get_helm.sh
  ./get_helm.sh
  ```

- D√©ployer `tiller`:
  ```bash
  helm init
  ```

- Ajouter l'ach√®vement `helm`:
  ```bash
  . <(helm completion $(basename $SHELL))
  ```

]

.debug[[kube/helm_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/helm_fr.md)]
---

## R√©parer les "account permissions"

- Le mod√®le d'autorisation Helm nous oblige √† modifier les autorisations (permissions)

- Dans un d√©ploiement plus r√©aliste, vous pouvez cr√©er par utilisateur ou par √©quipe
  comptes de service (service accounts), roles, et liaisons de r√¥les (role bindings)

.exercise[

- Accorder le r√¥le `cluster-admin` au compte de service `kube-system: default`:
  ```bash
    kubectl create clusterrolebinding add-on-cluster-admin \
      --clusterrole=cluster-admin --serviceaccount=kube-system:default
  ```

]

(D√©finir les r√¥les et les autorisations exacts sur votre cluster n√©cessite
une connaissance plus approfondie du mod√®le RBAC de Kubernetes. La commande ci-dessus est
amende pour les clusters personnels et de d√©veloppement.)

.debug[[kube/helm_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/helm_fr.md)]
---

## Voir les charts disponibles

- Un d√©p√¥t public est pr√©configur√© lors de l'installation de Helm

- Nous pouvons voir les charts disponibles avec `helm search` (et un mot-cl√© optionnel)

.exercise[

- Voir tous les charts disponibles:
  ```bash
  helm search
  ```

- Voir les charts li√©s √† `prometheus`:
  ```bash
  helm search prometheus
  ```

]

.debug[[kube/helm_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/helm_fr.md)]
---

## Installation d'un chart

- La plupart des charts utilisent les types de service `LoadBalancer` par d√©faut

- La plupart des charts n√©cessitent des volumes persistants pour stocker les donn√©es

- Nous devons d√©tendre ces exigences un peu

.exercise[

- Installez le collecteur de mesures Prometheus sur notre cluster:
  ```bash
    helm install stable/prometheus \
         --set server.service.type=NodePort \
         --set server.persistentVolume.enabled=false
  ```

]

D'o√π viennent ces options `--set`?

.debug[[kube/helm_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/helm_fr.md)]
---

## Inspection d'un chart

- `helm inspect` montre des d√©tails sur un chart (y compris les options disponibles)

.exercise[

- Voir les m√©tadonn√©es et toutes les options disponibles pour `stable/prometheus`:
  ```bash
  helm inspect stable/prometheus
  ```

]

Les m√©tadonn√©es du chart incluent une URL vers la page d'accueil du projet.

(Parfois, il pointe facilement vers la documentation de la carte.)

.debug[[kube/helm_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/helm_fr.md)]
---

## Cr√©ation d'un chart

- Nous allons montrer un moyen de cr√©er un chart *tr√®s simplifi√©*

- Dans un vrai chart, *beaucoup de choses* seraient templac√©es

  (Noms de ressources, types de services, nombre de replicas ...)

.exercise[

- Cr√©er un exemple de chart:
  ```bash
  helm create dockercoins
  ```

- √âloignez les exemples de templates et cr√©ez un r√©pertoire de template vide:
  ```bash
  mv dockercoins/templates dockercoins/default-templates
  mkdir dockercoins/templates
  ```

]

.debug[[kube/helm_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/helm_fr.md)]
---

## Exportation du YAML pour notre application

- La section suivante suppose que DockerCoins est en cours d'ex√©cution

.exercise[

- Cr√©ez un fichier YAML pour chaque ressource dont nous avons besoin:
  .small[
  ```bash

    while read kind name; do
      kubectl get -o yaml --export $kind $name > dockercoins/templates/$name-$kind.yaml
    done <<EOF
    deployment worker
    deployment hasher
    daemonset rng
    deployment webui
    deployment redis
    service hasher
    service rng
    service webui
    service redis
    EOF
  ```
  ]

]

.debug[[kube/helm_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/helm_fr.md)]
---

## Test de notre chart

.exercise[

- Installons notre helm chart! (`dockercoins` est le chemin vers le chart)
  ```bash
  helm install dockercoins
  ```
]

-

- Puisque l'application est d√©j√† d√©ploy√©e, cela √©chouera: <br>
`Error: release loitering-otter failed: services "hasher" already exists`

- Pour √©viter les conflits de noms, nous allons d√©ployer l'application dans un autre *namespace*

.debug[[kube/helm_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/helm_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/train-of-containers-2.jpg)]

---

name: toc-namespaces
class: title

Namespaces

.nav[
[Section pr√©c√©dente](#toc-gestion-des-stacks-avec-helm)
|
[Retour table des mati√®res](#toc-chapter-3)
|
[Section suivante](#toc-)
]

.debug[(automatically generated title slide)]

---
# Namespaces

- Nous ne pouvons pas avoir deux ressources avec le m√™me nom

  (Ou pouvons-nous ...?)

--

- Nous ne pouvons pas avoir deux ressources *du m√™me type* avec le m√™me nom

  (Mais c'est bien d'avoir un service `rng`, un deploiement `rng`, et un daemon set `rng`!)

--

- Nous ne pouvons pas avoir deux ressources du m√™me type avec le m√™me nom *dans le m√™me namespace*

  (Mais c'est OK d'avoir par exemple deux services `rng` dans diff√©rents namespaces!)

--

- En d'autres termes: **le tuple *(type, name, namespace)* doit √™tre unique**

  (Dans la ressource YAML, le type s'appelle `Kind`)

.debug[[kube/namespaces_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/namespaces_fr.md)]
---

## Namespaces pr√©existants

- Si nous d√©ployons un cluster avec `kubeadm`, nous avons trois namespaces:

  - `default` (pour nos applications)

  - `kube-system` (pour le control plane)

  - `kube-public` (contient un secret utilis√© pour la d√©couverte du cluster)

- Si nous d√©ployons diff√©remment, nous pouvons avoir diff√©rents namespaces

.debug[[kube/namespaces_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/namespaces_fr.md)]
---

## Cr√©ation des namespaces

- Nous pouvons cr√©er des namespaces avec un YAML tr√®s minime, par exemple:
  ```bash
    kubectl apply -f- <<EOF
    apiVersion: v1
    kind: Namespace
    metadata:
      name: blue
    EOF
  ```

- Si nous utilisons un outil comme Helm, il cr√©era automatiquement des namespaces

.debug[[kube/namespaces_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/namespaces_fr.md)]
---

## Utiliser les namespaces

- Nous pouvons passer un flag `-n` ou `--namespace` √† la plupart des commandes `kubectl`:
  ```bash
  kubectl -n blue get svc
  ```

- Nous pouvons √©galement utiliser *contexts*

- Un context est un tuple *(user, cluster, namespace)* 

- Nous pouvons manipuler les contexts avec la commande `kubectl config`

.debug[[kube/namespaces_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/namespaces_fr.md)]
---

## Cr√©er un context

- Nous allons cr√©er un context pour le namespace `blue`

.exercise[

- Afficher les contexts existants pour voir le nom du cluster et l'utilisateur actuel:
  ```bash
  kubectl config get-contexts
  ```

- Cr√©er un nouveau context:
  ```bash
  kubectl config set-context blue --namespace=blue \
      --cluster=kubernetes --user=kubernetes-admin
  ```

]

Nous avons cr√©√© un context; mais c'est juste quelques valeurs de configuration.

Le namespace n'existe pas encore.

.debug[[kube/namespaces_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/namespaces_fr.md)]
---

## Utiliser un contexte

- Passons √† notre nouveau context et d√©ployons le graphique DockerCoins

.exercise[

- Utilisez le context `blue`:
  ```bash
  kubectl config use-context blue
  ```

- D√©ployer DockerCoins:
  ```bash
  helm install dockercoins
  ```

]

Dans la derni√®re ligne de commande, `dockercoins` est juste le chemin local o√π
nous avons cr√©√© notre Helm chart avant.

.debug[[kube/namespaces_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/namespaces_fr.md)]
---

## Affichage de l'application d√©ploy√©e

- Voyons voir si notre carte Helm a fonctionn√© correctement!

.exercise[

- R√©cup√©rer le num√©ro de port attribu√© au service `webui`:
  ```bash
  kubectl get svc webui
  ```

- Pointez notre navigateur sur http://X.X.X.X:8080

]

Remarque: l'application peut prendre une minute ou deux pour √™tre op√©rationnelle.

.debug[[kube/namespaces_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/namespaces_fr.md)]
---

## Namespaces et isolation

- Les namespaces *ne fournissent pas* d'isolation

- Un pod dans le namespace `green` peut communiquer avec un pod dans le namespace `blue`

- Un pod dans le namespace `default` peut communiquer avec un pod dans le namespace `kube-system`

- `kube-dns` utilise un sous-domaine diff√©rent pour chaque namespace

- Exemple: depuis n'importe quel pod du cluster, vous pouvez vous connecter √† l'API Kubernetes avec:

  `https://kubernetes.default.svc.cluster.local:443/`

.debug[[kube/namespaces_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/namespaces_fr.md)]
---

## Isolation de Pods

- L'isolation r√©elle est impl√©ment√©e avec les *network policies* (politiques de r√©seau)

- Les network policies sont des ressources (comme des deployments, services, namespaces ...)

- Les network policies sp√©cifient les flux autoris√©s:

  - entre les pods

  - des pods au monde ext√©rieur

  - et vice versa

.debug[[kube/namespaces_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/namespaces_fr.md)]
---

## Pr√©sentation des network policies

- Nous pouvons cr√©er autant de politiques de r√©seau *network policies* que nous voulons

- Chaque network policy a:

  - un *pod selector*: "quels pods sont cibl√©s par la politique?"

  - des listes de r√®gles d'entr√©e (ingress) et/ou de sortie (egress): "quels peers et quels ports sont autoris√©s ou bloqu√©s?"

- Si un pod n'est pas cibl√© par aucune politique, le trafic est autoris√© par d√©faut

- Si un pod est cibl√© par au moins une politique, le trafic doit √™tre autoris√© explicitement

.debug[[kube/namespaces_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/namespaces_fr.md)]
---

## En savoir plus sur les network policies

- Cela reste un aper√ßu de haut niveau des network policies

- Pour plus de d√©tails regardez:

  - la [documentation de Kubernetes sur les network policies](https://kubernetes.io/docs/concepts/services-networking/network-policies/)

  - ceci [presentation de network policies chez KubeCon 2017 US](https://www.youtube.com/watch?v=3gGpMmYeEO8) par [@ahmetb](https://twitter.com/ahmetb)

.debug[[kube/namespaces_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/namespaces_fr.md)]</textarea>
    <script src="remark.min.js" type="text/javascript">
    </script>
    <script type="text/javascript">
      var slideshow = remark.create({
        ratio: '16:9',
        highlightSpans: true,
        excludedClasses: ["self-paced"]
      });
    </script>
    
    <!-- 
    These two scripts will be available only when loading the
    content using the pub/sub server. Otherwise, they'll just
    404 and that's OK.
    -->
    <script src="/socket.io/socket.io.js">
    </script>
    <script src="/remote.js">
    </script>

  </body>
</html>
