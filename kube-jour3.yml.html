<!DOCTYPE html>
<html>
  <head>
    <title>Kubernetes  Stockage, Configuration, Secrets et Concepts Avancés  </title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <link rel="stylesheet" href="workshop.css">
  </head>
  <body>
    <!--
    <div style="position: absolute; left: 20%; right: 20%; top: 30%;">
      <h1 style="font-size: 3em;">Loading ...</h1>
      The slides should show up here. If they don't, it might be
      because you are accessing this file directly from your filesystem.
      It needs to be served from a web server. You can try this:
      <pre>
        docker-compose up -d
        open http://localhost:8888/workshop.html # on MacOS
        xdg-open http://localhost:8888/workshop.html # on Linux
      </pre>
      Once the slides are loaded, this notice disappears when you
      go full screen (e.g. by hitting "f").
    </div>
    -->
    <textarea id="source">class: title, self-paced

Kubernetes <br/>Stockage, Configuration, Secrets et Concepts Avancés <br/>

.nav[*Self-paced version*]

.debug[
```
 M common/about-slides_fr.md
 M common/composescale_fr.md
 M index.html
 M intro-fullday.yml.html
 M intro-selfpaced.yml.html
 M kube-fullday.yml.html
 M kube-halfday.yml.html
 M kube-jour1.yml.html
 M kube-jour2.yml.html
 M kube-jour3.yml.html
 M kube-selfpaced.yml.html
 M kube/dashboard_fr.md
 M kube/ourapponkube_fr.md
 M logistics.md
 M swarm-fullday.yml.html
 M swarm-halfday.yml.html
 M swarm-selfpaced.yml.html
 M swarm-video.yml.html
?? .directory
?? common/.directory
?? intro/.directory
?? kube/.directory
?? slides/
?? swarm/.directory

```

These slides have been built from commit: 273b22a


[common/title_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/title_fr.md)]
---

class: title, in-person

Kubernetes <br/>Stockage, Configuration, Secrets et Concepts Avancés <br/><br/></br>


.debug[[common/title_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/title_fr.md)]
---

name: toc-chapter-1

## Chapter 1

- [Introducing Volumes](#toc-introducing-volumes)

- [Introducing PersistentVolumes and PersistentVolumeClaims](#toc-introducing-persistentvolumes-and-persistentvolumeclaims)

- [Dynamic Provisioning of PersistentVolumes](#toc-dynamic-provisioning-of-persistentvolumes)

- [Rook: orchestration of distributed storage](#toc-rook-orchestration-of-distributed-storage)

.debug[(auto-generated TOC)]
---
name: toc-chapter-2

## Chapter 2

- [Decoupling configuration with a ConfigMap](#toc-decoupling-configuration-with-a-configmap)

- [Introducing Secrets](#toc-introducing-secrets)

.debug[(auto-generated TOC)]
---
name: toc-chapter-3

## Chapter 3

- [Isolation and network policies](#toc-isolation-and-network-policies)

- [Deploy Jupiter on Kubernetes](#toc-deploy-jupiter-on-kubernetes)

- [Advanced scheduling with Kubernetes](#toc-advanced-scheduling-with-kubernetes)

- [Autoscaling with Kubernetes](#toc-autoscaling-with-kubernetes)

- [Big Data analytics on Kubernetes](#toc-big-data-analytics-on-kubernetes)

.debug[(auto-generated TOC)]



.debug[[common/toc.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/toc.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/Container-Ship-Freighter-Navigation-Elbe-Romance-1782991.jpg)]

---

name: toc-introducing-volumes
class: title

Introducing Volumes

.nav[
[Section précédente](#toc-)
|
[Retour table des matières](#toc-chapter-1)
|
[Section suivante](#toc-introducing-persistentvolumes-and-persistentvolumeclaims)
]

.debug[(automatically generated title slide)]

---
# Introducing Volumes

- Kubernetes volumes are a component of a pod and are thus defined in the pod’s specification—much like containers. 

- They aren’t a standalone Kubernetes object and cannot be created or deleted on their own. 

- A volume is available to all containers in the pod, but it must be mounted in each container that needs to access it. 

- In each container, you can mount the volume in any location of its filesystem.

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

## Explaining volumes in an example 

- Containers no common storage

- Containers sharing 2 volumes mounted in different mount paths

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

class: pic

![haha seulement blague](images/volumes1.png)

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

class: pic

![haha seulement blague](images/volume2.png)

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---


## Note

- The volume /var/logs is not mounted in the ContentAgent container. 

- The container cannot access its files, even though the container and the volume are part of the same pod. 

- It’s not enough to define a volume in the pod; you need to define a VolumeMount inside the container’s spec also, if you want the container to be able to access it.

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

## Volume Types

- A wide variety of volume types is available. Several are generic, while others are specific to the actual storage technologies used underneath. 

* `emptyDir`: A simple empty directory used for storing transient data.
* `hostPath`: Used for mounting directories from the worker node’s filesystem into the pod.
* `gitRepo`: A volume initialized by checking out the contents of a Git repository.
* `nfs`: An NFS share mounted into the pod.
* `gcePersistentDisk`, `awsElasticBlockStore`, `azureDisk`: Used for mounting cloud provider-specific storage.
* `cinder`, `cephfs`, ...: Used for mounting other types of network storage.
* `configMap`, `secret`, `downwardAPI`: Special types of volumes used to expose certain Kubernetes resources and cluster information to the pod.
* `persistentVolumeClaim`: A way to use a pre- or dynamically provisioned persistent storage. 

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

## Note

- A single pod can use multiple volumes of different types at the same time

- Each of the pod’s containers can either have the volume mounted or not.

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

## Example a pod using gitrepo volume

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

.exercise[
  ```bash
apiVersion: v1
kind: Pod
metadata:
  name: gitrepo-volume-pod
spec:
  containers:
  - image: nginx:alpine
    name: web-server
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html
      readOnly: true
    ports:
    - containerPort: 80
      protocol: TCP
  volumes:
  - name: html
    gitRepo:
      repository: https://github.com/luksa/kubia-website-example.git
      revision: master
      directory: .   
  ```
]

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

## Decoupling pods from the underlying storage technology

- Above case is  against the basic idea of Kubernetes, which aims to hide the actual infrastructure from both the application and its developer.

- When a developer needs a certain amount of persistent storage for their application, they should request it from Kubernetes. 

- The same way they request CPU, memory, and other resources when creating a pod. 

- The system administrator can configure the cluster so it can give the apps what they request.

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/ShippingContainerSFBay.jpg)]

---

name: toc-introducing-persistentvolumes-and-persistentvolumeclaims
class: title

Introducing PersistentVolumes and PersistentVolumeClaims

.nav[
[Section précédente](#toc-introducing-volumes)
|
[Retour table des matières](#toc-chapter-1)
|
[Section suivante](#toc-dynamic-provisioning-of-persistentvolumes)
]

.debug[(automatically generated title slide)]

---

# Introducing PersistentVolumes and PersistentVolumeClaims

- Instead of the developer adding a technology-specific volume to their pod, it’s the cluster administrator who sets up the underlying storage and then registers it in
Kubernetes by creating a PersistentVolume resource through the Kubernetes API server. 

- When creating the PersistentVolume, the admin specifies its size and the access
modes it supports.

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

## Introducing PersistentVolumes and PersistentVolumeClaims

- When a cluster user needs to use persistent storage in one of their pods, they first create a PersistentVolumeClaim manifest, specifying the minimum size and the access
mode they require. 

- The user then submits the PersistentVolumeClaim manifest to the Kubernetes API server, and Kubernetes finds the appropriate PersistentVolume and binds the volume to the claim.

- The PersistentVolumeClaim can then be used as one of the volumes inside a pod. Other users cannot use the same PersistentVolume until it has been released by deleting
the bound PersistentVolumeClaim.

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

## Example of PersistentVolumes and PersistentVolumeClaims

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

class: pic

![haha seulement blague](images/volumes3.png)

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

## PersistentVolumes and Namespaces

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

class: pic

![haha seulement blague](images/Volume4.png)

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

##  Lifespan of PersistentVolume and PersistentVolumeClaims

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

class: pic

![haha seulement blague](images/Volume5.png)

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/aerial-view-of-containers.jpg)]

---

name: toc-dynamic-provisioning-of-persistentvolumes
class: title

Dynamic Provisioning of PersistentVolumes

.nav[
[Section précédente](#toc-introducing-persistentvolumes-and-persistentvolumeclaims)
|
[Retour table des matières](#toc-chapter-1)
|
[Section suivante](#toc-rook-orchestration-of-distributed-storage)
]

.debug[(automatically generated title slide)]

---
# Dynamic Provisioning of PersistentVolumes

- We have seen how using PersistentVolumes and PersistentVolumeClaims makes it easy to obtain persistent storage without the developer having to deal with the actual storage
technology used underneath. 

- But this still requires a cluster administrator to provision the actual storage up front. 

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

## Dynamic Provisioning of PersistentVolumes

- Luckily, Kubernetes can also perform this job automatically through dynamic provisioning of PersistentVolumes.

- The cluster admin, instead of creating PersistentVolumes, can deploy a PersistentVolume provisioner and define one or more StorageClass objects to let users choose what type of PersistentVolume they want. 

- The users can refer to the StorageClass in their PersistentVolumeClaims and the provisioner will take that into account when provisioning the persistent storage. 

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

class: pic

![haha seulement blague](images/volume6.png)

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/blue-containers.jpg)]

---

name: toc-rook-orchestration-of-distributed-storage
class: title

Rook: orchestration of distributed storage

.nav[
[Section précédente](#toc-dynamic-provisioning-of-persistentvolumes)
|
[Retour table des matières](#toc-chapter-1)
|
[Section suivante](#toc-decoupling-configuration-with-a-configmap)
]

.debug[(automatically generated title slide)]

---

# Rook: orchestration of distributed storage

- Rook is an open source orchestrator for distributed storage systems.

- Rook turns distributed storage software into a self-managing, self-scaling, and self-healing storage services. 

- It does this by automating deployment, bootstrapping, configuration, provisioning, scaling, upgrading, migration, disaster recovery, monitoring, and resource management. 

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---
## Rook: orchestration of distributed storage
- Rook is focused initially on orchestrating Ceph on-top of Kubernetes. Ceph is a distributed storage system that provides file, block and object storage and is deployed in large scale production clusters. 

- Rook is hosted by the Cloud Native Computing Foundation (CNCF) as an inception level project.

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

## Example of dynamic provisioning of PersistentVolumes using Rook

.exercise[
  ```bash
   git clone https://github.com/rook/rook.git
   cd rook/cluster/examples/kubernetes/ceph
   kubectl create -f operator.yaml
   kubect create -f cluster.yaml
  ```
- check to see everything is running as expected
 ```bash
   kubectl get pods -n rook-ceph
  ```

]

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

## Example of dynamic provisioning of PersistentVolumes using Rook

- Block storage allows you to mount storage to a single pod. 

- Let's see how to build a simple, multi-tier web application on Kubernetes using persistent volumes enabled by Rook.

--

- Before Rook can start provisioning storage, a StorageClass and its storage pool need to be created. 

- This is needed for Kubernetes to interoperate with Rook for provisioning persistent volumes.

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

## Example of dynamic provisioning of PersistentVolumes using Rook

.exercise[
- Save this storage class definition part as pool.yaml:
   ```bash
   apiVersion: ceph.rook.io/v1alpha1
   kind: Pool
   metadata:
     name: replicapool
     namespace: rook-ceph
   spec:
     replicated:
       size: 3
   ```
]

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---


## Example of dynamic provisioning of PersistentVolumes using Rook

.exercise[
- Save this storage class definition part as storageclass.yaml:
   ```bash
   apiVersion: storage.k8s.io/v1
   kind: StorageClass
   metadata:
     name: rook-ceph-block
   provisioner: ceph.rook.io/block
   parameters:
      pool: replicapool
      #The value of "clusterNamespace" MUST be the same as the one in which your rook cluster exist
      clusterNamespace: rook-ceph
   ```
]

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

## Example of dynamic provisioning of PersistentVolumes using Rook

.exercise[
- Create the pool and storage class:
  ```bash
  kubectl create -f pool.yaml
  kubectl create -f storageclass.yaml
  ```
]
- Consume the storage with wordpress sample
- We create a sample app to consume the block storage provisioned by Rook with the classic wordpress and mysql apps. 
- Both of these apps will make use of block volumes provisioned by Rook.

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

## Example of dynamic provisioning of PersistentVolumes using Rook


.exercise[
- Start mysql and wordpress from the cluster/examples/kubernetes folder:
  ```bash
kubectl create -f mysql.yaml
kubectl create -f wordpress.yaml
```
- Both of these apps create a block volume and mount it to their respective pod. You can see the Kubernetes volume claims by running the following:

 ```bash
kubectl get pvc
```
- You should see something like this:
```bash
NAME             STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE
mysql-pv-claim   Bound     pvc-95402dbc-efc0-11e6-bc9a-0cc47a3459ee   20Gi       RWO           1m
wp-pv-claim      Bound     pvc-39e43169-efc1-11e6-bc9a-0cc47a3459ee   20Gi       RWO           1m
```
]
.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

## Example of dynamic provisioning of PersistentVolumes using Rook

.exercise[
- Once the wordpress and mysql pods are in the Running state, get the cluster IP of the wordpress app and enter it in your browser along with the port:

```bash
kubectl get svc wordpress
```
]
You should see the wordpress app running.

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

## Launch another example of dynamic provisioning

.exercise[

- Copy the file from here : https://github.com/zonca/jupyterhub-deploy-kubernetes-jetstream/blob/master/storage_rook/alpine-rook.yaml

- Modify it so that it fits the specification you have at your cluster and run it using:
  ```bash
  kubectl create -f alpine-rook.yaml
  ```

]

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

## Launch another example of dynamic provisioning (suite)

- It is a very small pod with Alpine Linux that creates a 2 GB volume from Rook and mounts it on /data.

- This creates a Pod with Alpine Linux that requests a Persistent Volume Claim to be mounted under /data. 

- The Persistent Volume Claim specified the type of storage and its size. 

- Once the Pod is created, it asks the Persistent Volume Claim to actually request Rook to prepare a Persistent Volume that is then mounted into the Pod.

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

## Launch another example of dynamic provisioning (suite)

- We can verify the Persistent Volumes are created and associated with the pod, check:

.exercise[
  ```bash
  kubectl get pv
  kubectl get pvc
  kubectl get logs alpine
  ```
- Get a shell in the pod with:
  ```bash
  kubectl exec -it alpine  -- /bin/sh
  ```
- Access /data/ and write some files.
- Exit the shell 
- Now delete the pod and see if you can retrieve the data you wrote.
]

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---
## Launch another example of dynamic provisioning (suite)

.exercise[
- How could have we retrieved the data in the last case?
- Let's change the alpine-rook.yaml to `kind:deployment` write some files and kill again the pod to see what happens.
]





.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/chinook-helicopter-container.jpg)]

---

name: toc-decoupling-configuration-with-a-configmap
class: title

Decoupling configuration with a ConfigMap

.nav[
[Section précédente](#toc-rook-orchestration-of-distributed-storage)
|
[Retour table des matières](#toc-chapter-2)
|
[Section suivante](#toc-introducing-secrets)
]

.debug[(automatically generated title slide)]

---
# Decoupling configuration with a ConfigMap

- The whole point of an app’s configuration is to keep the config options that vary between environments, or change frequently, separate from the application’s source
code. 

- If you think of a pod descriptor as source code for your app (it defines how to compose the individual components into a functioning system), it’s clear you should move the configuration out of the pod description.

.debug[[kube/configs_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/configs_fr.md)]
---

## Introducing ConfigMap

- Kubernetes allows separating configuration options into a separate object called a ConfigMap, which is a map containing key/value pairs with the values ranging from
short literals to full config files.

- An application doesn’t need to read the ConfigMap directly or even know that it exists. The contents of the map are instead passed to containers as either environment
variables or as files in a volume. 

.debug[[kube/configs_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/configs_fr.md)]
---

## Introducing ConfigMap

- You can define the map’s entries by passing literals to the kubectl command or you can create the ConfigMap from files stored on your disk. 

- Use a simple literal first:

.exercise[
  ```bash
  kubectl create configmap fortune-config --from-literal=sleep-interval=25
  ```

- NOTE ConfigMap keys must be a valid DNS subdomain (they may only contain alphanumeric characters, dashes, underscores, and dots). They may optionally include a leading dot.
]

.debug[[kube/configs_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/configs_fr.md)]
---

## Explaining Configmaps in an example

- Execute the example described here: https://kubernetes.io/docs/tutorials/configuration/configure-redis-using-configmap/


.debug[[kube/configs_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/configs_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/container-cranes.jpg)]

---

name: toc-introducing-secrets
class: title

Introducing Secrets

.nav[
[Section précédente](#toc-decoupling-configuration-with-a-configmap)
|
[Retour table des matières](#toc-chapter-2)
|
[Section suivante](#toc-isolation-and-network-policies)
]

.debug[(automatically generated title slide)]

---

# Introducing Secrets

- Kubernetes provides a separate object called Secret. Secrets are much like ConfigMaps 

- They’re also maps that hold key-value pairs. They can be used the same way as a ConfigMap. 

- You can Pass Secret entries to the container as environment variables

- Expose Secret entries as files in a volume

.debug[[kube/configs_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/configs_fr.md)]
---

## Introducing Secrets

- Kubernetes helps keep your Secrets safe by making sure each Secret is only distributed
to the nodes that run the pods that need access to the Secret. 

- Also, on the nodes themselves, Secrets are always stored in memory and never written to physical storage,
which would require wiping the disks after deleting the Secrets from them.

.debug[[kube/configs_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/configs_fr.md)]
---
## Introducing Secrets

- On the master node itself etcd stores Secrets in encrypted form, making the system much more secure. Because of this, it’s imperative you properly
choose when to use a Secret or a ConfigMap. Choosing between them is simple:

 * Use a ConfigMap to store non-sensitive, plain configuration data.
--

 * Use a Secret to store any data that is sensitive in nature and needs to be kept under key. If a config file includes both sensitive and not-sensitive data, you
should store the file in a Secret.

.debug[[kube/configs_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/configs_fr.md)]
---

## Exercises using Secrets

- Some initial exercises using Secrets can be found here: https://kubernetes.io/docs/concepts/configuration/secret/

.debug[[kube/configs_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/configs_fr.md)]
---









.debug[[kube/configs_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/configs_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/container-housing.jpg)]

---

name: toc-isolation-and-network-policies
class: title

Isolation and network policies

.nav[
[Section précédente](#toc-introducing-secrets)
|
[Retour table des matières](#toc-chapter-3)
|
[Section suivante](#toc-deploy-jupiter-on-kubernetes)
]

.debug[(automatically generated title slide)]

---
# Isolation and network policies

- Namespaces *do not* provide isolation

- A pod in the `green` namespace can communicate with a pod in the `blue` namespace

- A pod in the `default` namespace can communicate with a pod in the `kube-system` namespace

- `kube-dns` uses a different subdomain for each namespace

- Example: from any pod in the cluster, you can connect to the Kubernetes API with:

  `https://kubernetes.default.svc.cluster.local:443/`

.debug[[kube/advanced_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/advanced_fr.md)]
---

## Isolating pods

- Actual isolation is implemented with *network policies*

- Network policies are resources (like deployments, services, namespaces...)

- Network policies specify which flows are allowed:

  - between pods

  - from pods to the outside world

  - and vice-versa

.debug[[kube/advanced_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/advanced_fr.md)]
---

## Network policies overview

- We can create as many network policies as we want

- Each network policy has:

  - a *pod selector*: "which pods are targeted by the policy?"

  - lists of ingress and/or egress rules: "which peers and ports are allowed or blocked?"

- If a pod is not targeted by any policy, traffic is allowed by default

- If a pod is targeted by at least one policy, traffic must be allowed explicitly

.debug[[kube/advanced_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/advanced_fr.md)]
---

## More about network policies

- This remains a high level overview of network policies

- For more details, check:

  - the [Kubernetes documentation about network policies](https://kubernetes.io/docs/concepts/services-networking/network-policies/)

  - this [talk about network policies at KubeCon 2017 US](https://www.youtube.com/watch?v=3gGpMmYeEO8) by [@ahmetb](https://twitter.com/ahmetb)

.debug[[kube/advanced_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/advanced_fr.md)]
---

## Exercises on network policies

.exercise[
- To execute some exercises on network policies we will follow some examples from here: 
  https://github.com/ahmetb/kubernetes-network-policy-recipes
]
.debug[[kube/advanced_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/advanced_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/containers-by-the-water.jpg)]

---

name: toc-deploy-jupiter-on-kubernetes
class: title

Deploy Jupiter on Kubernetes

.nav[
[Section précédente](#toc-isolation-and-network-policies)
|
[Retour table des matières](#toc-chapter-3)
|
[Section suivante](#toc-advanced-scheduling-with-kubernetes)
]

.debug[(automatically generated title slide)]

---

# Deploy Jupiter on Kubernetes

.exercise[
- We will follow the procedure provided here: 
  https://zonca.github.io/2017/12/scalable-jupyterhub-kubernetes-jetstream.html
]

.debug[[kube/advanced_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/advanced_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/distillery-containers.jpg)]

---

name: toc-advanced-scheduling-with-kubernetes
class: title

Advanced scheduling with Kubernetes

.nav[
[Section précédente](#toc-deploy-jupiter-on-kubernetes)
|
[Retour table des matières](#toc-chapter-3)
|
[Section suivante](#toc-autoscaling-with-kubernetes)
]

.debug[(automatically generated title slide)]

---
--- 

# Advanced scheduling with Kubernetes

.exercise[
- We will follow the procedure provided here: 
   https://github.com/RyaxTech/kube-tutorial#4-activate-an-advanced-scheduling-policy-and-test-its-usage
]
.debug[[kube/advanced_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/advanced_fr.md)]
---
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/lots-of-containers.jpg)]

---

name: toc-autoscaling-with-kubernetes
class: title

Autoscaling with Kubernetes

.nav[
[Section précédente](#toc-advanced-scheduling-with-kubernetes)
|
[Retour table des matières](#toc-chapter-3)
|
[Section suivante](#toc-big-data-analytics-on-kubernetes)
]

.debug[(automatically generated title slide)]

---
# Autoscaling with Kubernetes

.exercise[
- We will follow the procedure provided here: 
  https://github.com/RyaxTech/kube-tutorial#6-enable-and-use-pod-autoscaling
]
.debug[[kube/advanced_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/advanced_fr.md)]
---
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/plastic-containers.JPG)]

---

name: toc-big-data-analytics-on-kubernetes
class: title

Big Data analytics on Kubernetes

.nav[
[Section précédente](#toc-autoscaling-with-kubernetes)
|
[Retour table des matières](#toc-chapter-3)
|
[Section suivante](#toc-)
]

.debug[(automatically generated title slide)]

---
# Big Data analytics on Kubernetes

.exercise[
- We will follow the procedure provided here:
https://github.com/RyaxTech/kube-tutorial#3-execute-big-data-job-with-spark-on-the-kubernetes-cluster
]


.debug[[kube/advanced_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/advanced_fr.md)]</textarea>
    <script src="remark.min.js" type="text/javascript">
    </script>
    <script type="text/javascript">
      var slideshow = remark.create({
        ratio: '16:9',
        highlightSpans: true,
        excludedClasses: ["self-paced"]
      });
    </script>
    
    <!-- 
    These two scripts will be available only when loading the
    content using the pub/sub server. Otherwise, they'll just
    404 and that's OK.
    -->
    <script src="/socket.io/socket.io.js">
    </script>
    <script src="/remote.js">
    </script>

  </body>
</html>
