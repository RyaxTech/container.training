<!DOCTYPE html>
<html>
  <head>
    <title>Kubernetes  Stockage, Configuration, Secrets et Concepts Avancés  </title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <link rel="stylesheet" href="workshop.css">
    <link rel="stylesheet" href="override.css">
  </head>
  <body>
    <!--
    <div style="position: absolute; left: 20%; right: 20%; top: 30%;">
      <h1 style="font-size: 3em;">Loading ...</h1>
      The slides should show up here. If they don't, it might be
      because you are accessing this file directly from your filesystem.
      It needs to be served from a web server. You can try this:
      <pre>
        docker-compose up -d
        open http://localhost:8888/workshop.html # on MacOS
        xdg-open http://localhost:8888/workshop.html # on Linux
      </pre>
      Once the slides are loaded, this notice disappears when you
      go full screen (e.g. by hitting "f").
    </div>
    -->
    <textarea id="source">class: title, self-paced

Kubernetes <br/>Stockage, Configuration, Secrets et Concepts Avancés <br/>

.nav[*Self-paced version*]

.debug[
```
 M dock-kube-day1.yml.html
 M dock-kube-day2.yml.html
 M dock-kube-day3.yml.html
 M dock-kube-day4.yml.html
 M exemples.yml.html
 M intro-fullday.yml.html
 M intro-selfpaced.yml.html
 M kube-day1.yml.html
 M kube-fullday.yml.html
 M kube-halfday.yml.html
 M kube-jour1.yml.html
 M kube-jour2.yml.html
 M kube-jour3.yml
 M kube-jour3.yml.html
 M kube/exo-ab-testing/exo1.md
 M kube/exo-wordpress/tp_wordpress.md
?? .directory
?? common/.directory
?? intro/.directory
?? kube/.directory
?? kube/authn-authz.md
?? kube/gitworkflows.md
?? kube/ingress.md
?? prepare-vms/.directory
?? prepare-vms/lib/.directory
?? prepare-vms/settings/.directory
?? swarm/.directory

```

These slides have been built from commit: 0004ddb


[common/title_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/title_fr.md)]
---

class: title, in-person

Kubernetes <br/>Stockage, Configuration, Secrets et Concepts Avancés <br/><br/></br>


.debug[[common/title_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/title_fr.md)]
---

name: toc-chapter-1

## Chapter 1

- [Serverless](#toc-serverless)

.debug[(auto-generated TOC)]
---
name: toc-chapter-2

## Chapter 2

- [Introduction aux volumes](#toc-introduction-aux-volumes)

- [Introduction de PersistentVolumes et PersistentVolumeClaims](#toc-introduction-de-persistentvolumes-et-persistentvolumeclaims)

- [Provisionnement dynamique des volumes persistants](#toc-provisionnement-dynamique-des-volumes-persistants)

- [Rook orchestration de stockage distribué](#toc-rook-orchestration-de-stockage-distribu)

.debug[(auto-generated TOC)]
---
name: toc-chapter-3

## Chapter 3

- [Exercice A/B testing, la suite](#toc-exercice-ab-testing-la-suite)

.debug[(auto-generated TOC)]
---
name: toc-chapter-4

## Chapter 4

- [Découplage de configuration avec un ConfigMap](#toc-dcouplage-de-configuration-avec-un-configmap)

- [Introduction des secrets](#toc-introduction-des-secrets)

.debug[(auto-generated TOC)]
---
name: toc-chapter-5

## Chapter 5

- [Monitoring avec Prometheus et Grafana](#toc-monitoring-avec-prometheus-et-grafana)

.debug[(auto-generated TOC)]
---
name: toc-chapter-6

## Chapter 6

- [StatefulSets](#toc-statefulsets)

.debug[(auto-generated TOC)]
---
name: toc-chapter-7

## Chapter 7

- [TP Wordpress](#toc-tp-wordpress)

.debug[(auto-generated TOC)]
---
name: toc-chapter-8

## Chapter 8

- [Autoscaling avec Kubernetes](#toc-autoscaling-avec-kubernetes)

- [CI/CD with Spinnaker](#toc-cicd-with-spinnaker)

- [Déploiement de Jupiter sur Kubernetes](#toc-dploiement-de-jupiter-sur-kubernetes)

- [Scheduling avancée avec Kubernetes](#toc-scheduling-avance-avec-kubernetes)

- [Big Data analytics sur Kubernetes](#toc-big-data-analytics-sur-kubernetes)

.debug[(auto-generated TOC)]
---
name: toc-chapter-9

## Chapter 9

- [Authentication and authorization](#toc-authentication-and-authorization)

.debug[(auto-generated TOC)]
---
name: toc-chapter-10

## Chapter 10

- [Exposing HTTP services with Ingress resources](#toc-exposing-http-services-with-ingress-resources)

.debug[(auto-generated TOC)]
---
name: toc-chapter-11

## Chapter 11

- [Git-based workflows](#toc-git-based-workflows)

.debug[(auto-generated TOC)]



.debug[[common/toc.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//common/toc.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/Container-Ship-Freighter-Navigation-Elbe-Romance-1782991.jpg)]

---

name: toc-serverless
class: title

Serverless

.nav[
[Section précédente](#toc-)
|
[Retour table des matières](#toc-chapter-1)
|
[Section suivante](#toc-introduction-aux-volumes)
]

.debug[(automatically generated title slide)]

---

# Serverless

.debug[[kube/serverless_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/serverless_fr.md)]
---
class: pic

![apigateway](images/serverless_apigateway.png)

.debug[[kube/serverless_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/serverless_fr.md)]
---

## Exemple d'une fonction serverless
```python
import json, logging, os, time, uuid
import boto3
dynamodb = boto3.resource('dynamodb')

def create(event, context):
    data = json.loads(event['body'])
    table = dynamodb.Table(os.environ['DYNAMODB_TABLE'])

    item = {
        'id': str(uuid.uuid1()),
        'text': data['text'],
        'createdAt': int(time.time() * 1000),
    }

    table.put_item(Item=item)
    return {
        "statusCode": 200,
        "body": json.dumps(item)
    }
```
.debug[[kube/serverless_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/serverless_fr.md)]
---

## Que faire de ça ?

1) l'uploader sur une plateforme de serverless

2) configurer l'API Gateway de cette plateforme pour que la fonction soit appelé lors d'un evenement choisi.


.debug[[kube/serverless_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/serverless_fr.md)]
---

## Les avantages du serverless

- Pas d'administration (pour l'utilisateur)

- Auto-scaling

- Pay-per-use

- déployer plus souvent et plus rapidement

- Applications multi-language

- Le développeur n'a pas à se soucier de la scalabilité de son code

.debug[[kube/serverless_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/serverless_fr.md)]
---

## Les défauts du serverless

- Le développeur n'a pas à se soucier de la scalabilité de son code
    - la difficulté de parallélisation est dans la base de donnée sous jacente (ex: dynamoDB)

- Plus d'éléments indépendants à gérer

- Une erreur peut être propager à travers plusieurs fonctions avant de causer des problèmes

- Programmer des fonctions *"purs"* peut être difficile

- Les sysadmins changent du code (en mettant à jour par exemple python), sans connaitre le code qui l'utilise => possibilité de bugs!


.debug[[kube/serverless_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/serverless_fr.md)]
---

## Les Use Cases

- Gestion de contenu (ex: genérer des aperçus d'images)
- Traiter des évenements venant de SaaS (ex: quand un msg "pizza" reçu sur slack, commander un pizza)
- Auto-scaling des sites webs et des APIs
- Hybrid-cloud Applications
- Data Pipeline, principalement du Extract-Transform-Load
- Notifications
- Real time update
- Client access : un fonction pour valider un token
- Live data migration
- CI/CD


.debug[[kube/serverless_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/serverless_fr.md)]
---

## Exemple de use case


- Exemple avec un site web de livraison mensuel de box de chocolats:

    - le frontend est gérer par un service wordpress
    - les formulaires sont géré par un service externe (ex: formstack)
    - le paiement est géré par un service externe (ex: stripe, paypal)
    - la base de donnée client est géré par un service externe (ex: google sheet, firebase)
    - la gestion des stocks est faites via un service externe (ex: Odoo, SAP)
    - quand un nouvel utilisateur s'enregistre, ses informations sont rajouté dans la base client
    - quand un nouveau client est entrée dans la base, la commande est ajouter dans le gestionnaire de stock
    - quand un nouveau mail apparet dans la base client, des mails marketing sont envoyé (pour acheter des *super* boites, avec plus de chocolats)
    - ...

.debug[[kube/serverless_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/serverless_fr.md)]
---

## Le serverless permet un découplage

- Une application coder avec du serverless permet de découpler les services, des données, de l'intéligence.

- Le developpeur a juste à connécter les bons services entre eux.

- Voir "API economy".



.debug[[kube/serverless_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/serverless_fr.md)]
---

class: pic
![Landscape](images/CloudNativeLandscape_Serverless_latest.png)

.debug[[kube/serverless_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/serverless_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/ShippingContainerSFBay.jpg)]

---

name: toc-introduction-aux-volumes
class: title

Introduction aux volumes

.nav[
[Section précédente](#toc-serverless)
|
[Retour table des matières](#toc-chapter-2)
|
[Section suivante](#toc-introduction-de-persistentvolumes-et-persistentvolumeclaims)
]

.debug[(automatically generated title slide)]

---
# Introduction aux volumes

- Les volumes de Kubernetes sont une composante d'un pod et sont donc définis dans la spécification de pods, tout comme les conteneurs.

- Ils ne sont pas un objet Kubernetes autonome et ne peuvent pas être créés ou supprimés par eux-mêmes.

- Un volume est disponible pour tous les conteneurs du pod, mais il doit être monté dans chaque conteneur qui doit y accéder.

- Dans chaque conteneur, vous pouvez monter le volume à n'importe quel emplacement de son système de fichiers.

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

## Les volumes par un exemple

- Conteneurs qui n'ont pas de stockage commune

- Conteneurs qui partageant 2 volumes montés dans des chemins de montage différents

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

class: pic

![haha seulement blague](images/volumes1.png)

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

class: pic

![haha seulement blague](images/volume2.png)

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---


## Remarques

- Les schemas ont été repris du livre de Marko Luksa "Kubernetes in Action"

- Le volume /var/logs n'est pas monté dans le conteneur ContentAgent.

- Le conteneur ne peut pas accéder à ses fichiers, même si le conteneur et le volume font partie du même conteneur.

- Il ne suffit pas de définir un volume dans le pod; vous devez également définir un VolumeMount dans la spécification du conteneur, si vous voulez que le conteneur puisse y accéder.

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

## Types de volume

- Une grande variété de types de volumes est disponible. Plusieurs sont génériques, tandis que d'autres sont spécifiques aux technologies de stockage utilisées en dessous.

* `emptyDir`: un répertoire vide simple utilisé pour stocker des données transitoires.
* `hostPath`: Utilisé pour monter les répertoires du système de fichiers du noeud worker dans le pod.
* `gitRepo`: Un volume initialisé en vérifiant le contenu d'un dépôt Git.
* `nfs`: un partage NFS monté dans le pod.
* `gcePersistentDisk`, `awsElasticBlockStore`, `azureDisk`: Utilisé pour monter le stockage spécifique au fournisseur de cloud.
* `cinder`, `cephfs`, ...: Utilisé pour monter d'autres types de stockage réseau.
* `configMap`, `secret`, `downwardAPI`: Types spéciaux de volumes utilisés pour exposer certaines ressources et informations de cluster Kubernetes au pod.
* `persistentVolumeClaim`: un moyen d'utiliser un stockage persistant pré-provisionné ou dynamiquement.

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

## Remarque

- Un seul pod peut utiliser plusieurs volumes de types différents en même temps

- Chacun des conteneurs du pod peut avoir le volume monté ou non.

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

## Exemple d'un pod utilisant le volume gitrepo

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

.exercise[
  ```bash
apiVersion: v1
kind: Pod
metadata:
  name: gitrepo-volume-pod
spec:
  containers:
  - image: nginx:alpine
    name: web-server
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html
      readOnly: true
    ports:
    - containerPort: 80
      protocol: TCP
  volumes:
  - name: html
    gitRepo:
      repository: https://github.com/luksa/kubia-website-example.git
      revision: master
      directory: .   
  ```
]

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

## Découplage des pods de la technologie de stockage sous-jacente

- Le cas ci-dessus est contre l'idée de base de Kubernetes, qui vise à cacher l'infrastructure réelle de l'application et de son développeur.

- Lorsqu'un développeur a besoin d'une certaine quantité de stockage persistant pour son application, il doit le demander à Kubernetes.

- De la même manière qu'ils demandent du CPU, de la mémoire et d'autres ressources lors de la création d'un pod.

- L'administrateur système peut configurer le cluster afin qu'il puisse donner aux applications ce qu'elles demandent.

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/aerial-view-of-containers.jpg)]

---

name: toc-introduction-de-persistentvolumes-et-persistentvolumeclaims
class: title

Introduction de PersistentVolumes et PersistentVolumeClaims

.nav[
[Section précédente](#toc-introduction-aux-volumes)
|
[Retour table des matières](#toc-chapter-2)
|
[Section suivante](#toc-provisionnement-dynamique-des-volumes-persistants)
]

.debug[(automatically generated title slide)]

---

# Introduction de PersistentVolumes et PersistentVolumeClaims

- Au lieu que le développeur ajoute un volume spécifique à son pod, c'est l'administrateur du cluster qui configure le stockage sous-jacent, puis l'enregistre dans
Kubernetes en créant une ressource PersistentVolume via le serveur de l'API Kubernetes.

- Lors de la création de PersistentVolume, l'administrateur spécifie sa taille et les modes d'accès
qu'il supporte.

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

## Introduction de PersistentVolumes et PersistentVolumeClaims

- Lorsqu'un utilisateur de cluster doit utiliser un stockage persistant dans l'un de ses pods, il crée d'abord un manifeste PersistentVolumeClaim, en spécifiant la taille minimale et le mode d'accès qu'ils exigent.

- L'utilisateur soumet ensuite le manifeste PersistentVolumeClaim au serveur de l'API Kubernetes, et Kubernetes trouve le PersistentVolume approprié et lie au Volume Claim.

- Le PersistentVolumeClaim peut alors être utilisé comme l'un des volumes à l'intérieur d'un pod. Les autres utilisateurs ne peuvent pas utiliser le même PersistentVolume jusqu'à ce qu'il ait été libéré en supprimant
le PersistentVolumeClaim lié.

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

## Exemple de PersistentVolumes et PersistentVolumeClaims

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

class: pic

![haha seulement blague](images/volumes3.png)

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

## PersistentVolumes et Namespaces

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

class: pic

![haha seulement blague](images/Volume4.png)

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

## Durée de vie de PersistentVolume et PersistentVolumeClaims

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

class: pic

![haha seulement blague](images/Volume5.png)

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/blue-containers.jpg)]

---

name: toc-provisionnement-dynamique-des-volumes-persistants
class: title

Provisionnement dynamique des volumes persistants

.nav[
[Section précédente](#toc-introduction-de-persistentvolumes-et-persistentvolumeclaims)
|
[Retour table des matières](#toc-chapter-2)
|
[Section suivante](#toc-rook-orchestration-de-stockage-distribu)
]

.debug[(automatically generated title slide)]

---
# Provisionnement dynamique des volumes persistants

- Les schemas précédents ont été repris du livre de Marko Luksa "Kubernetes in Action"

- Nous avons vu comment l'utilisation de PersistentVolumes et PersistentVolumeClaims facilite l'obtention d'un stockage persistant sans que le développeur n'ait à gérer le stockage réel utilisée en dessous.

- Mais cela nécessite toujours un administrateur de cluster pour provisionner le stockage réel à l'avance.

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

## Provisionnement dynamique des volumes persistants

- Heureusement, Kubernetes peut également effectuer ce travail automatiquement grâce au provisionnement dynamique de PersistentVolumes.

- L'administrateur du cluster, au lieu de créer PersistentVolumes, peut déployer un provisionneur PersistentVolume et définir un ou plusieurs objets StorageClass pour permettre aux utilisateurs de choisir le type de PersistentVolume souhaité.

- Les utilisateurs peuvent se référer à StorageClass dans leur PersistanceVolumeClaims et le provisionneur en tiendra compte lors de l'approvisionnement du stockage persistant.

- Le schema suivant a été repris du livre de Marko Luksa "Kubernetes in Action" 

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

## Provisionnement dynamique des volumes persistants

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

class: pic

![haha seulement blague](images/volume6.png)

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/chinook-helicopter-container.jpg)]

---

name: toc-rook-orchestration-de-stockage-distribu
class: title

Rook orchestration de stockage distribué

.nav[
[Section précédente](#toc-provisionnement-dynamique-des-volumes-persistants)
|
[Retour table des matières](#toc-chapter-2)
|
[Section suivante](#toc-exercice-ab-testing-la-suite)
]

.debug[(automatically generated title slide)]

---

# Rook orchestration de stockage distribué

- Rook est un orchestrateur open source pour les systèmes de stockage distribués.

- Rook transforme le logiciel de stockage distribué en un service de stockage auto-géré, auto-scalable et auto-guérisant.

- Il le fait en automatisant le déploiement, l'amorçage, la configuration, l'approvisionnement, la mise à l'échelle, la mise à niveau, la migration, la reprise après sinistre, la surveillance et la gestion des ressources.

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

## Rook orchestration de stockage distribué

- Rook se concentre d'abord sur l'orchestration de Ceph sur Kubernetes. Ceph est un système de stockage distribué qui permet le stockage de fichiers, de blocs et d'objets et qui est déployé dans des clusters de production à grande échelle.

- Rook est hébergé par la Cloud Native Computing Foundation (CNCF) en tant que projet de niveau initial.

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

## Exemple de provisionnement dynamique de PersistentVolumes à l'aide de Rook

.exercise[
  ```bash
   git clone https://github.com/rook/rook.git
   cd rook/cluster/examples/kubernetes/ceph
   kubectl create -f operator.yaml
   kubectl create -f cluster.yaml
  ```
- vérifiez pour voir si tout fonctionne comme prévu
  ```bash
   kubectl get pods -n rook-ceph
  ```

]

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

## Exemple de provisionnement dynamique de PersistentVolumes à l'aide de Rook

- Le stockage 'block' vous permet de monter le stockage dans un seul pod.

- Voyons comment construire une application web simple et multi-niveaux sur Kubernetes en utilisant des volumes persistants activés par Rook.

--

- Avant que Rook puisse démarrer le provisionnement, une classe StorageClass et son pool de stockage doivent être créés.

- Ceci est nécessaire pour que Kubernetes puisse interopérer avec Rook pour provisionner des volumes persistants.

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---
## Exemple de provisionnement dynamique de PersistentVolumes à l'aide de Rook

<!--   kubectl create -f pool.yaml -->
.exercise[
- Créez le pool et le storage class:
  ```bash
  kubectl create -f storageclass.yaml
  ```
]
- Consommez le stockage avec l'échantillon wordpress
- Nous créons un exemple d'application pour consommer le stockage en 'block' provisionné par Rook avec les applications classiques wordpress et mysql.
- Ces deux applications utiliseront les volumes 'block' provisionnés par Rook.

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

## Exemple de provisionnement dynamique de PersistentVolumes à l'aide de Rook


.exercise[
- Démarrez mysql et wordpress depuis le dossier cluster/examples/kubernetes:
  ```bash
  kubectl create -f mysql.yaml
  kubectl create -f wordpress.yaml
  ```
- Ces deux applications créent un volume en 'block' et le montent dans leur pod respectif. Vous pouvez voir les 'volume claims' de Kubernetes en exécutant les opérations suivantes:

  ```bash
  kubectl get pvc
  ```
- Vous devriez voir quelque chose comme ça:
```bash
NAME             STATUS    VOLUME        CAPACITY   ACCESSMODES   AGE
mysql-pv-claim   Bound     pvc-954459ee   20Gi       RWO           1m
wp-pv-claim      Bound     pvc-39e459ee   20Gi       RWO           1m
```
]
.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

## Exemple de provisionnement dynamique de PersistentVolumes à l'aide de Rook

.exercise[
- Une fois que les pods wordpress et mysql sont dans l'état Running, récupérez l'adresse IP du cluster de l'application wordpress et entrez-la dans votre navigateur avec le port:

 ```bash
 kubectl get svc wordpress
 ```
]
Vous devriez voir l'application wordpress en cours d'exécution.

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

## Lancez un autre exemple de provisionnement dynamique

.exercise[

- Récupérez le fichier:
  ```bash
  wget https://raw.githubusercontent.com/zonca/jupyterhub-deploy-kubernetes-jetstream/master/storage_rook/alpine-rook.yaml
  ```

- Modifiez-le pour qu'il corresponde aux spécifications de votre cluster (notre `storageClassName` est `"rook-ceph-block"`) et exécutez-le en utilisant:
  ```bash
  kubectl create -f alpine-rook.yaml
  ```

]

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

## Lancez un autre exemple de provisionnement dynamique (suite)

- C'est un petit pod avec Alpine Linux qui crée un volume de 2 Go à partir de Rook et le monte sur /data.

- Cela crée un Pod avec Alpine Linux qui demande qu'un Persistent Volume Claim soit montée sous /data.

- Le PersistentVolumeClaim spécifiait le type de stockage et sa taille.

- Une fois le Pod créé, il demande au PersistentVolumeClaim de demander à Rook de préparer un volume persistant qui sera ensuite monté dans le pod.

.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

## Lancez un autre exemple de provisionnement dynamique (suite)

- Nous pouvons vérifier que les Volumes Persistants sont créés et associés au pod, vérifiez:

.exercise[
  ```bash
  kubectl get pv
  kubectl get pvc
  kubectl logs alpine
  ```
- Obtenez un shell dans le pod avec:
  ```bash
  kubectl exec -it alpine /bin/sh
  ```
- Créer des fichiers dans `/data/`.
- Quitter le terminal
- Maintenant supprimez le pod. Est-il encore possible daccéder ces données ? Si oui, comment ?
]

<!-----
## Lancez un autre exemple de provisionnement dynamique (suite)

.exercise[
- Comment aurions-nous pu récupérer les données dans le dernier cas?
- Changeons alpine-rook.yaml en `kind:deployment`, écrivez quelques fichiers et tuez à nouveau le pod pour voir ce qui se passe.
]-->



.debug[[kube/stockage_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/stockage_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/container-cranes.jpg)]

---

name: toc-exercice-ab-testing-la-suite
class: title

Exercice A/B testing, la suite

.nav[
[Section précédente](#toc-rook-orchestration-de-stockage-distribu)
|
[Retour table des matières](#toc-chapter-3)
|
[Section suivante](#toc-dcouplage-de-configuration-avec-un-configmap)
]

.debug[(automatically generated title slide)]

---

# Exercice A/B testing, la suite

L'équipe marketing à determiner que la vA du serveurweb était la meilleure.

Vous aller donc lancer un service qui exposera un **pod**.

Cependant, ils veulent aussi que vous rajoutiez un dossier `bonus` dans /tmp/ qui contiendra un fichier `bonus.txt`. Ce fichier aura juste le texte "bonus" dedans.

Lors de la réunion vous sentez que ce fichier va surement changer souvent... Vous décidez qu'au lieu de changer l'image docker, vous aller monter un volume persitant qui vous permettera de changer rapidement le contenu de ce dossier.

- Conseil: ne foncez pas tête baisser. Faites un plan, puis éxecutez le.

- Conseil 2: la slide suivante contient de l'aide... A vous de voir...

.debug[[kube/exo-ab-testing/exo2.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/exo-ab-testing/exo2.md)]
---

## Aide

- Repartez de https://raw.githubusercontent.com/zonca/jupyterhub-deploy-kubernetes-jetstream/master/storage_rook/alpine-rook.yaml

- l'image du serveur A s'appelle `127.0.0.1:32092/serverweb:vA`

- Pour écrire le fichier bonus, connéctez-vous au pod:
  ```bash
kubectl exec -ti alpine -- sh
  ```




.debug[[kube/exo-ab-testing/exo2.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/exo-ab-testing/exo2.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/container-housing.jpg)]

---

name: toc-dcouplage-de-configuration-avec-un-configmap
class: title

Découplage de configuration avec un ConfigMap

.nav[
[Section précédente](#toc-exercice-ab-testing-la-suite)
|
[Retour table des matières](#toc-chapter-4)
|
[Section suivante](#toc-introduction-des-secrets)
]

.debug[(automatically generated title slide)]

---
# Découplage de configuration avec un ConfigMap

- Le but de la configuration d'une application est de garder les options de configuration qui varient d'un environnement à l'autre, ou de changer fréquemment, séparément de la source de l'application
code.

- Si vous considérez un descripteur de pod comme un code source pour votre application (il définit comment composer les composants individuels dans un système fonctionnel), il est clair que vous devez supprimer la configuration de la description du pod.

.debug[[kube/configs_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/configs_fr.md)]
---

## Introduction de ConfigMap

- Kubernetes permet de séparer les options de configuration en un objet distinct appelé ConfigMap, qui est une map contenant des paires clé / valeur avec des valeurs allant de
des littéraux courts aux fichiers de configuration complets.

- Une application n'a pas besoin de lire directement le ConfigMap ou même de savoir qu'il existe. Le contenu de la map est plutôt transmis aux conteneurs comme environnement
variables ou en tant que fichiers dans un volume.

.debug[[kube/configs_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/configs_fr.md)]
---

## Introduction de ConfigMap

- Vous pouvez définir les entrées de la map en transmettant des littéraux à la commande kubectl ou vous pouvez créer ConfigMap à partir de fichiers stockés sur votre disque.

.exercise[
- Pour créer une nouvelle entrée:
  ```bash
  kubectl create configmap fortune-config --from-literal=sleep-interval=25
  ```
- Pour regarder les valeurs d'un ConfigMap:
  ```bash
  kubectl describe configmap fortune-config
  ```
]

- REMARQUE Les clés ConfigMap doivent être un sous-domaine DNS valide (elles ne peuvent contenir que des caractères alphanumériques, des tirets, des traits de soulignement et des points). Ils peuvent éventuellement commencer par un point.

.debug[[kube/configs_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/configs_fr.md)]
---

## Explication de Configmaps dans un exemple

- Exécutez l'exemple décrit ici: https://kubernetes.io/docs/tutorials/configuration/configure-redis-using-configmap/

Attention ! Votre version de Kubernetes ne supporte pas encore qu'il est une URL dans l'option --from-file !


- Pour aller plus loin: https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/

.debug[[kube/configs_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/configs_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/containers-by-the-water.jpg)]

---

name: toc-introduction-des-secrets
class: title

Introduction des secrets

.nav[
[Section précédente](#toc-dcouplage-de-configuration-avec-un-configmap)
|
[Retour table des matières](#toc-chapter-4)
|
[Section suivante](#toc-monitoring-avec-prometheus-et-grafana)
]

.debug[(automatically generated title slide)]

---

# Introduction des secrets

- Kubernetes fournit un objet séparé appelé Secret. Les secrets ressemblent beaucoup à ConfigMaps

- Ce sont aussi des maps qui contiennent des paires clé-valeur. Ils peuvent être utilisés de la même manière qu'un ConfigMap.

- Vous pouvez passer des entrées secrètes au conteneur en tant que variables d'environnement

- Expose les entrées secrètes en tant que fichiers dans un volume

.debug[[kube/configs_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/configs_fr.md)]
---

## Introduction des secrets

- Kubernetes aide à garder vos secrets en toute sécurité en s'assurant que chaque secret est seulement distribué
aux nœuds qui exécutent les pods qui ont besoin d'accéder au secret.

- De plus, sur les nœuds eux-mêmes, les Secrets sont toujours stockés en mémoire et jamais écrits dans le stockage physique,
ce qui nécessiterait de nettoyer les disques après avoir supprimé les secrets d'eux.

.debug[[kube/configs_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/configs_fr.md)]
---
## Introduction des secrets

- Sur le nœud maître proprement dit, etcd stocke les secrets sous forme cryptée, ce qui rend le système beaucoup plus sécurisé. Pour cette raison, il est impératif que vous choisissez correctement quand utiliser un Secret ou un ConfigMap. Choisir entre eux est simple:

 * Utilisez un fichier ConfigMap pour stocker des données de configuration non sensibles et simples.
--

 * Utilisez un secret pour stocker toutes les données sensibles et doivent être conservées sous clé. Si un fichier de configuration contient des données sensibles mais aussi non sensibles, vous
devrez stocker le fichier dans un secret.

.debug[[kube/configs_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/configs_fr.md)]
---

## Exercices utilisant des secrets

- Quelques exercices initiaux utilisant Secrets peuvent être trouvés ici: https://kubernetes.io/docs/concepts/configuration/secret/

.debug[[kube/configs_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/configs_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/distillery-containers.jpg)]

---

name: toc-monitoring-avec-prometheus-et-grafana
class: title

Monitoring avec Prometheus et Grafana

.nav[
[Section précédente](#toc-introduction-des-secrets)
|
[Retour table des matières](#toc-chapter-5)
|
[Section suivante](#toc-statefulsets)
]

.debug[(automatically generated title slide)]

---
# Monitoring avec Prometheus et Grafana

- Prometheus, pour le monitoring

- Grafana, pour afficher les métriques et jouer avec.

.debug[[kube/monitoring_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/monitoring_fr.md)]
---

## Kube Prometheus

Kube Prometheus est un dépot git qui permet d'installer une stack de monitoring Prometheus+Grafana pour kubernetes.
Il configure aussi Grafana pour intégrer des graphiques utiles.

.exercise[
  ```bash
git clone https://github.com/coreos/prometheus-operator.git

cd prometheus-operator/contrib/kube-prometheus/

kubectl create -f manifests/
  ```
]

.debug[[kube/monitoring_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/monitoring_fr.md)]
---

## Grafana

.exercise[
- Ouvrez le service grafana à l'exterieur, connectez-vous y avec votre navigateur web

- Si vous avez besoin  de vous logguer : admin/admin

- Cliquez sur "Home" tout en haut de l'écran, choisissez le dashboard *Pods*.

- Quel est l'utilisation mémoire du pod registery ?

- Quel est le coût des pods du daemonset de prometheus ?

]


.debug[[kube/monitoring_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/monitoring_fr.md)]
---

## reset

  ```bash
kubectl delete -f manifests/
  ```


.debug[[kube/monitoring_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/monitoring_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/lots-of-containers.jpg)]

---

name: toc-statefulsets
class: title

StatefulSets

.nav[
[Section précédente](#toc-monitoring-avec-prometheus-et-grafana)
|
[Retour table des matières](#toc-chapter-6)
|
[Section suivante](#toc-tp-wordpress)
]

.debug[(automatically generated title slide)]

---

# StatefulSets

- A StatefulSet allows to have a group of pods that have a stable name and state.

- What is the difference with ReplicaSet (deployment) ?

  - A ReplicaSet is like managing a cattle of cows : we do not care about the names of cows, we just want to know how many we have. If a cow is ill we replace her.

  - A StatefulSet is like managing a group of domestic animals : we give them names and we cannot replace them easily. If we have to replace one we need to find one with the same name and the same appearance.


.debug[[kube/statefulsets.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/statefulsets.md)]
---
## Replicate Stateful pods


![replicasets](images/replicasets.png)

Because of the ReplicaSet template system, we can only give one and only name for the PersistentVolumeClaim.

For a ReplicaSet, all replicas use the same PersistentVolumeClaim !

.debug[[kube/statefulsets.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/statefulsets.md)]
---

## The StatefulSets allow us to have unique names 


![replicasets](images/replicasets_statefulsets.png)

What happens if a node dies ?

.debug[[kube/statefulsets.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/statefulsets.md)]
---

class: pic

![replicasets](images/statefulset_nodefail.png)

.debug[[kube/statefulsets.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/statefulsets.md)]
---

## Statefulset, change of the replicas number

![replicasets](images/statefulset_scaledown.png)

The pod with the higher ID is destroyed first!

What happens with the attached PVC?

.debug[[kube/statefulsets.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/statefulsets.md)]
---

class: pic


![replicasets](images/statefulsetPVC_scaledown.png)


.debug[[kube/statefulsets.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/statefulsets.md)]
---
## Statefulset exercises

- The schemas have been taken from the book of Marko Luksa "Kubernetes in Action"

.exercise[

- *mehdb* is database (*meh* in anglais).
It replicates automatically the data between each instance.

  ```bash
wget https://gist.githubusercontent.com/glesserd/a0db0439e69426d92c632fb5c9bcba1c/raw/56b05fcdf9d4d1bbdf5f5cdca3fc104d7dca7d24/app.yaml
  ```

- Let's check the YAML...

]

Attention ! This application does not work... Indeed the data are not replicated. But it is not important for our tests with Kubernetes.


.debug[[kube/statefulsets.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/statefulsets.md)]
---
## Deployment
.exercise[
- Deploy it

  ```bash
kubectl get statefulset
kubectl get sts
  ```

- We scale the bdd

  ```bash
kubectl scale sts mehdb --replicas=4
  ```

- How did everything go ?
  ```bash
kubectl get sts
kubectl get pvc
  ```

]

.debug[[kube/statefulsets.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/statefulsets.md)]
---
## Resistance to crashes

.exercise[
- Let's kill a pod!

  ```bash
kubectl delete pod mehdb-1
  ```

- Which pod is going to be re-created ?

  ```bash
kubectl get pod
  ```

]

.debug[[kube/statefulsets.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/statefulsets.md)]
---
## Scale down

.exercise[
- Let's scale down:
  ```bash
kubectl scale sts mehdb --replicas=2
  ```


- Did everything go well ?
  ```bash
kubectl get sts
kubectl get pvc
  ```

- The PVC are still there as expected !

]

.debug[[kube/statefulsets.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/statefulsets.md)]
---
## Reset

.exercise[
- Reset:

  ```bash
kubectl delete -f app.yaml
  ```

* Do not forget to delete the PVC !!!*

]



.debug[[kube/statefulsets.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/statefulsets.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/plastic-containers.JPG)]

---

name: toc-tp-wordpress
class: title

TP Wordpress

.nav[
[Section précédente](#toc-statefulsets)
|
[Retour table des matières](#toc-chapter-7)
|
[Section suivante](#toc-autoscaling-avec-kubernetes)
]

.debug[(automatically generated title slide)]

---
# TP Wordpress

Wordpress est un moteur de site web écrit en PHP. Il est utilisé par plus de 60 millions de sites webs dans le monde.

Nous allons déployer une instance wordpress sur notre cluster.

Wordpress necessite une base de donnée pour fonctionner (généralement MySQL).

Mais aussi d'un dossier de cache.

Et un dossier permettant de stocker les fichiers uploadés (images ou documents pour le site).


.debug[[kube/exo-wordpress/tp_wordpress.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/exo-wordpress/tp_wordpress.md)]
---

## Question 1

.exercise[

De quels "objets" Kubernetes avons-nous besoin ?

**(Solution sur la slide suivante)**

]


.debug[[kube/exo-wordpress/tp_wordpress.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/exo-wordpress/tp_wordpress.md)]
---

## Question 1

.exercise[

De quels "objets" Kubernetes avons-nous besoin ?

]

- 2 pods : 1 pour Kubernetes, 1 pour Mysql

- 2 deployments : pour gérer les pods

- 1 service "ClusterIP" mysql, pour n'éxposer la BdD en interne uniquement

- 1 service "NodePort" (ou "LoadBalancer") wordpress, pour exposer le service au monde exterieur

- 3 persistentVolume pour stocker les données.

- 1 secret pour partager entre les pods le mot de passe d'accès à MySQL.

.debug[[kube/exo-wordpress/tp_wordpress.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/exo-wordpress/tp_wordpress.md)]
---
## C'est parti !

.exercise[
Créez les YAMLs nécessaire (ou non) pour chaque objet.

**(Sur les prochaines slides sont détaillées chaque objet avec un bout de solution)**
**(Puis, la solution est donnée tout à la fin)**

]



.debug[[kube/exo-wordpress/tp_wordpress.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/exo-wordpress/tp_wordpress.md)]
---
## persistentVolume

Sur le cluster est installé rook.
Rook est un fournisseur de persistentVolume basé sur Ceph.

Il nous suffira de créer des PersistentVolumeClaim avec comme classe "rook-ceph-block" pour obtenir dynamiquent de nouveaux volumes.

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-pv-claim
  labels:
    app: wordpress
spec:
  storageClassName: rook-ceph-block
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi
```

.debug[[kube/exo-wordpress/tp_wordpress.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/exo-wordpress/tp_wordpress.md)]
---
## persistentVolume

Et pour utiliser ce PersistentVolumeClaim, on ajoutera un volume au pod, et on montera ce volume dans le bon dossier:


```yaml
    ...
    spec:
      containers:
        ...
        volumeMounts:
        - name: mysql-persistent-storage
          mountPath: /var/lib/mysql
      volumes:
      - name: mysql-persistent-storage
        persistentVolumeClaim:
          claimName: mysql-pv-claim
```

.debug[[kube/exo-wordpress/tp_wordpress.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/exo-wordpress/tp_wordpress.md)]
---
## Secrets

.exercise[
- Créer, depuis la ligne de commande, un secret contenant le mot de passe MySQL.
```bash
kubectl create secret generic mysql-pass --from-literal=password=superpassword```

]

On pourra ensutie utiliser cette valeure au moment de définir les variables d'environement de l'image:

```yaml
env:
- name: MYSQL_ROOT_PASSWORD
    valueFrom:
    secretKeyRef:
        name: mysql-pass
        key: password
```

.debug[[kube/exo-wordpress/tp_wordpress.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/exo-wordpress/tp_wordpress.md)]
---
## Deployments

Nous utiliserons des deployments pour gérer nos pods.

```yaml
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: wordpress-mysql
  labels:
    app: wordpress
spec:
  template:
    metadata:
      labels:
        app: wordpress
        tier: mysql
    spec:
      containers:
      - image: mysql:5.6
        name: mysql
      ...
```

.debug[[kube/exo-wordpress/tp_wordpress.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/exo-wordpress/tp_wordpress.md)]
---
## Service

Enfin nous auront des services pour éxposer en interne ou externe les déployments wordpress et mysql.

Exemple:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: wordpress-mysql
  labels:
    app: wordpress
spec:
  ports:
    - port: 3306
  selector:
    app: wordpress
    tier: mysql
  clusterIP: None
```

.debug[[kube/exo-wordpress/tp_wordpress.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/exo-wordpress/tp_wordpress.md)]
---
## Let's go!

.exercise[
Récupérer les yamls et déployez-les!

```bash
wget https://gist.githubusercontent.com/glesserd/24310a37f464d6d7569c358bcec3213d/raw/f3d0ad463659b002d2d7a047ccae9dea7fe6a60f/mysql.yaml

wget https://gist.githubusercontent.com/glesserd/e89d765ee9cfa99bd274350c3fbdb12b/raw/66e26359f5b81d7be3b58f043500428db7fe78ea/wordpress.yaml

kubectl apply -f mysql.yaml
kubectl apply -f wordpress.yaml

```

]









.debug[[kube/exo-wordpress/tp_wordpress.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/exo-wordpress/tp_wordpress.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/train-of-containers-1.jpg)]

---

name: toc-autoscaling-avec-kubernetes
class: title

Autoscaling avec Kubernetes

.nav[
[Section précédente](#toc-tp-wordpress)
|
[Retour table des matières](#toc-chapter-8)
|
[Section suivante](#toc-cicd-with-spinnaker)
]

.debug[(automatically generated title slide)]

---
# Autoscaling avec Kubernetes

Nous allons mettre en place un système d'autoscaling basé sur k8s-prom-hpa. Ce logicil à l'avantage d'être simple, fonctionel et extensible grâce à Prometheus.

.exercise[
- Récupérez le logiciel:

  ```bash
git clone https://github.com/stefanprodan/k8s-prom-hpa.git
cd k8s-prom-hpa/
  ```

]

.debug[[kube/advanced_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/advanced_fr.md)]
---

## Récupérez les métriques

Déployons le *Metrics Server*, le server qui va récupérez les métriques sur les noeuds et les transmettre en utilisant l'API standard de kubernetes:

.exercise[
  ```bash
kubectl create -f ./metrics-server
  ```
]

(Avant k8s 1.8, Heapster permettait de faire cela. Ce projet a été fusionner avec Kubernetes pour faire *Metrics Server*.)

.debug[[kube/advanced_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/advanced_fr.md)]
---

## HorizontalPodAutoscaler CRD

.exercise[
La prochaine commande définit un nouveau type d'objet (un Custorm Resource Definition (CRD)), et lance le controlleur de ce CRD.

On peut donc maintenant intéragir avec le type HorizontalPodAutoscaler comme tout les autres objets de kubernetes:

  ```bash
kubectl get hpa
  ```

]

.debug[[kube/advanced_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/advanced_fr.md)]
---

## Déploiement d'une application

.exercise[
On deploie maintenant une application affichant sur une interface web des informations décrivant son pod:

  ```bash
kubectl create -f ./podinfo/podinfo-svc.yaml,./podinfo/podinfo-dep.yaml
  ```

Vous pouvez regarder ce que ces infos sont:
  ```bash
curl IP_PUBLIC:PORT_DU_SERVICE
  ```
]

.debug[[kube/advanced_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/advanced_fr.md)]
---

## Mise en place des règles d'auto-scaling

.exercise[
On va maintenant créer les règles HorizontalPodAutoscaler:

  ```bash
kubectl create -f ./podinfo/podinfo-hpa.yaml
  ```

Regardez ce yaml pour voir ce que définissent ces règles !

On a donc maintenant une nouvelle hpa:
  ```bash
kubectl get hpa
  ```
]

.debug[[kube/advanced_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/advanced_fr.md)]
---

## Stress de l'application

.exercise[
On va maintenant strésser le service pour que le scaling ce lance, dans un nouveau terminal :
  ```bash
cd ~/
mkdir golang
export GOPATH=~/golang/
export GOROOT=/usr/lib/go-1.10
export PATH=$GOPATH/bin:$GOROOT/bin:$PATH
go get -u github.com/rakyll/hey
hey -n 10000 -q 10 -c 5 http://IP_PUBLIC:PORT_DU_SERVICE/
  ```

]
.debug[[kube/advanced_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/advanced_fr.md)]
---

## Stress de l'application 2

.exercise[

Observez ce qui se passe pour le hpa:
  ```bash
kubectl describe hpa
  ```

]

L'autoscaler ne réagit pas immédiatement au pics d'utilisation.
Par défaut, les métriques sont synchronisées toutes les 30s.
De plus, le scaling up/down peut seulement avoir lieu si il n'y pas déjà eu un scaling dans les 5 minutes.
Cela permet au HPA de prendre des décisions trop rapides et/ou contradictoires.

.debug[[kube/advanced_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/advanced_fr.md)]
---

## En savoir plus

- The procedure is explained in detail and with more examples here: https://www.weave.works/blog/kubernetes-horizontal-pod-autoscaler-and-prometheus

- Pour reset:
  ```bash
kubectl delete -f ./podinfo/podinfo-hpa.yaml
kubectl delete -f ./podinfo/podinfo-svc.yaml,./podinfo/podinfo-dep.yaml
kubectl delete -f ./metrics-server
  ```







.debug[[kube/advanced_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/advanced_fr.md)]
---
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/train-of-containers-2.jpg)]

---

name: toc-cicd-with-spinnaker
class: title

CI/CD with Spinnaker

.nav[
[Section précédente](#toc-autoscaling-avec-kubernetes)
|
[Retour table des matières](#toc-chapter-8)
|
[Section suivante](#toc-dploiement-de-jupiter-sur-kubernetes)
]

.debug[(automatically generated title slide)]

---

# CI/CD with Spinnaker

.exercise[
- We will perform the exercise from the following link:
  https://thenewstack.io/getting-started-spinnaker-kubernetes/

 * However, since we already have our own cluster and Helm installed, we will start the tutorial from the "Installing Spinnaker" section

]


.debug[[kube/advanced_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/advanced_fr.md)]
---
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/two-containers-on-a-truck.jpg)]

---

name: toc-dploiement-de-jupiter-sur-kubernetes
class: title

Déploiement de Jupiter sur Kubernetes

.nav[
[Section précédente](#toc-cicd-with-spinnaker)
|
[Retour table des matières](#toc-chapter-8)
|
[Section suivante](#toc-scheduling-avance-avec-kubernetes)
]

.debug[(automatically generated title slide)]

---

# Déploiement de Jupiter sur Kubernetes

.exercise[
- Nous suivrons la procédure fournie ici : 
  https://zonca.github.io/2017/12/scalable-jupyterhub-kubernetes-jetstream.html

]

.debug[[kube/advanced_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/advanced_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/wall-of-containers.jpeg)]

---

name: toc-scheduling-avance-avec-kubernetes
class: title

Scheduling avancée avec Kubernetes

.nav[
[Section précédente](#toc-dploiement-de-jupiter-sur-kubernetes)
|
[Retour table des matières](#toc-chapter-8)
|
[Section suivante](#toc-big-data-analytics-sur-kubernetes)
]

.debug[(automatically generated title slide)]

---
--- 

# Scheduling avancée avec Kubernetes

.exercise[
- Nous suivrons la procédure fournie ici : 
   https://github.com/RyaxTech/kube-tutorial#4-activate-an-advanced-scheduling-policy-and-test-its-usage
]

.debug[[kube/advanced_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/advanced_fr.md)]
---
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/Container-Ship-Freighter-Navigation-Elbe-Romance-1782991.jpg)]

---

name: toc-big-data-analytics-sur-kubernetes
class: title

Big Data analytics sur Kubernetes

.nav[
[Section précédente](#toc-scheduling-avance-avec-kubernetes)
|
[Retour table des matières](#toc-chapter-8)
|
[Section suivante](#toc-authentication-and-authorization)
]

.debug[(automatically generated title slide)]

---

# Big Data analytics sur Kubernetes

.exercise[
- Nous suivrons la procédure fournie ici :
https://github.com/RyaxTech/kube-tutorial#3-execute-big-data-job-with-spark-on-the-kubernetes-cluster
]



.debug[[kube/advanced_fr.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/advanced_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/ShippingContainerSFBay.jpg)]

---

name: toc-authentication-and-authorization
class: title

Authentication and authorization

.nav[
[Section précédente](#toc-big-data-analytics-sur-kubernetes)
|
[Retour table des matières](#toc-chapter-9)
|
[Section suivante](#toc-exposing-http-services-with-ingress-resources)
]

.debug[(automatically generated title slide)]

---
# Authentication and authorization

*And first, a little refresher!*

- Authentication = verifying the identity of a person

  On a UNIX system, we can authenticate with login+password, SSH keys ...

- Authorization = listing what they are allowed to do

  On a UNIX system, this can include file permissions, sudoer entries ...

- Sometimes abbreviated as "authn" and "authz"

- In good modular systems, these things are decoupled

   (so we can e.g. change a password or SSH key without having to reset access rights)

.debug[[kube/authn-authz.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/authn-authz.md)]
---

## Authentication in Kubernetes

- When the API server receives a request, it tries to authenticate it

  (it examines headers, certificates ... anything available)

- Many authentication methods are available and can be used simultaneously

  (we will see them on the next slide)

- It's the job of the authentication method to produce:

  - the user name
  - the user ID
  - a list of groups

- The API server doesn't interpret these; it'll be the job of *authorizers*

.debug[[kube/authn-authz.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/authn-authz.md)]
---

## Authentication methods

- TLS client certificates

  (that's what we've been doing with `kubectl` so far)

- Bearer tokens

  (a secret token in the HTTP headers of the request)

- [HTTP basic auth](https://en.wikipedia.org/wiki/Basic_access_authentication)

  (carrying user and password in a HTTP header)

- Authentication proxy

  (sitting in front of the API and setting trusted headers)

.debug[[kube/authn-authz.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/authn-authz.md)]
---

## Anonymous requests

- If any authentication method *rejects* a request, it's denied

  (`401 Unauthorized` HTTP code)

- If a request is neither accepted nor accepted by anyone, it's anonymous

  - the user name is `system:anonymous`

  - the list of groups is `[system:unauthenticated]`

- By default, the anonymous user can't do anything

  (that's what you get if you just `curl` the Kubernetes API)

.debug[[kube/authn-authz.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/authn-authz.md)]
---

## Authentication with TLS certificates

- This is enabled in most Kubernetes deployments

- The user name is derived from the `CN` in the client certificates

- The groups are derived from the `O` fields in the client certificate

- From the point of view of the Kubernetes API, users do not exist

  (i.e. they are not stored in etcd or anywhere else)

- Users can be created (and given membership to groups) independently of the API

- The Kubernetes API can be set up to use your custom CA to validate client certs

.debug[[kube/authn-authz.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/authn-authz.md)]
---

class: extra-details

## Viewing our admin certificate

- Let's inspect the certificate we've been using all this time!

.exercise[

- This command will show the `CN` and `O` fields for our certificate:
  ```bash
  kubectl config view \
          --raw \
          -o json \
          | jq -r .users[0].user[\"client-certificate-data\"] \
          | base64 -d \
          | openssl x509 -text \
          | grep Subject:
  ```

]

Let's break down that command together! 😅

.debug[[kube/authn-authz.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/authn-authz.md)]
---

class: extra-details

## Breaking down the command

- `kubectl config view` shows the Kubernetes user configuration
- `--raw` includes certificate information (which shows as REDACTED otherwise)
- `-o json` outputs the information in JSON format
- `| jq ...` extracts the field with the user certificate (in base64)
- `| base64 -d` decodes the base64 format (now we have a PEM file)
- `| openssl x509 -text` parses the certificate and outputs it as plain text
- `| grep Subject:` shows us the line that interests us

→ We are user `kubernetes-admin`, in group `system:masters`.

(We will see later how and why this gives us the permissions that we have.)

.debug[[kube/authn-authz.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/authn-authz.md)]
---

## User certificates in practice

- The Kubernetes API server does not support certificate revocation

  (see issue [#18982](https://github.com/kubernetes/kubernetes/issues/18982))

- As a result, we cannot easily suspend a user's access

- There are workarounds, but they are very inconvenient:

  - issue short-lived certificates (e.g. 24 hours) and regenerate them often

  - re-create the CA and re-issue all certificates in case of compromise

  - grant permissions to individual users, not groups
    <br/>
    (and remove all permissions to a compromised user)

- Until this is fixed, we probably want to use other methods

.debug[[kube/authn-authz.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/authn-authz.md)]
---

## Authentication with tokens

- Tokens are passed as HTTP headers:

  `Authorization: Bearer and-then-here-comes-the-token`

- Tokens can be validated through a number of different methods:

  - static tokens hard-coded in a file on the API server

  - [bootstrap tokens](https://kubernetes.io/docs/reference/access-authn-authz/bootstrap-tokens/) (special case to create a cluster or join nodes)

  - [OpenID Connect tokens](https://kubernetes.io/docs/reference/access-authn-authz/authentication/#openid-connect-tokens) (to delegate authentication to compatible OAuth2 providers)

  - service accounts (these deserve more details, coming right up!)

.debug[[kube/authn-authz.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/authn-authz.md)]
---

## Service accounts

- A service account is a user that exists in the Kubernetes API

  (it is visible with e.g. `kubectl get serviceaccounts`)

- Service accounts can therefore be created / updated dynamically

  (they don't require hand-editing a file and restarting the API server)

- A service account is associated with a set of secrets

  (the kind that you can view with `kubectl get secrets`)

- Service accounts are generally used to grant permissions to applications, services ...

  (as opposed to humans)

.debug[[kube/authn-authz.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/authn-authz.md)]
---

class: extra-details

## Token authentication in practice

- We are going to list existing service accounts

- Then we will extract the token for a given service account

- And we will use that token to authenticate with the API

.debug[[kube/authn-authz.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/authn-authz.md)]
---

class: extra-details

## Listing service accounts

.exercise[

- The resource name is `serviceaccount` or `sa` in short:
  ```bash
  kubectl get sa
  ```

]

There should be just one service account in the default namespace: `default`.

.debug[[kube/authn-authz.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/authn-authz.md)]
---

class: extra-details

## Finding the secret

.exercise[

- List the secrets for the `default` service account:
  ```bash
  kubectl get sa default -o yaml
  SECRET=$(kubectl get sa default -o json | jq -r .secrets[0].name)
  ```

]

It should be named `default-token-XXXXX`.

.debug[[kube/authn-authz.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/authn-authz.md)]
---

class: extra-details

## Extracting the token

- The token is stored in the secret, wrapped with base64 encoding

.exercise[

- View the secret:
  ```bash
  kubectl get secret $SECRET -o yaml
  ```

- Extract the token and decode it:
  ```bash
  TOKEN=$(kubectl get secret $SECRET -o json \
          | jq -r .data.token | base64 -d)
  ```

]

.debug[[kube/authn-authz.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/authn-authz.md)]
---

class: extra-details

## Using the token

- Let's send a request to the API, without and with the token

.exercise[

- Find the ClusterIP for the `kubernetes` service:
  ```bash
  kubectl get svc kubernetes
  API=$(kubectl get svc kubernetes -o json | jq -r .spec.clusterIP)
  ```

- Connect without the token:
  ```bash
  curl -k https://$API
  ```

- Connect with the token:
  ```bash
  curl -k -H "Authorization: Bearer $TOKEN" https://$API
  ```

]

.debug[[kube/authn-authz.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/authn-authz.md)]
---

class: extra-details

## Results

- In both cases, we will get a "Forbidden" error

- Without authentication, the user is `system:anonymous`

- With authentication, it is shown as `system:serviceaccount:default:default`

- The API "sees" us as a different user

- But neither user has any right, so we can't do nothin'

- Let's change that!

.debug[[kube/authn-authz.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/authn-authz.md)]
---

## Authorization in Kubernetes

- There are multiple ways to grant permissions in Kubernetes, called [authorizers](https://kubernetes.io/docs/reference/access-authn-authz/authorization/#authorization-modules):

  - [Node Authorization](https://kubernetes.io/docs/reference/access-authn-authz/node/) (used internally by kubelet; we can ignore it)

  - [Attribute-based access control](https://kubernetes.io/docs/reference/access-authn-authz/abac/) (powerful but complex and static; ignore it too)

  - [Webhook](https://kubernetes.io/docs/reference/access-authn-authz/webhook/) (each API request is submitted to an external service for approval)

  - [Role-based access control](https://kubernetes.io/docs/reference/access-authn-authz/rbac/) (associates permissions to users dynamically)

- The one we want is the last one, generally abbreviated as RBAC

.debug[[kube/authn-authz.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/authn-authz.md)]
---

## Role-based access control

- RBAC allows to specify fine-grained permissions

- Permissions are expressed as *rules*

- A rule is a combination of:

  - [verbs](https://kubernetes.io/docs/reference/access-authn-authz/authorization/#determine-the-request-verb) like create, get, list, update, delete ...

  - resources (as in "API resource", like pods, nodes, services ...)

  - resource names (to specify e.g. one specific pod instead of all pods)

  - in some case, [subresources](https://kubernetes.io/docs/reference/access-authn-authz/rbac/#referring-to-resources) (e.g. logs are subresources of pods)

.debug[[kube/authn-authz.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/authn-authz.md)]
---

## From rules to roles to rolebindings

- A *role* is an API object containing a list of *rules*

  Example: role "external-load-balancer-configurator" can:
  - [list, get] resources [endpoints, services, pods]
  - [update] resources [services]

- A *rolebinding* associates a role with a user

  Example: rolebinding "external-load-balancer-configurator":
  - associates user "external-load-balancer-configurator"
  - with role "external-load-balancer-configurator"

- Yes, there can be users, roles, and rolebindings with the same name

- It's a good idea for 1-1-1 bindings; not so much for 1-N ones

.debug[[kube/authn-authz.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/authn-authz.md)]
---

## Cluster-scope permissions

- API resources Role and RoleBinding are for objects within a namespace

- We can also define API resources ClusterRole and ClusterRoleBinding

- These are a superset, allowing to:

  - specify actions on cluster-wide objects (like nodes)

  - operate across all namespaces

- We can create Role and RoleBinding resources within a namespaces

- ClusterRole and ClusterRoleBinding resources are global

.debug[[kube/authn-authz.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/authn-authz.md)]
---

## Pods and service accounts

- A pod can be associated to a service account

  - by default, it is associated to the `default` service account

  - as we've seen earlier, this service account has no permission anyway

- The associated token is exposed into the pod's filesystem

  (in `/var/run/secrets/kubernetes.io/serviceaccount/token`)

- Standard Kubernetes tooling (like `kubectl`) will look for it there

- So Kubernetes tools running in a pod will automatically use the service account

.debug[[kube/authn-authz.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/authn-authz.md)]
---

## In practice

- We are going to create a service account

- We will use an existing cluster role (`view`)

- We will bind together this role and this service account

- Then we will run a pod using that service account

- In this pod, we will install `kubectl` and check our permissions

.debug[[kube/authn-authz.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/authn-authz.md)]
---

## Creating a service account

- We will call the new service account `viewer`

  (note that nothing prevents us from calling it `view`, like the role)

.exercise[

- Create the new service account:
  ```bash
  kubectl create serviceaccount viewer
  ```

- List service accounts now:
  ```bash
  kubectl get serviceaccounts
  ```

]

.debug[[kube/authn-authz.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/authn-authz.md)]
---

## Binding a role to the service account

- Binding a role = creating a *rolebinding* object

- We will call that object `viewercanview`

  (but again, we could call it `view`)

.exercise[

- Create the new role binding:
  ```bash
  kubectl create rolebinding viewercanview \
          --clusterrole=view \
          --serviceaccount=default:viewer
  ```

]

It's important to note a couple of details in these flags ...

.debug[[kube/authn-authz.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/authn-authz.md)]
---

## Roles vs Cluster Roles

- We used `--clusterrole=view`

- What would have happened if we had used `--role=view`?

  - we would have bound the role `view` from the local namespace
    <br/>(instead of the cluster role `view`)

  - the command would have worked fine (no error)

  - but later, our API requests would have been denied

- This is a deliberate design decision

  (we can reference roles that don't exist, and create/update them later)

.debug[[kube/authn-authz.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/authn-authz.md)]
---

## Users vs Service Accounts

- We used `--serviceaccount=default:viewer`

- What would have happened if we had used `--user=default:viewer`?

  - we would have bound the role to a user instead of a service account

  - again, the command would have worked fine (no error)

  - ... but our API requests would have been denied later

- What's about the `default:` prefix?

  - that's the namespace of the service account

  - yes, it could be inferred from context, but ... `kubectl` requires it

.debug[[kube/authn-authz.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/authn-authz.md)]
---

## Testing

- We will run an `alpine` pod and install `kubectl` there

.exercise[

- Run a one-time pod:
  ```bash
  kubectl run eyepod --rm -ti --restart=Never \
          --serviceaccount=viewer \
          --image alpine
  ```

- Install `curl`, then use it to install `kubectl`:
  ```bash
  apk add --no-cache curl
  URLBASE=https://storage.googleapis.com/kubernetes-release/release
  KUBEVER=$(curl -s $URLBASE/stable.txt)
  curl -LO $URLBASE/$KUBEVER/bin/linux/amd64/kubectl
  chmod +x kubectl
  ```

]

.debug[[kube/authn-authz.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/authn-authz.md)]
---

## Running `kubectl` in the pod

- We'll try to use our `view` permissions, then to create an object

.exercise[

- Check that we can, indeed, view things:
  ```bash
  ./kubectl get all
  ```

- But that we can't create things:
  ```
  ./kubectl create deployment testrbac --image=nginx
  ```

- Exit the container with `exit` or `^D`

<!-- ```keys ^D``` -->

]

.debug[[kube/authn-authz.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/authn-authz.md)]
---

## Testing directly with `kubectl`

- We can also check for permission with `kubectl auth can-i`:
  ```bash
  kubectl auth can-i list nodes
  kubectl auth can-i create pods
  kubectl auth can-i get pod/name-of-pod
  kubectl auth can-i get /url-fragment-of-api-request/
  kubectl auth can-i '*' services
  ```

- And we can check permissions on behalf of other users:
  ```bash
  kubectl auth can-i list nodes \
          --as some-user
  kubectl auth can-i list nodes \
          --as system:serviceaccount:<namespace>:<name-of-service-account>
  ```

.debug[[kube/authn-authz.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/authn-authz.md)]
---

class: extra-details

## Where do our permissions come from?

- When interacting with the Kubernetes API, we are using a client certificate

- We saw previously that this client certificate contained:

  `CN=kubernetes-admin` and `O=system:masters`

- Let's look for these in existing ClusterRoleBindings:
  ```bash
  kubectl get clusterrolebindings -o yaml | 
    grep -e kubernetes-admin -e system:masters
  ```

  (`system:masters` should show up, but not `kubernetes-admin`.)

- Where does this match come from?

.debug[[kube/authn-authz.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/authn-authz.md)]
---

class: extra-details

## The `system:masters` group

- If we eyeball the output of `kubectl get clusterrolebindings -o yaml`, we'll find out!

- It is in the `cluster-admin` binding:
  ```bash
  kubectl describe clusterrolebinding cluster-admin
  ```

- This binding associates `system:masters` to the cluster role `cluster-admin`

- And the `cluster-admin` is, basically, `root`:
  ```bash
  kubectl describe clusterrole cluster-admin
  ```

.debug[[kube/authn-authz.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/authn-authz.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/aerial-view-of-containers.jpg)]

---

name: toc-exposing-http-services-with-ingress-resources
class: title

Exposing HTTP services with Ingress resources

.nav[
[Section précédente](#toc-authentication-and-authorization)
|
[Retour table des matières](#toc-chapter-10)
|
[Section suivante](#toc-git-based-workflows)
]

.debug[(automatically generated title slide)]

---
# Exposing HTTP services with Ingress resources

- *Services* give us a way to access a pod or a set of pods

- Services can be exposed to the outside world:

  - with type `NodePort` (on a port >30000)

  - with type `LoadBalancer` (allocating an external load balancer)

- What about HTTP services?

  - how can we expose `webui`, `rng`, `hasher`?

  - the Kubernetes dashboard?

  - a new version of `webui`?

.debug[[kube/ingress.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/ingress.md)]
---

## Exposing HTTP services

- If we use `NodePort` services, clients have to specify port numbers

  (i.e. http://xxxxx:31234 instead of just http://xxxxx)

- `LoadBalancer` services are nice, but:

  - they are not available in all environments

  - they often carry an additional cost (e.g. they provision an ELB)

  - they require one extra step for DNS integration
    <br/>
    (waiting for the `LoadBalancer` to be provisioned; then adding it to DNS)

- We could build our own reverse proxy

.debug[[kube/ingress.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/ingress.md)]
---

## Building a custom reverse proxy

- There are many options available:

  Apache, HAProxy, Hipache, NGINX, Traefik, ...

  (look at [jpetazzo/aiguillage](https://github.com/jpetazzo/aiguillage) for a minimal reverse proxy configuration using NGINX)

- Most of these options require us to update/edit configuration files after each change

- Some of them can pick up virtual hosts and backends from a configuration store

- Wouldn't it be nice if this configuration could be managed with the Kubernetes API?

--

- Enter.red[¹] *Ingress* resources!

.footnote[.red[¹] Pun maybe intended.]

.debug[[kube/ingress.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/ingress.md)]
---

## Ingress resources

- Kubernetes API resource (`kubectl get ingress`/`ingresses`/`ing`)

- Designed to expose HTTP services

- Basic features:

  - load balancing
  - SSL termination
  - name-based virtual hosting

- Can also route to different services depending on:

  - URI path (e.g. `/api`→`api-service`, `/static`→`assets-service`)
  - Client headers, including cookies (for A/B testing, canary deployment...)
  - and more!

.debug[[kube/ingress.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/ingress.md)]
---

## Principle of operation

- Step 1: deploy an *ingress controller*

  - ingress controller = load balancer + control loop

  - the control loop watches over ingress resources, and configures the LB accordingly

- Step 2: setup DNS

  - associate DNS entries with the load balancer address

- Step 3: create *ingress resources*

  - the ingress controller picks up these resources and configures the LB

- Step 4: profit!

.debug[[kube/ingress.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/ingress.md)]
---

## Ingress in action

- We will deploy the Traefik ingress controller

  - this is an arbitrary choice

  - maybe motivated by the fact that Traefik releases are named after cheeses

- For DNS, we will use [nip.io](http://nip.io/)

  - `*.1.2.3.4.nip.io` resolves to `1.2.3.4`

- We will create ingress resources for various HTTP services

.debug[[kube/ingress.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/ingress.md)]
---

## Deploying pods listening on port 80

- We want our ingress load balancer to be available on port 80

- We could do that with a `LoadBalancer` service

  ... but it requires support from the underlying infrastructure

- We could use pods specifying `hostPort: 80` 

  ... but with most CNI plugins, this [doesn't work or require additional setup](https://github.com/kubernetes/kubernetes/issues/23920)

- We could use a `NodePort` service

  ... but that requires [changing the `--service-node-port-range` flag in the API server](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/)

- Last resort: the `hostNetwork` mode

.debug[[kube/ingress.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/ingress.md)]
---

## Without `hostNetwork`

- Normally, each pod gets its own *network namespace*

  (sometimes called sandbox or network sandbox)

- An IP address is associated to the pod

- This IP address is routed/connected to the cluster network

- All containers of that pod are sharing that network namespace

  (and therefore using the same IP address)

.debug[[kube/ingress.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/ingress.md)]
---

## With `hostNetwork: true`

- No network namespace gets created

- The pod is using the network namespace of the host

- It "sees" (and can use) the interfaces (and IP addresses) of the host

- The pod can receive outside traffic directly, on any port

- Downside: with most network plugins, network policies won't work for that pod

  - most network policies work at the IP address level

  - filtering that pod = filtering traffic from the node

.debug[[kube/ingress.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/ingress.md)]
---

## Running Traefik

- The [Traefik documentation](https://docs.traefik.io/user-guide/kubernetes/#deploy-trfik-using-a-deployment-or-daemonset) tells us to pick between Deployment and Daemon Set

- We are going to use a Daemon Set so that each node can accept connections

- We will do two minor changes to the [YAML provided by Traefik](https://github.com/containous/traefik/blob/master/examples/k8s/traefik-ds.yaml):

  - enable `hostNetwork`

  - add a *toleration* so that Traefik also runs on `node1`

.debug[[kube/ingress.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/ingress.md)]
---

## Taints and tolerations

- A *taint* is an attribute added to a node

- It prevents pods from running on the node

- ... Unless they have a matching *toleration*

- When deploying with `kubeadm`:

  - a taint is placed on the node dedicated the control plane

  - the pods running the control plane have a matching toleration

.debug[[kube/ingress.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/ingress.md)]
---

class: extra-details

## Checking taints on our nodes

.exercise[

- Check our nodes specs:
  ```bash
  kubectl get node node1 -o json | jq .spec
  kubectl get node node2 -o json | jq .spec
  ```

]

We should see a result only for `node1` (the one with the control plane):

```json
  "taints": [
    {
      "effect": "NoSchedule",
      "key": "node-role.kubernetes.io/master"
    }
  ]
```

.debug[[kube/ingress.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/ingress.md)]
---

class: extra-details

## Understanding a taint

- The `key` can be interpreted as:

  - a reservation for a special set of pods
    <br/>
    (here, this means "this node is reserved for the control plane")

  - an error condition on the node
    <br/>
    (for instance: "disk full", do not start new pods here!)

- The `effect` can be:

  - `NoSchedule` (don't run new pods here)

  - `PreferNoSchedule` (try not to run new pods here)

  - `NoExecute` (don't run new pods and evict running pods)

.debug[[kube/ingress.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/ingress.md)]
---

class: extra-details

## Checking tolerations on the control plane

.exercise[

- Check tolerations for CoreDNS:
  ```bash
  kubectl -n kube-system get deployments coredns -o json |
          jq .spec.template.spec.tolerations
  ```

]

The result should include:
```json
  {
    "effect": "NoSchedule",
    "key": "node-role.kubernetes.io/master"
  }
```

It means: "bypass the exact taint that we saw earlier on `node1`."

.debug[[kube/ingress.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/ingress.md)]
---

class: extra-details

## Special tolerations

.exercise[

- Check tolerations on `kube-proxy`:
  ```bash
  kubectl -n kube-system get ds kube-proxy -o json | 
          jq .spec.template.spec.tolerations
  ```

]

The result should include:
```json
  {
    "operator": "Exists"
  }
```

This one is a special case that means "ignore all taints and run anyway."

.debug[[kube/ingress.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/ingress.md)]
---

## Running Traefik on our cluster

- We provide a YAML file (`k8s/traefik.yaml`) which is essentially the sum of:

  - [Traefik's Daemon Set resources](https://github.com/containous/traefik/blob/master/examples/k8s/traefik-ds.yaml) (patched with `hostNetwork` and tolerations)

  - [Traefik's RBAC rules](https://github.com/containous/traefik/blob/master/examples/k8s/traefik-rbac.yaml) allowing it to watch necessary API objects

.exercise[

- Apply the YAML:
  ```bash
  kubectl apply -f ~/container.training/k8s/traefik.yaml
  ```

]

.debug[[kube/ingress.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/ingress.md)]
---

## Checking that Traefik runs correctly

- If Traefik started correctly, we now have a web server listening on each node

.exercise[

- Check that Traefik is serving 80/tcp:
  ```bash
  curl localhost
  ```

]

We should get a `404 page not found` error.

This is normal: we haven't provided any ingress rule yet.

.debug[[kube/ingress.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/ingress.md)]
---

## Setting up DNS

- To make our lives easier, we will use [nip.io](http://nip.io)

- Check out `http://cheddar.A.B.C.D.nip.io`

  (replacing A.B.C.D with the IP address of `node1`)

- We should get the same `404 page not found` error

  (meaning that our DNS is "set up properly", so to speak!)

.debug[[kube/ingress.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/ingress.md)]
---

## Traefik web UI

- Traefik provides a web dashboard

- With the current install method, it's listening on port 8080

.exercise[

- Go to `http://node1:8080` (replacing `node1` with its IP address)

]

.debug[[kube/ingress.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/ingress.md)]
---

## Setting up host-based routing ingress rules

- We are going to use `errm/cheese` images

  (there are [3 tags available](https://hub.docker.com/r/errm/cheese/tags/): wensleydale, cheddar, stilton)

- These images contain a simple static HTTP server sending a picture of cheese

- We will run 3 deployments (one for each cheese)

- We will create 3 services (one for each deployment)

- Then we will create 3 ingress rules (one for each service)

- We will route `<name-of-cheese>.A.B.C.D.nip.io` to the corresponding deployment

.debug[[kube/ingress.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/ingress.md)]
---

## Running cheesy web servers

.exercise[

- Run all three deployments:
  ```bash
  kubectl create deployment cheddar --image=errm/cheese:cheddar
  kubectl create deployment stilton --image=errm/cheese:stilton
  kubectl create deployment wensleydale --image=errm/cheese:wensleydale
  ```

- Create a service for each of them:
  ```bash
  kubectl expose deployment cheddar --port=80
  kubectl expose deployment stilton --port=80
  kubectl expose deployment wensleydale --port=80
  ```

]

.debug[[kube/ingress.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/ingress.md)]
---

## What does an ingress resource look like?

Here is a minimal host-based ingress resource:

```yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: cheddar
spec:
  rules:
  - host: cheddar.`A.B.C.D`.nip.io
    http:
      paths:
      - path: /
        backend:
          serviceName: cheddar
          servicePort: 80

```

(It is in `k8s/ingress.yaml`.)

.debug[[kube/ingress.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/ingress.md)]
---

## Creating our first ingress resources

.exercise[

- Edit the file `~/container.training/k8s/ingress.yaml`

- Replace A.B.C.D with the IP address of `node1`

- Apply the file

- Open http://cheddar.A.B.C.D.nip.io

]

(An image of a piece of cheese should show up.)

.debug[[kube/ingress.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/ingress.md)]
---

## Creating the other ingress resources

.exercise[

- Edit the file `~/container.training/k8s/ingress.yaml`

- Replace `cheddar` with `stilton` (in `name`, `host`, `serviceName`)

- Apply the file

- Check that `stilton.A.B.C.D.nip.io` works correctly

- Repeat for `wensleydale`

]

.debug[[kube/ingress.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/ingress.md)]
---

## Using multiple ingress controllers

- You can have multiple ingress controllers active simultaneously

  (e.g. Traefik and NGINX)

- You can even have multiple instances of the same controller

  (e.g. one for internal, another for external traffic)

- The `kubernetes.io/ingress.class` annotation can be used to tell which one to use

- It's OK if multiple ingress controllers configure the same resource

  (it just means that the service will be accessible through multiple paths)

.debug[[kube/ingress.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/ingress.md)]
---

## Ingress: the good

- The traffic flows directly from the ingress load balancer to the backends

  - it doesn't need to go through the `ClusterIP`

  - in fact, we don't even need a `ClusterIP` (we can use a headless service)

- The load balancer can be outside of Kubernetes

  (as long as it has access to the cluster subnet)

- This allows to use external (hardware, physical machines...) load balancers

- Annotations can encode special features

  (rate-limiting, A/B testing, session stickiness, etc.) 

.debug[[kube/ingress.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/ingress.md)]
---

## Ingress: the bad

- Aforementioned "special features" are not standardized yet

- Some controllers will support them; some won't

- Even relatively common features (stripping a path prefix) can differ:

  - [traefik.ingress.kubernetes.io/rule-type: PathPrefixStrip](https://docs.traefik.io/user-guide/kubernetes/#path-based-routing)

  - [ingress.kubernetes.io/rewrite-target: /](https://github.com/kubernetes/contrib/tree/master/ingress/controllers/nginx/examples/rewrite)

- This should eventually stabilize

  (remember that ingresses are currently `apiVersion: extensions/v1beta1`)

.debug[[kube/ingress.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/ingress.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/blue-containers.jpg)]

---

name: toc-git-based-workflows
class: title

Git-based workflows

.nav[
[Section précédente](#toc-exposing-http-services-with-ingress-resources)
|
[Retour table des matières](#toc-chapter-11)
|
[Section suivante](#toc-)
]

.debug[(automatically generated title slide)]

---
# Git-based workflows

- Deploying with `kubectl` has downsides:

  - we don't know *who* deployed *what* and *when*

  - there is no audit trail (except the API server logs)

  - there is no easy way to undo most operations

  - there is no review/approval process (like for code reviews)

- We have all these things for *code*, though

- Can we manage cluster state like we manage our source code?

.debug[[kube/gitworkflows.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/gitworkflows.md)]
---

## Reminder: Kubernetes is *declarative*

- All we do is create/change resources

- These resources have a perfect YAML representation

- All we do is manipulating these YAML representations

  (`kubectl run` generates a YAML file that gets applied)

- We can store these YAML representations in a code repository

- We can version that code repository and maintain it with best practices

  - define which branch(es) can go to qa/staging/production

  - control who can push to which branches

  - have formal review processes, pull requests ...

.debug[[kube/gitworkflows.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/gitworkflows.md)]
---

## Enabling git-based workflows

- There are a few tools out there to help us do that

- We'll see demos of two of them: [Flux] and [Gitkube]

- There are *many* other tools, some of them with even more features

- There are also *many* integrations with popular CI/CD systems

  (e.g.: GitLab, Jenkins, ...)

[Flux]: https://www.weave.works/oss/flux/
[Gitkube]: https://gitkube.sh/

.debug[[kube/gitworkflows.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/gitworkflows.md)]
---

## Flux overview

- We put our Kubernetes resources as YAML files in a git repository

- Flux polls that repository regularly (every 5 minutes by default)

- The resources described by the YAML files are created/updated automatically

- Changes are made by updating the code in the repository

.debug[[kube/gitworkflows.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/gitworkflows.md)]
---

## Preparing a repository for Flux

- We need a repository with Kubernetes YAML files

- I have one: https://github.com/jpetazzo/kubercoins

- Fork it to your GitHub account

- Create a new branch in your fork; e.g. `prod`

  (e.g. by adding a line in the README through the GitHub web UI)

- This is the branch that we are going to use for deployment

.debug[[kube/gitworkflows.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/gitworkflows.md)]
---

## Setting up Flux

- Clone the Flux repository:
  ```
  git clone https://github.com/weaveworks/flux
  ```

- Edit `deploy/flux-deployment.yaml`

- Change the `--git-url` and `--git-branch` parameters:
  ```yaml
  - --git-url=git@github.com:your-git-username/kubercoins
  - --git-branch=prod
  ```

- Apply all the YAML:
  ```
  kubectl apply -f deploy/
  ```

.debug[[kube/gitworkflows.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/gitworkflows.md)]
---

## Allowing Flux to access the repository

- When it starts, Flux generates an SSH key

- Display that key:
  ```
  kubectl logs deployment flux | grep identity
  ```

- Then add that key to the repository, giving it **write** access

  (some Flux features require write access)

- After a minute or so, DockerCoins will be deployed to the current namespace

.debug[[kube/gitworkflows.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/gitworkflows.md)]
---

## Making changes

- Make changes (on the `prod` branch), e.g. change `replicas` in `worker`

- After a few minutes, the changes will be picked up by Flux and applied

.debug[[kube/gitworkflows.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/gitworkflows.md)]
---

## Other features

- Flux can keep a list of all the tags of all the images we're running

- The `fluxctl` tool can show us if we're running the latest images

- We can also "automate" a resource (i.e. automatically deploy new images)

- And much more!

.debug[[kube/gitworkflows.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/gitworkflows.md)]
---

## Gitkube overview

- We put our Kubernetes resources as YAML files in a git repository

- Gitkube is a git server (or "git remote")

- After making changes to the repository, we push to Gitkube

- Gitkube applies the resources to the cluster

.debug[[kube/gitworkflows.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/gitworkflows.md)]
---

## Setting up Gitkube

- Install the CLI:
  ```
  sudo curl -L -o /usr/local/bin/gitkube \
       https://github.com/hasura/gitkube/releases/download/v0.2.1/gitkube_linux_amd64
  sudo chmod +x /usr/local/bin/gitkube
  ```

- Install Gitkube on the cluster:
  ```
  gitkube install --expose ClusterIP
  ```

.debug[[kube/gitworkflows.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/gitworkflows.md)]
---

## Creating a Remote

- Gitkube provides a new type of API resource: *Remote*

  (this is using a mechanism called Custom Resource Definitions or CRD)

- Create and apply a YAML file containing the following manifest:
  ```yaml
	apiVersion: gitkube.sh/v1alpha1
	kind: Remote
	metadata:
	  name: example
	spec:
	  authorizedKeys:
	  - `ssh-rsa AAA...`
	  manifests:
	    path: "."
  ```

  (replace the `ssh-rsa AAA...` section with the content of `~/.ssh/id_rsa.pub`)

.debug[[kube/gitworkflows.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/gitworkflows.md)]
---

## Pushing to our remote

- Get the `gitkubed` IP address:
  ```
  kubectl -n kube-system get svc gitkubed
  IP=$(kubectl -n kube-system get svc gitkubed -o json | 
  	   jq -r .spec.clusterIP)
  ```

- Get ourselves a sample repository with resource YAML files:
  ```
  git clone git://github.com/jpetazzo/kubercoins
  cd kubercoins
  ```

- Add the remote and push to it:
  ```
  git remote add k8s ssh://default-example@$IP/~/git/default-example
  git push k8s master
  ```

.debug[[kube/gitworkflows.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/gitworkflows.md)]
---

## Making changes

- Edit a local file

- Commit

- Push!

- Make sure that you push to the `k8s` remote

.debug[[kube/gitworkflows.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/gitworkflows.md)]
---

## Other features

- Gitkube can also build container images for us

  (see the [documentation](https://github.com/hasura/gitkube/blob/master/docs/remote.md) for more details)

- Gitkube can also deploy Helm Charts

  (instead of raw YAML files)

.debug[[kube/gitworkflows.md](https://github.com/RyaxTech/kube.training.git/tree/gh-pages//kube/gitworkflows.md)]</textarea>
    <script src="remark.min.js" type="text/javascript">
    </script>
    <script src="viz.js"></script>
    <script type="text/javascript">
      var slideshow = remark.create({
        ratio: '16:9',
        highlightSpans: true,
        excludedClasses: ["self-paced"]
      });

    var viz = new Viz({ workerURL: './lite.render.js' });
    var currentSlideViz = document.querySelectorAll(".vizgraph");
    currentSlideViz.forEach((elm) => {
        viz.renderSVGElement(elm.innerText)
        .then(function(element) {
            //the new svg element inherit class of the old element
            elm.classList.forEach((l) => { element.classList.add(l)});
            elm.replaceWith(element);
        });
    });

    </script>
    <!-- 
    These two scripts will be available only when loading the
    content using the pub/sub server. Otherwise, they'll just
    404 and that's OK.
    -->
    <script src="/socket.io/socket.io.js">
    </script>
    <script src="/remote.js">
    </script>

  </body>
</html>
