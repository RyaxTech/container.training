# Daemon sets

- Nous voulons passer √† l'√©chelle `rng` d'une mani√®re qui est diff√©rente de la fa√ßon dont nous avons passer √† l'√©chelle `worker`

- Nous voulons une (et exactement une) instance de `rng` par noeud

- Et si nous d√©ployions simplement `deploy/rng` sur le nombre de n≈ìuds?

  - rien ne garantit que les conteneurs `rng` seront distribu√©s uniform√©ment

  - Si nous ajoutons des n≈ìuds plus tard, ils ne lanceront pas automatiquement une copie de `rng`

  - Si nous supprimons (ou red√©marrons) un n≈ìud, un conteneur `rng` va red√©marrer ailleurs

- Au lieu d'un `deployment`, nous utiliserons un `daemonset`

---

## Daemon set en pratique

- Les daemons sets sont parfaits pour les processus par n≈ìud √† l'√©chelle du cluster:

  - `kube-proxy`

  - `weave` (notre overlay de r√©seau)

  - agents de surveillance

  - des outils de gestion du mat√©riel (par exemple des agents SCSI / FC HBA)

  - etc.

- Ils peuvent √©galement √™tre limit√©s √† l'ex√©cution [uniquement sur certains n≈ìuds](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#running-pods-on-only-some-nodes)

---

## Cr√©ation d'un daemon set

- Malheureusement, √† partir de Kubernetes 1.10, l'interface de ligne de commande ne peut pas cr√©er de jeux de d√©mons

--

- Plus pr√©cis√©ment: il n'a pas de sous-commande pour cr√©er un daemon set

--

- Mais n'importe quel type de ressource peut toujours √™tre cr√©√© en fournissant une description YAML:
  ```bash
  kubectl apply -f foo.yaml
  ```

--

- Comment cr√©ons-nous le fichier YAML pour notre daemon set?

--

  - option 1: [lire les docs](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#create-a-daemonset)

--

  - option 2: `vi` notre moyen de sortir de celui-ci

---

## Cr√©ation du fichier YAML pour notre daemon set

- Commen√ßons par le fichier YAML pour la ressource `rng` courante

.exercise[

- Faites un dump du ressource `rng` dans YAML:
  ```bash
  kubectl get deploy/rng -o yaml --export> rng.yml
  ```

- Modifier `rng.yml`

]

Note: `--export` supprimera les informations sp√©cifiques au cluster, c'est-√†-dire:
- namespace (pour que la ressource ne soit pas li√©e √† un espace de noms sp√©cifique)
- status et creation timestamp (inutile lors de la cr√©ation d'une nouvelle ressource)
- resourceVersion et uid (ils causeraient des probl√®mes ... *int√©ressants*)

---

## "Casting" d'une ressource √† un autre

- Et si on changeait juste le champ `kind`?

  (√áa ne peut pas √™tre aussi simple, non?)

.exercise[

- Changez `kind: Deployment` en `kind: DaemonSet`

- Enregistrer, quitter

- Essayez de cr√©er notre nouvelle ressource:
  ```bash
  kubectl apply -f rng.yml
  ```

]

--

Nous savions tous que cela ne pourrait pas √™tre aussi facile, non!

---

## Comprendre le probl√®me

- Le coeur de l'erreur est:
  ```
  error validating data:
  [ValidationError(DaemonSet.spec):
  unknown field "replicas" in io.k8s.api.extensions.v1beta1.DaemonSetSpec,
  ...
  ```
--

- *De toute √©vidence,* cela n'a pas de sens de sp√©cifier un nombre de r√©plicas pour un ensemble de d√©mons

--

- Solution de contournement: r√©solvez le probl√®me du YAML

  - supprimer le champ `replicas`
  - supprimer le champ `strategy` (qui d√©finit le m√©canisme de d√©ploiement pour un d√©ploiement)
  - enlever la ligne `status: {}` √† la fin

--

- Ou, on pourrait aussi ...

---

## Utilise le `--force`, Luke

- Nous pourrions aussi dire √† Kubernetes d'ignorer ces erreurs et essayer quand m√™me

- Le nom actuel du flag `--force` est` --validate = false`

.exercise[

- Essayez de charger notre fichier YAML et ignorez les erreurs:
  ```bash
  kubectl apply -f rng.yml --validate=false
  ```

]

--

üé©‚ú®üêá

--

Attendez ... Maintenant, cela peut-il √™tre * aussi facile?

---

## V√©rification de ce que nous avons fait

- Avons-nous transform√© notre ¬´deployment¬ª en un ¬´daemonset¬ª?

.exercise[

- Regardez les ressources que nous avons maintenant:
  ```bash
  kubectl get all
  ```

]

--

Nous avons deux ressources appel√©es `rng`:

- le *deployment* existant avant

- le *daemon set* que nous venons de cr√©er

Nous avons aussi un trop grand nombre de pods.
<br/>
(Le module correspondant au *deployment* existe toujours.)

---

## `deploy/rng` et `ds/rng`

- Vous pouvez avoir diff√©rents types de ressources avec le m√™me nom

  (c'est-√†-dire un *deploiement* et un *daemon set* tous deux nomm√©s "rng")

- Nous avons toujours l'ancien d√©ploiement `rng` * *

  ```
NAME                       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/rng        1         1         1            1           18m
  ```


- Mais maintenant nous avons le nouveau *daemon set* `rng`

  ```
NAME                DESIRED  CURRENT  READY  UP-TO-DATE  AVAILABLE  NODE SELECTOR  AGE
daemonset.apps/rng  2        2        2      2           2          <none>         9s
  ```

---

## Trop de pods

- Si nous v√©rifions avec `kubectl get pods`, nous voyons:
  - *un pod* pour le deployment (nomm√© `rng-xxxxxxxxxx-yyyyy`)
  - *un pod par n≈ìud* pour le daemon set (nomm√© `rng-zzzzz`)

  ```
  NAME                        READY     STATUS    RESTARTS   AGE
  rng-54f57d4d49-7pt82        1/1       Running   0          11m
  rng-b85tm                   1/1       Running   0          25s
  rng-hfbrr                   1/1       Running   0          25s
  [...]
  ```
--

Le daemon set a cr√©√© un pod par n≈ìud, sauf sur le n≈ìud master. 
Le n≈ìud principal a [taints](https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/) emp√™chant les pods de s'ex√©cuter.

(Pour planifier un pod sur ce n≈ìud de toute fa√ßon, le pod requiert les [tolerations](https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/) appropri√©es.)

<!-- .footnote[(D√©sactiv√© par un? Nous n'ex√©cutons pas ces pods sur le n≈ìud h√©bergeant le Control Plane.)] -->

---

## Que font tous ces pods?

- V√©rifions les logs de tous ces pods `rng`

- Tous ces pods ont un label `run=rng`:

  - le premier pod, parce que c'est ce que `kubectl run` fait
  - les autres (dans le daemon set), parce que nous avons
    *copi√© la sp√©cification du premier*

- Par cons√©quent, nous pouvons interroger les logs de tout le monde en utilisant ce selector `run=rng`

.exercise[

- V√©rifier les logs de tous les pods ayant un label `run=rng`:
  ```bash
  kubectl logs -l run=rng --tail 1
  ```

]

--

Il semble que *tous les pods* servent des requ√™tes pour le moment.

---

## La magie des selectors

- Le *service* `rng` fait l'√©quilibrage de charge de requ√™tes vers un ensemble de pods

- Cet ensemble de pods est d√©fini comme "pods ayant le label `run=rng`"

.exercise[

- V√©rifiez le *selector* dans la d√©finition du service `rng`:
  ```bash
  kubectl describe service rng
  ```

]

Lorsque nous avons cr√©√© des pods suppl√©mentaires avec ce label, ils √©taient
automatiquement d√©tect√© par `svc/rng` et ajout√© en tant que *endpoints*
√† l'√©quilibreur de charge associ√©.

---

## Retrait du premier pod de l'√©quilibreur de charge

- Que se passerait-il si on supprimait ce pod, avec `kubectl delete pod ...`?

--

  Le `replica set` le recr√©erait imm√©diatement.

--

- Que se passerait-il si nous enlevions le label `run=rng` de ce pod?

--

  Le `replica set` le recr√©erait imm√©diatement.

--

  ... Parce que ce qui compte pour le `replica set`, c'est le nombre de pods *correspondant √† ce selector.*

--

- Mais mais mais ... N'avons-nous pas plus d'un pod avec `run=rng` maintenant?

--

  La r√©ponse r√©side dans le selector exact utilis√© par le `replica set` ...

---

## Plong√©e profonde dans les selectors

- Regardons les selectors pour le *deployment* `rng` et le *replica set* associ√©

.exercise[

- Afficher des informations d√©taill√©es sur le deployment `rng`:
  ```bash
  kubectl describe deploy rng
  ```

- Afficher des informations d√©taill√©es sur le r√©plica `rng`:
  <br/> (La deuxi√®me commande ne n√©cessite pas que vous obteniez le nom exact du replica set)
  ```bash
  kubectl describe rs rng-yyyy
  kubectl describe rs -l run=rng
  ```

]

--

Le selector du replica set poss√®de √©galement un `pod-template-hash`, contrairement aux pods de notre daemon set.

---

# Mise √† jour d'un service via des labels et des selectors

- Et si nous voulons supprimer le deployment `rng` de l'√©quilibreur de charge?

- Option 1:

  - Detruis-le

- Option 2:

  - ajouter un *label* suppl√©mentaire au daemon set

  - mettre √† jour le service *selector* pour faire r√©f√©rence √† ce *label*

--

Bien s√ªr, l'option 2 offre plus d'opportunit√©s d'apprentissage. Non?

---

## Ajouter un label suppl√©mentaire au daemon set

- Nous mettrons √† jour le daemon set "spec"

- Option 1:

  - √©ditez le fichier `rng.yml` que nous avons utilis√© plus t√¥t

  - chargez la nouvelle d√©finition avec `kubectl apply`

- Option 2:

  - utilisez `kubectl edit`

--

*Nous avons inclus quelques conseils sur les prochaines diapositives pour votre facilitation!*

---

## Nous avons mis des ressources dans vos ressources

- Rappel : un daemon set est une ressource qui cr√©e plus de ressources!
- Il y a une diff√©rence entre:

  - le(s) label(s) d'une ressource (dans le bloc `metadata` au d√©but)

  - le selector d'une ressource (dans le bloc `spec`)

  - le(s) label(s) de la (des) ressource(s) cr√©√©e(s) par la premi√®re ressource (dans le bloc `template`)

- Vous devez mettre √† jour le selector et le template (les labels de metadata ne sont pas obligatoires)

- Le template doit correspondre au selector
  (c'est-√†-dire que la ressource refusera de cr√©er des ressources qu'elle ne s√©lectionnera pas)

---

## Ajout de notre label

- Ajoutons un label `isactive: yes`

- En YAML, `yes` doit √™tre cit√©; c'est-√†-dire `isactive: "yes"`

.exercise[

- Mettre √† jour le daemon set pour ajouter `isactive: "yes"` au label du selector et du template:
  ```bash
  kubectl edit daemonset rng
  ```

- Mettre √† jour le service pour ajouter `isactive: "yes"` √† son selector:
  ```bash
  kubectl edit service rng
  ```

]

---

## V√©rification de ce que nous avons fait

.exercise[

- V√©rifiez la ligne du log la plus r√©cente de tous les pods `run=rng` pour confirmer que exactement un par n≈ìud est maintenant actif:
  ```bash
  kubectl logs -l run=rng --tail 1
  ```

]

Les timestamps devraient nous donner un indice sur le nombre de pods qui re√ßoivent actuellement du trafic.

.exercise[

- Regardez les pods que nous avons en ce moment:
  ```bash
  kubectl get pods
  ```

]

---

## Nettoyage

- Les pods du deployment et le "vieux" daemon set sont toujours en cours d'ex√©cution

- Nous allons les identifier par programme

.exercise[

- Listez les pods avec `run=rng` mais sans `isactive=yes`:
  ```bash
  kubectl get pods -l run=rng,isactive!=yes
  ```

- Enlevez ces pods:
  ```bash
  kubectl delete pods -l run=rng,isactive!=yes
  ```

]

---

## Nettoyage des pods morts

```
$ kubectl get pods
NAME                        READY     STATUS        RESTARTS   AGE
rng-54f57d4d49-7pt82        1/1       Terminating   0          51m
rng-54f57d4d49-vgz9h        1/1       Running       0          22s
rng-b85tm                   1/1       Terminating   0          39m
rng-hfbrr                   1/1       Terminating   0          39m
rng-vplmj                   1/1       Running       0          7m
rng-xbpvg                   1/1       Running       0          7m
[...]
```

- Les pods suppl√©mentaires (not√©s `Terminating` ci-dessus) vont dispara√Ætre

- ... Mais un nouveau (`rng-54f57d4d49-vgz9h` ci-dessus) a √©t√© red√©marr√© imm√©diatement!

--

- Rappelez-vous, le *deployment* existe toujours et v√©rifie qu'un pod est op√©rationnel

- Si on supprime le pod associ√© au deployment, il est recr√©√© automatiquement

---

## Suppression d'un d√©ploiement

.exercise[

- Supprimez le d√©ploiement `rng`:
  ```bash
  kubectl delete deployment rng
  ```
]

-

- Le pod cr√©√© par le d√©ploiement est en cours de finalisation:

```
$ kubectl get pods
NAME                        READY     STATUS        RESTARTS   AGE
rng-54f57d4d49-vgz9h        1/1       Terminating   0          4m
rng-vplmj                   1/1       Running       0          11m
rng-xbpvg                   1/1       Running       0          11m
[...]
```

Ding, dong, le d√©ploiement est mort! Et le daemon set continue √† vivre.

---

## √âviter les pods suppl√©mentaires

- Lorsque nous avons chang√© la d√©finition de daemon set, il a imm√©diatement cr√©√© de nouveaux pods. Nous avons d√ª enlever les anciens manuellement.

- Comment aurions-nous pu √©viter cela?

-

- En ajoutant le label `isactive:"yes"` aux pods avant de changer le daemon set!

- Cela peut √™tre fait par programme avec `kubectl patch`:

  ```bash
    PATCH='
    metadata:
      labels:
        isactive: "yes"
    '
    kubectl get pods -l run=rng -l controller-revision-hash -o name |
      xargs kubectl patch -p "$PATCH" 
  ```


---

## Labels et d√©bogage

- Quand un pod se comporte mal, on peut le supprimer: un autre sera recr√©√©

- Mais on peut aussi changer ses labels

- Il sera supprim√© de l'√©quilibreur de charge (load balancer) (il ne recevra plus de trafic)

- Un autre pod sera recr√©√© imm√©diatement

- Mais le pod probl√©matique est toujours l√†, et nous pouvons l'inspecter et le d√©boguer

- Nous pouvons m√™me le rajouter √† la rotation si n√©cessaire

  (Tr√®s utile pour r√©soudre les bugs intermittents et insaisissables)

---

## Labels et contr√¥le de rollout avanc√©

- Inversement, nous pouvons ajouter des pods correspondant au selector d'un service

- Ces pods recevront alors des requ√™tes et serviront le trafic

- Exemples:

  - pod One-shot avec tous les flags de d√©bogage activ√©, pour collecter des logs

  - pods cr√©√©s automatiquement, mais ajout√©s √† la rotation dans une seconde √©tape
    <br/>
    (en d√©finissant leur label en cons√©quence)

- Cela nous donne des blocs de construction pour les "canary" et "blue/green deployments"

---

## Pour aller plus loin

- Comprendre le concept des [DaemonSet](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/)

- Comprendre les [labels et selectors](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/)




