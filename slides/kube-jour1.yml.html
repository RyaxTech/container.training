<!DOCTYPE html>
<html>
  <head>
    <title>Kubernetes Introduction, Architecture et Installation </title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <link rel="stylesheet" href="workshop.css">
  </head>
  <body>
    <!--
    <div style="position: absolute; left: 20%; right: 20%; top: 30%;">
      <h1 style="font-size: 3em;">Loading ...</h1>
      The slides should show up here. If they don't, it might be
      because you are accessing this file directly from your filesystem.
      It needs to be served from a web server. You can try this:
      <pre>
        docker-compose up -d
        open http://localhost:8888/workshop.html # on MacOS
        xdg-open http://localhost:8888/workshop.html # on Linux
      </pre>
      Once the slides are loaded, this notice disappears when you
      go full screen (e.g. by hitting "f").
    </div>
    -->
    <textarea id="source">```
Invalid chapter: common/title_fr.md
```

---
## Some infos about the instructor

- Yiannis Georgiou - CTO/Co-Founder Ryax Technologies (Startup that provides a software platform for Data Analytics Workflows Automation)

- PhD Université Grenoble-Alpes - Resource Management and Scheduling on High Performance Computing

- 11 ans at Bull/Atos Technologies - R&D Architect / Software Engineer 

.debug[
```
 M slides/kube-jour1.yml.html

```

These slides have been built from commit: 665dc53


[logistics.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/logistics.md)]
---

## Logistics

- The training will take place from  9h until 17h30

- There will be a break for lunch from 12h30 to 14h

- Feel free to interrupt for questions at any time

- Especially when you see full screen container pictures!

.debug[[logistics.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/logistics.md)]
---
```
Invalid chapter: common/about-slides_fr.md
```

---
```
Invalid chapter: common/toc.md
```

---
```
Invalid chapter: common/prereqs_fr.md
```

---
```
Invalid chapter: intro/Docker_Overview_fr.md
```

---
```
Invalid chapter: intro/Docker_History_fr.md
```

---
```
Invalid chapter: common/sampleapp_fr.md
```

---
```
Invalid chapter: common/composescale_fr.md
```

---
```
Invalid chapter: common/composedown_fr.md
```

---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/Container-Ship-Freighter-Navigation-Elbe-Romance-1782991.jpg)]

---

name: toc-introduction-de-kubernetes
class: title

Introduction de Kubernetes

.nav[
[Section précédente](#toc-)
|
[Retour table des matières](#toc-chapter-1)
|
[Section suivante](#toc-modle-de-rseau-de-kubernetes)
]

.debug[(automatically generated title slide)]

---
# Introduction de Kubernetes

- Kubernetes est un système de gestion de conteneur

- Il exécute et gère les applications conteneurisées sur un cluster

--

- Qu'est-ce que cela signifie vraiment?

.debug[[kube/concepts-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/concepts-k8s_fr.md)]
---

## Introduction de Kubernetes

--

- C'est un logiciel pour *déployer et gérer* des applications conteneurisées tout en offrant la *meilleure utilisation possible* de la plate-forme de calcul.

--

- Il fait *l'abstraction* de l'infrastructure sous-jacente en *simplifiant le développement* d'applications et la *gestion du matériel*.

.debug[[kube/concepts-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/concepts-k8s_fr.md)]
---

## Benefices de Kubernetes

--

- Simplification du *déploiement* d'applications.

--

- Amélioration de *l'utilisation* du système matériel.

--

- *Passage à l'échelle* automatique de l'application.

--

- *Simplification du développement* d'applications

--

- *Tolérance aux pannes*, *haute disponibilité* et *auto-guérison*

.debug[[kube/concepts-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/concepts-k8s_fr.md)]
---

## Choses de base que nous pouvons demander à Kubernetes

--

- Démarrer 5 conteneurs en utilisant l'image `atseashop/api:v1.3`

--

- Placer un 'load balancer' interne devant ces conteneurs

--

- Démarrer 10 conteneurs en utilisant l'image `atseashop/webfront:v1.3`

--

- Placez un 'load balancer' public devant ces conteneurs

--

- C'est Noël, beaucoup de trafic, augmenter notre cluster et ajouter des conteneurs

--

- Nouvelle version! Remplacer mes conteneurs avec la nouvelle image `atseashop/webfront:v1.4`

--

- Continuez à traiter les demandes pendant la mise à niveau; mettre à jour mes conteneurs un à la fois

.debug[[kube/concepts-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/concepts-k8s_fr.md)]
---

## D'autres choses que Kubernetes peut faire pour nous

- Autoscaling de base

- Déploiement bleu / vert, déploiement canari

- Les services à long terme, mais aussi les travaux par batch (ponctuels)

- Overcommit notre cluster et *expulser* les jobs de basse priorité

- Exécuter des services avec des données * stateful * (bases de données, etc.)

- Contrôle d'accès à grain fin définissant * ce qui * peut être fait par * qui * sur * quelles * ressources

- Intégration de services tiers (* catalogue de services *)

- Automatiser des tâches complexes (* opérateurs *)

.debug[[kube/concepts-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/concepts-k8s_fr.md)]
---

## Kubernetes architecture

.debug[[kube/concepts-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/concepts-k8s_fr.md)]
---

class: pic

![haha seulement blague](images/k8s-arch1.png)

.debug[[kube/concepts-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/concepts-k8s_fr.md)]
---

## Kubernetes architecture

- Ha ha ha ha

- OK, j'essayais de vous faire peur, c'est beaucoup plus simple que ça ❤️

.debug[[kube/concepts-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/concepts-k8s_fr.md)]
---

class: pic

![Celui-là ressemble plus à la réalité](images/kube_archi_simple.png)

.debug[[kube/concepts-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/concepts-k8s_fr.md)]
---

class: pic

![Celui-là ressemble plus à la réalité](images/k8s-arch2.png)

.debug[[kube/concepts-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/concepts-k8s_fr.md)]
---

## Crédits

- Le premier schéma est un cluster Kubernetes avec stockage soutenu par iSCSI "multi-path"

  (Source: [Yongbok Kim](https://www.yongbok.net/blog/))

- Le second est repris par le livre de Marko Luksa "Kubernetes in Action"

- Le troisieme est une représentation simplifiée d'un cluster Kubernetes

  (Source: [Imesh Gunaratne](https://medium.com/containermind/a-reference-architecture-for-deploying-wso2-middleware-on-kubernetes-d4dee7601e8e))

.debug[[kube/concepts-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/concepts-k8s_fr.md)]
---

## A savoir...

- Comment prononce-t-on kubernetes?
    - Mot venant du grecque κυβερνήτης, prononcé "kivernitis"
    - En anglais : "coubernetis"
    - En français : "cubernetesse" ou "cubernette"

- On peut abbréger Kubernetes en k8s

- Kubernetes viens avec de l'autocomplétion à intégrer dans votre bash :

`source <(kubectl completion bash)`

Ça complète les commandes mais aussi les noms des objets!

Commande déjà éfféctuée dans vos VMs.


.debug[[kube/concepts-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/concepts-k8s_fr.md)]
---

## Architecture de Kubernetes: les noeuds

- Les nœuds exécutant nos conteneurs exécutent une collection de services:

  - **Container Runtime** (typiquement Docker pour le deploiment de conteneurs)

  - **Kubelet** (l'agent de noeud, gere les conteneurs, communique avec l'API)

  - **Kube-proxy** (un composant réseau qui fait du "load-balancing")

- Les nœuds étaient autrefois appelés "minions"

  (Vous pourriez voir ce mot dans les anciens articles ou dans la documentation)

.debug[[kube/concepts-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/concepts-k8s_fr.md)]
---

## Architecture de Kubernetes: le "Control Plane"

- La logique de Kubernetes (ses "cerveaux") est une collection de services:

  - **API server**: notre point d'entrée pour tout!

  - **Scheduler**: affecte des nœuds aux composants
  
  - **Controller Manager**: fonctions de niveau de cluster
  
  - **Etcd**: un key/value store fiable, la "base de données" de Kubernetes

- Ensemble, ces services forment le "Control Plane" de notre cluster

- Le "Control Plane" est aussi appelé le "master"

.debug[[kube/concepts-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/concepts-k8s_fr.md)]
---

## Exécution du "Control Plane" sur des noeuds spéciaux

- Il est courant de réserver un noeud dédié au plan de contrôle

  (Sauf pour les clusters de développement à noeud unique, comme lorsque vous utilisez minikube)

- Ce noeud s'appelle alors un "master"

  (Oui, c'est ambigu: le "master" est-il un nœud, ou tout le plan de contrôle?)

- Les applications normales sont limitées à l'exécution sur ce noeud

  (En utilisant un mécanisme appelé ["taints"](https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/))

- Lorsque la haute disponibilité est requise, chaque service du plan de contrôle doit être résilient

- Le plan de contrôle est ensuite répliqué sur plusieurs nœuds

  (Ceci est parfois appelé une configuration "multi-master")

.debug[[kube/concepts-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/concepts-k8s_fr.md)]
---

## Exécution du "Control Plane" en dehors des conteneurs

- Les services du "Control Plane" peuvent fonctionner dans ou hors des conteneurs

- Par exemple: puisque `etcd` est un service critique, certaines personnes
  déployer directement sur un cluster dédié (sans conteneurs)

  (Ceci est illustré sur le premier schéma "super compliqué")

- Dans certaines offres Kubernetes hébergées (par exemple, GKE), le plan de contrôle est invisible

  (Nous ne "voyons" qu'un point de terminaison API Kubernetes)

- Dans ce cas, il n'y a pas de "noeud master"

* Pour cette raison, il est plus juste de dire "Control Plane" plutôt que "master". *

.debug[[kube/concepts-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/concepts-k8s_fr.md)]
---

## Doit-on lancer Docker du tout?

Non!

--

- Par défaut, Kubernetes utilise Docker Engine pour exécuter les conteneurs

- Nous pourrions aussi utiliser `rkt` ("Rocket") de CoreOS/Redhat

- Ou tirer parti d'autres runtimes connectables via l'interface *Container Runtime*

  (comme CRI-O, ou containerd)

.debug[[kube/concepts-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/concepts-k8s_fr.md)]
---

## Doit-on lancer Docker du tout?

Oui!

--

- Dans cette formation, nous exécutons notre application sur un seul noeud en premier

- Nous aurons besoin de construire des images et de les expédier

- Nous pouvons faire ces choses sans Docker mais

- Docker est toujours le moteur de conteneur le plus stable aujourd'hui
  <br/>
  (mais d'autres options mûrissent très rapidement)

.debug[[kube/concepts-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/concepts-k8s_fr.md)]
---

## Doit-on lancer Docker du tout?

- Sur nos environnements de développement, les pipelines CI ...:

  *Oui, presque certainement*

- Sur nos serveurs de production:

  *Oui (aujourd'hui)*

  *Probablement pas (dans le futur)*

.footnote[Plus d'informations sur CRI [sur le blog Kubernetes](https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes)]

.debug[[kube/concepts-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/concepts-k8s_fr.md)]
---

## Ressources de Kubernetes

- L'API Kubernetes définit beaucoup d'objets appelés *ressources*

- Ces ressources sont organisées par type, ou `Kind` (dans l'API)

- Quelques types de ressources communs sont:

  - noeud (une machine - physique ou virtuelle - dans notre cluster)
  - pod (groupe de conteneurs fonctionnant ensemble sur un noeud)
  - service (point de terminaison réseau stable pour se connecter à un ou plusieurs conteneurs)
  - namespace (groupe de choses plus ou moins isolé)
  - secret (paquet de données sensibles à transmettre à un conteneur)
 
  Et beaucoup plus! (Nous pouvons voir la liste complète en exécutant `kubectl get`)

.debug[[kube/concepts-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/concepts-k8s_fr.md)]
---

class: pic

![Nœud, pod, conteneur](images/k8s-arch3-thanks-weave.png)

.debug[[kube/concepts-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/concepts-k8s_fr.md)]
---

class: pic

![Un des meilleurs diagrammes d'architecture Kubernetes disponibles](images/k8s-arch4-thanks-luxas.png)

.debug[[kube/concepts-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/concepts-k8s_fr.md)]
---

## Crédits

- Le premier diagramme est une gracieuseté de Weave Works

  - un *pod* peut avoir plusieurs conteneurs travaillant ensemble

  - Les adresses IP sont associées à *pods*, pas avec des conteneurs individuels

- Le deuxième diagramme est une gracieuseté de Lucas Käldström, dans [cette présentation](https://speakerdeck.com/luxas/kubeadm-cluster-creation-internals-from-self-hosting-to-upgradability-and-ha)

  - c'est l'un des meilleurs diagrammes d'architecture Kubernetes disponibles!


.debug[[kube/concepts-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/concepts-k8s_fr.md)]
---
```
Invalid chapter: common/declarative_fr.md
```

---
## Déclaratif vs impératif dans Kubernetes

- Pratiquement tout ce que nous créons dans Kubernetes est créé à partir d'un *spec*

- Surveillez les champs `spec` dans les fichiers YAML plus tard!

- Le *spec* décrit *comment nous voulons que la chose soit*

- Kubernetes va *réconcilier* l'état actuel avec les spécifications
  <br/> (techniquement, cela est fait par un certain nombre de *contrôleurs*)

- Quand on veut changer de ressource, on met à jour la *spec*

- Kubernetes va ensuite *converger* cette ressource

.debug[[kube/declarative_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/declarative_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/ShippingContainerSFBay.jpg)]

---

name: toc-modle-de-rseau-de-kubernetes
class: title

Modèle de réseau de Kubernetes

.nav[
[Section précédente](#toc-introduction-de-kubernetes)
|
[Retour table des matières](#toc-chapter-1)
|
[Section suivante](#toc-premier-contact-avec-kubectl)
]

.debug[(automatically generated title slide)]

---
# Modèle de réseau de Kubernetes

- TL,DR:

  *Notre cluster (nœuds et pods) est un grand réseau IP plat.*

--

- En détail:

 - tous les nœuds doivent pouvoir se rejoindre, sans NAT

 - Tous les pods doivent pouvoir se rejoindre, sans NAT

 - Les pods et les nœuds doivent pouvoir se rejoindre, sans NAT

 - chaque pod est au courant de son adresse IP (pas de NAT)

- Kubernetes n'impose aucune implémentation particulière

.debug[[kube/kubenet_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/kubenet_fr.md)]
---

## Modèle de réseau de Kubernetes: le bon

- Tout peut atteindre tout

- Pas de traduction d'adresse

- Pas de traduction de port

- Pas de nouveau protocole

- Les pods ne peuvent pas se déplacer d'un noeud à l'autre et conserver leur adresse IP

- Les adresses IP ne doivent pas être "portables" d'un nœud à l'autre

  (Nous pouvons utiliser par exemple un sous-réseau par nœud et utiliser une topologie routée simple)

- La spécification est assez simple pour permettre de nombreuses implémentations différentes

.debug[[kube/kubenet_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/kubenet_fr.md)]
---

## Modèle de réseau Kubernetes: le moins bon

- Tout peut atteindre tout

  - Si vous voulez de la sécurité, vous devez ajouter des règles de réseau

  - l'implémentation réseau dont vous avez besoin doit les prendre en charge

- Il y a littéralement des dizaines d'implémentations là-bas

  (15 sont répertoriés dans la documentation de Kubernetes)

- Les pods ont une connectivité de niveau 3 (IP), mais les *services* sont de niveau 4

  (Services mappent vers un seul port UDP ou TCP, aucune plage de ports ou paquets IP arbitraires)

- `kube-proxy` est sur le chemin de données lors de la connexion à un pod ou un conteneur,
  <br/> et ce n'est pas particulièrement rapide (repose sur un proxy utilisateur ou sur iptables)

.debug[[kube/kubenet_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/kubenet_fr.md)]
---

## Modèle de réseau Kubernetes: en pratique

- Les nœuds que nous utilisons ont été configurés pour utiliser [Weave](https://github.com/weaveworks/weave)

- Nous n'approuvons pas Weave d'une manière particulière, ça marche juste pour nous

- Ne vous inquiétez pas de l'avertissement concernant les performances de `kube-proxy`

- À moins que vous:

  - saturer régulièrement les interfaces réseau 10G
  - compte les taux de paquets en millions par seconde
  - lancer des plateformes VOIP ou de jeu à fort trafic
  - faire des choses bizarres qui impliquent des millions de connexions simultanées
    <br/> (auquel cas vous connaissez déjà le réglage du noyau)

- Si nécessaire, il existe des alternatives à `kube-proxy`, par exemple [kube-router](https://www.kube-router.io)

.debug[[kube/kubenet_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/kubenet_fr.md)]
---

## Le "Container Network Interface" (CNI)

- Le CNI a une [spécification](https://github.com/containernetworking/cni/blob/master/SPEC.md#network-configuration) bien définie pour les plugins réseau

- Lorsqu'un pod est créé, Kubernetes délègue la configuration réseau aux plugins CNI

- Typiquement, un plugin CNI va:

  - allouer une adresse IP (en appelant un plugin IPAM)

  - ajouter une interface réseau dans l'espace de noms réseau du pod

  - configurer l'interface ainsi que les routes requises, etc.

- Utiliser plusieurs plugins peut être fait avec des "méta-plugins" comme CNI-Genie ou Multus

- Tous les plugins CNI ne sont pas égaux

  (par exemple, ils n'implémentent pas tous les stratégies de réseau, qui sont nécessaires pour isoler les pods)

.debug[[kube/kubenet_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/kubenet_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/aerial-view-of-containers.jpg)]

---

name: toc-premier-contact-avec-kubectl
class: title

Premier contact avec `kubectl`

.nav[
[Section précédente](#toc-modle-de-rseau-de-kubernetes)
|
[Retour table des matières](#toc-chapter-1)
|
[Section suivante](#toc-grer-nos-premiers-conteneurs-sur-kubernetes)
]

.debug[(automatically generated title slide)]

---
# Premier contact avec `kubectl`

- `kubectl` est (presque) le seul outil dont nous aurons besoin pour parler à Kubernetes

- C'est un outil CLI riche autour de l'API Kubernetes

  (Tout ce que vous pouvez faire avec `kubectl`, vous pouvez le faire directement avec l'API)

- Sur nos machines, il y a un fichier `~/.kube/config` avec:

  - l'adresse de l'API Kubernetes

  - le chemin vers nos certificats TLS utilisés pour l'authentification

- Vous pouvez également utiliser l'indicateur `--kubeconfig` pour passer un fichier de configuration

- Ou directement `--server`,` --user`, etc.

- «kubectl» peut être prononcé «Cube C T L», «Cube cuttle» ...

.debug[[kube/kubectlget_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/kubectlget_fr.md)]
---

## `kubectl get`

- Regardons nos ressources `Node` avec` kubectl get`!

.exercise[

- Regardez la composition de notre cluster:
  ```bash
  kubectl get node
  ```

- Ces commandes sont équivalentes:
  ```bash
  kubectl get no
  kubectl get node
  kubectl get nodes
  ```

]

.debug[[kube/kubectlget_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/kubectlget_fr.md)]
---

## Obtention d'une sortie lisible par machine

- `kubectl get` peut sortir JSON, YAML, ou être directement formaté

.exercise[

- Donnez-nous plus d'informations sur les nœuds:
  ```bash
  kubectl get nodes -o wide
  ```

- Ayons du YAML:
  ```bash
  kubectl get no -o yaml
  ```
  Vous voyez ce `kind: List` à la fin? C'est le type de notre résultat!

]

.debug[[kube/kubectlget_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/kubectlget_fr.md)]
---

## Utilisation de `kubectl` et `jq`

- C'est super facile de construire des rapports personnalisés

.exercise[

- Afficher la capacité de tous nos nœuds en tant que flux d'objets JSON:
  ```bash
    kubectl get nodes -o json |
            jq ".items [] | {nom: .metadata.name} + .status.capacity"
  ```

]

.debug[[kube/kubectlget_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/kubectlget_fr.md)]
---

## Qu'est-ce qui est disponible?

- `kubectl` a de très bonnes installations d'introspection

- Nous pouvons lister tous les types de ressources disponibles en exécutant `kubectl get`

- Nous pouvons voir les détails d'une ressource avec:
  ```bash
  kubectl describe type/name
  kubectl describe type name
  ```

- Nous pouvons voir la définition d'un type de ressource avec:
  ```bash
  kubectl explain type
  ```

Chaque fois, `type` peut être un nom de type singulier, pluriel ou abrégé.

.debug[[kube/kubectlget_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/kubectlget_fr.md)]
---

## Services

- Un *service* est un point de terminaison stable pour se connecter à "quelque chose"

  (Dans la proposition initiale, ils étaient appelés "portals")

.exercise[

- Listez les services sur notre cluster avec l'une de ces commandes:
  ```bash
  kubectl get services
  kubectl get svc
  ```

]

--

Il y a déjà un service sur notre cluster: l'API Kubernetes elle-même.

.debug[[kube/kubectlget_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/kubectlget_fr.md)]
---

## Services ClusterIP

- Un service `ClusterIP` est interne, disponible uniquement à partir du cluster

- Ceci est utile pour l'introspection à l'intérieur des conteneurs

.exercise[

- Essayez de vous connecter à l'API:
  ```bash
  curl -k https://`10.96.0.1`
  ```
  
  - `-k` est utilisé pour ignorer la vérification du certificat

  - Assurez-vous de remplacer 10.96.0.1 avec le CLUSTER-IP montré par `kubectl get svc`

]

--

L'erreur que nous voyons est attendue: l'API Kubernetes nécessite une authentification.

.debug[[kube/kubectlget_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/kubectlget_fr.md)]
---

## Liste des conteneurs en cours d'exécution

- Les conteneurs sont manipulés via *pods*

- Un pod est un groupe de conteneurs:

 - fonctionnant ensemble (sur le même noeud)

 - partage des ressources (RAM, CPU, mais aussi réseau, volumes)

.exercise[

- Liste des pods sur notre cluster:
  ```bash
  kubectl get pods
  ```

]

--

*Ce ne sont pas les pods que vous cherchez.* Mais où sont-ils?!?

.debug[[kube/kubectlget_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/kubectlget_fr.md)]
---

## Namespaces

- Les namespaces nous permettent de séparer les ressources

.exercise[

- Liste les espaces de noms sur notre cluster avec l'une de ces commandes:
  ```bash
  kubectl get namespaces
  kubectl get namespace
  kubectl get ns
  ```

]

--

*Vous savez quoi ... Ce truc de "kube-system" semble suspect. *

.debug[[kube/kubectlget_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/kubectlget_fr.md)]
---

## Accès aux namespaces

- Par défaut, `kubectl` utilise le namespace `default`

- Nous pouvons passer à un namespace différent avec l'option `-n`

.exercise[

- Lister les pods dans l'espace de noms `kube-system`:
  ```bash
  kubectl -n kube-system get pods
  ```

]

--

* Ding ding ding ding! *

L'espace de noms `kube-system` est utilisé pour le "Control Plane".

.debug[[kube/kubectlget_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/kubectlget_fr.md)]
---

## Quels sont tous ces pod du Control Plane?

- `etcd` est notre serveur etcd

- `kube-apiserver` est le serveur API

- `kube-controller-manager` et `kube-scheduler` sont d'autres composants principaux

- `kube-dns` (ou `coredns`) est un composant supplémentaire (pas obligatoire mais super utile, donc c'est là)

- `kube-proxy` est le composant (par noeud) gérant les mappages de ports et tel

- `weave` est le composant (par noeud) gérant l'overlay du réseau

- la colonne `READY` indique le nombre de conteneurs dans chaque pod

- les pods dont le nom se termine par `-node1` sont les composants master
  <br/>
  (Ils ont été spécifiquement "épinglé" au nœud master)

.debug[[kube/kubectlget_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/kubectlget_fr.md)]
---

## Qu'en est-il de `kube-public`?

.exercise[

- Lister les pods dans le namespace `kube-public`:
  ```bash
  kubectl -n kube-public get pods
  ```

]

--

- Peut-être n'a-t-il pas de pods, mais quels sont les secrets du `kube-public`?

--

.exercise[

- Liste les secrets dans le namespace `kube-public`:
  ```bash
  kubectl -n kube-public get secrets
  ```

]
--

- `kube-public` est créé par kubeadm & [utilisé pour le bootstrap de sécurité](https://kubernetes.io/blog/2017/01/stronger-foundation-for-creating-and-managing-kubernetes-clusters)


.debug[[kube/kubectlget_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/kubectlget_fr.md)]
---

## Pour aller plus loin

- Les [composants de bases](https://kubernetes.io/docs/concepts/overview/components/) de Kubernetes.

- Comprendre [les objets](https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/) de Kubernetes.




.debug[[kube/kubectlget_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/kubectlget_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/blue-containers.jpg)]

---

name: toc-grer-nos-premiers-conteneurs-sur-kubernetes
class: title

Gérer nos premiers conteneurs sur Kubernetes

.nav[
[Section précédente](#toc-premier-contact-avec-kubectl)
|
[Retour table des matières](#toc-chapter-1)
|
[Section suivante](#toc-installation-de-kubernetes)
]

.debug[(automatically generated title slide)]

---
# Gérer nos premiers conteneurs sur Kubernetes

- Premières choses d'abord: nous ne pouvons pas executer un conteneur

--

- Nous allons lancer un pod, et dans ce pod il y aura un seul conteneur

--

- Dans ce conteneur dans le pod, nous allons lancer une simple commande `ping`

- Ensuite, nous allons commencer des copies supplémentaires du pod

.debug[[kube/kubectlrun_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/kubectlrun_fr.md)]
---

## Démarrer un pod simple avec `kubectl run`

- Nous devons spécifier au moins un *name* et l'image que nous voulons utiliser

.exercise[

- Nous allong pinger `1.1.1.1`, Cloudflare's
  [DNS public resolver](https://blog.cloudflare.com/announcing-1111/):
  ```bash
  kubectl run pingpong --image alpine ping 1.1.1.1
  ```

]

--

OK, qu'est-ce qui vient de se passer?

.debug[[kube/kubectlrun_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/kubectlrun_fr.md)]
---

## Dans les coulisses de `kubectl run`

- Regardons les ressources qui ont été créées par `kubectl run`

.exercise[

- Listez la plupart des types de ressources:
  ```bash
  kubectl get all
  ```

]

--

Nous devrions voir les choses suivantes:
- `deployment.apps/pingpong` (le *deployment* que nous venons de créer)
- `replicaset.apps/pingpong-xxxxxxxxxx` (un *replica set* créé par le deployment)
- `pod/pingpong-xxxxxxxxxx-yyyyy` (un *pod* créé par le replica set)

Note: à partir de 1.10.1, les types de ressources sont affichés plus en détail.

.debug[[kube/kubectlrun_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/kubectlrun_fr.md)]
---

## Quelles sont ces différentes choses?

- Un *deployment* est une construction de haut niveau

  - permet le "scaling", "rolling updates", "rollbacks"

  - plusieurs déploiements peuvent être utilisés ensemble pour mettre en œuvre
    [canary deployment](https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#canary-deployments)

  - délègue la gestion des pods aux *replica sets*

- Un *replica set* est une construction de bas niveau

  - s'assure qu'un nombre donné de pods identiques fonctionnent

  - permet le "scaling"

  - rarement utilisé directement

.debug[[kube/kubectlrun_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/kubectlrun_fr.md)]
---

## Notre deployment `pingpong`

- `kubectl run` a créé un *deployment*,`deployment.apps/pingpong`

```
NAME                       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/pingpong   1         1         1            1           10m
```

- Ce deployment a créé un *replica set*, `replicaset.apps/pingpong-xxxxxxxxxx`

```
NAME                                  DESIRED   CURRENT   READY     AGE
replicaset.apps/pingpong-7c8bbcd9bc   1         1         1         10m
```

- Ce replica set a créé un *pod*, `pod/pingpong-xxxxxxxxxx-yyyyy`

```
NAME                            READY     STATUS    RESTARTS   AGE
pod/pingpong-7c8bbcd9bc-6c9qz   1/1       Running   0          10m
```

- Nous verrons plus tard comment ces trucs jouent ensemble pour:

    - scaling, high availability, rolling updates
.debug[[kube/kubectlrun_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/kubectlrun_fr.md)]
---

## Affichage de la sortie du conteneur

- Utilisons la commande `kubectl logs`

- Nous allons passer soit un *pod name*, soit un *type/name*

  (Par exemple, si nous spécifions un deployment ou un replica set, il recevra le premier pod)

- Sauf indication contraire, il ne montrera que les logs du premier conteneur dans le pod

  (Bonne chose, il n'y en a qu'une dans la nôtre!)

.exercise[

- Voir le résultat de notre commande `ping`:
  ```bash
  kubectl logs deploy/pingpong
  ```

]

.debug[[kube/kubectlrun_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/kubectlrun_fr.md)]
---

## Flux de logs en temps réel

- Tout comme `docker logs`, `kubectl logs` supporte les options pratiques:

  - `-f`/`--follow` pour streamer les logs en temps réel (à la `tail -f`)

  - `--tail` pour indiquer combien de lignes vous voulez voir (à partir de la fin)

  - `--since` pour obtenir les logs seulement après un timestamp donné

.exercise[

- Voir les derniers logs de notre commande `ping`:
  ```bash
  kubectl logs deploy/pingpong --tail 1 --follow
  ```

]

.debug[[kube/kubectlrun_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/kubectlrun_fr.md)]
---

## "Scaling" de notre application

- Nous pouvons créer des copies supplémentaires de notre conteneur (je veux dire, notre pod) avec «kubectl scale»

.exercise[

- Scale notre deployment `pingpong`:
  ```bash
  kubectl scale deploy/pingpong --replicas 8
  ```

]

Note: Et si nous essayions de scaler `replicaset.apps/pingpong-xxxxxxxxxx`?

Nous pourrions! Mais le *deployment* le remarquerait tout de suite et reviendrait au niveau initial.

.debug[[kube/kubectlrun_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/kubectlrun_fr.md)]
---

## Résilience

- Le *deployment* `pingpong` regarde son *replica set*

- Le *replica set* assure que le bon nombre de *pods* sont en cours d'exécution

- Que se passe-t-il si les pods disparaissent?

.exercise[

- Dans une fenêtre séparée, affichez les pods et continuez à les regarder:
  ```bash
  kubectl get pods -w
  ```

- Détruire un pod:
  ```bash
  kubectl delete pod pingpong-xxxxxxxxxx-yyyyy
  ```
]

.debug[[kube/kubectlrun_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/kubectlrun_fr.md)]
---

## Et si on voulait quelque chose de différent?

- Et si nous voulions démarrer un conteneur "one-shot" qui ne redémarre pas?

- Nous pourrions utiliser `kubectl run --restart=OnFailure` ou `kubectl run --restart=Never`

- Ces commandes créeraient *jobs* ou *pods* au lieu de *deployments*

- Sous le tapis, `kubectl run` invoque des "generators" pour créer des descriptions de ressources

- Nous pourrions aussi écrire nous-mêmes ces descriptions de ressources (typiquement en YAML), et les créer sur le cluster avec `kubectl apply -f` (discuté plus tard)

- Avec `kubectl run --schedule=...`, nous pouvons aussi créer des *cronjobs*

.debug[[kube/kubectlrun_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/kubectlrun_fr.md)]
---

## Affichage des logs de plusieurs pods

- Lorsque nous spécifions un nom de deployment, seuls les logs d'un seul pod sont affichés

- Nous pouvons voir les logs de plusieurs pods en spécifiant un *selector*

- Un selector est une expression logique utilisant des *labels*

- Commodément, quand vous "kubectl run somename", les objets associés ont un label `run = somename`

.exercise[

- Regardez la dernière ligne du log de tous les pods avec le label `run = pingpong`:
  ```bash
  kubectl logs -l run=pingpong --tail 1
  ```

]

Malheureusement, `--follow` ne peut pas (encore) être utilisé pour diffuser les logs de plusieurs conteneurs.

.debug[[kube/kubectlrun_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/kubectlrun_fr.md)]
---

## Ne nous inondons pas 1.1.1.1?

- Si vous vous posez cette question, bonne question!

- Ne vous inquiétez pas, cependant:

  *Le groupe de recherche de l'APNIC détenait les adresses IP 1.1.1.1 et 1.0.0.1. Alors que les adresses étaient valides, tant de gens les avaient entrés dans divers systèmes aléatoires qu'ils étaient continuellement submergés par un flot de déchets. L'APNIC voulait étudier ce trafic de déchets, mais chaque fois qu'il avait essayé d'annoncer les IP, l'inondation submergerait tout réseau conventionnel.*

  (Source: https://blog.cloudflare.com/announcing-1111/)

- Il est très peu probable que nos pings concertés parviennent à produire
  même un modeste coup au Cloudflare!

.debug[[kube/kubectlrun_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/kubectlrun_fr.md)]
---

## Tout couper

.exercise[

- Arrétez le deployment:
  ```bash
  kubectl delete deploy/pingpong
  ```
- Quel est l'état de l'application ?
  ```bash
  kubectl get all
  ```

]

- Pour aller plus loin:

  - [Lancer un deployement dupuis un fichier YAML](https://kubernetes.io/docs/tasks/run-application/run-stateless-application-deployment/)

  - [Lancer un *cronjob*](https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/)


.debug[[kube/kubectlrun_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/kubectlrun_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/chinook-helicopter-container.jpg)]

---

name: toc-installation-de-kubernetes
class: title

Installation de Kubernetes

.nav[
[Section précédente](#toc-grer-nos-premiers-conteneurs-sur-kubernetes)
|
[Retour table des matières](#toc-chapter-2)
|
[Section suivante](#toc-le-dashboard-de-kubernetes)
]

.debug[(automatically generated title slide)]

---
# Installation de Kubernetes

- Comment avons-nous mis en place ces clusters Kubernetes que nous utilisons?

--

- Nous avons utilisé `kubeadm` sur des instances VM fraîchement installées exécutant Ubuntu 16.04 LTS

    1. Installez Docker

    2. Installer les paquets Kubernetes

    3. Exécutez `kubeadm init` sur le noeud maître

    4. Configurer Weave (le réseau de superposition)
       <br/>
       (cette étape est juste une commande "kubectl apply", discutée plus tard)

    5. Exécutez `kubeadm join` sur les autres noeuds (avec le token produit par` kubeadm init`)

    6. Copiez le fichier de configuration généré par `kubeadm init`

- Vérifiez le [prepare VMs README](https://github.com/RyaxTech/kube.training/blob/master/prepare-vms/README.md) pour plus de détails

.debug[[kube/setup-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/setup-k8s_fr.md)]
---

## "kubeadm" inconvénients

- Ne configure pas Docker ou tout autre moteur de conteneur

- Ne configure pas le réseau d'overlay

- Ne configure pas multi-master (pas de haute disponibilité)

--

  (Au moins pas encore!)

--

- "Il reste deux fois plus d'étapes que la mise en place d'un cluster Docker Swarm" 

.debug[[kube/setup-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/setup-k8s_fr.md)]
---

## Autres options de déploiement

- Si vous êtes sur Azure:
  [AKS](https://azure.microsoft.com/services/container-service/)

- Si vous êtes sur Google Cloud:
  [GKE](https://cloud.google.com/kubernetes-engine/)

- Si vous êtes sur AWS:
  [EKS](https://aws.amazon.com/eks/)
  ou
  [kops](https://github.com/kubernetes/kops)

- Sur une machine locale:
  [minikube](https://kubernetes.io/docs/getting-started-guides/minikube/),
  [kubespawn](https://github.com/kinvolk/kube-spawn),
  [Docker4Mac](https://docs.docker.com/docker-for-mac/kubernetes/)

- Si vous voulez quelque chose de personnalisable:
  [kubicorn](https://github.com/kubicorn/kubicorn)

  Probablement le plus proche d'une solution multi-cloud / hybride jusqu'à présent, mais en développement

.debug[[kube/setup-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/setup-k8s_fr.md)]
---

## Encore plus d'options de déploiement

- Si vous aimez Ansible:
  [kubespray](https://github.com/kubernetes-incubator/kubespray)

- Si vous aimez Terraform:
  [typhon](https://github.com/poseidon/typhoon/)

- Vous pouvez également apprendre à installer chaque composant manuellement, avec
  l'excellent tutoriel [Kubernetes The Hard Way](https://github.com/kelseyhightower/kubernetes-the-hard-way)

  *Kubernetes The Hard Way est optimisé pour l'apprentissage, ce qui signifie prendre le long chemin pour s'assurer que vous comprenez chaque tâche requise pour démarrer un cluster Kubernetes.*

- Il y a aussi beaucoup d'options commerciales disponibles!

- Pour une liste plus longue, consultez la documentation de Kubernetes:
  <br/>
  il a un excellent guide pour [choisir la bonne solution](https://kubernetes.io/docs/setup/pick-right-solution/) pour configurer Kubernetes.

.debug[[kube/setup-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/setup-k8s_fr.md)]
---

## Installation avec Kubeadm

- Nous n'allons pas faire l'installation de Kubernetes.

- Les slides suivantes vous permettes d'avoir les bases avec Kubeadm.

.debug[[kube/setup-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/setup-k8s_fr.md)]
---

## Installation avec Kubeadm


.exercise[

- Installation de paquets Docker si ils ne sont pas installé sur chaque noeud du cluster:
  ```bash
    sudo su
    apt-get update
    apt-get install -y apt-transport-https ca-certificates curl software-properties-common
    curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
    add-apt-repository "deb https://download.docker.com/linux/$(. /etc/os-release; echo "$ID") $(lsb_release -cs) stable"
    apt-get update && apt-get install -y docker-ce docker-compose
    exit
    sudo groupadd docker
    sudo usermod -aG docker $USER
  ```
]

.debug[[kube/setup-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/setup-k8s_fr.md)]
---

## Installation avec Kubeadm suite


.exercise[

- Installation de paquets Kubernetes si ils ne sont pas installé sur chaque noeud du cluster:
  ```bash
    sudo apt-get update && sudo apt-get install -y apt-transport-https curl
    curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
    sudo su
    cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
    deb http://apt.kubernetes.io/ kubernetes-xenial main
    EOF
    exit
    sudo apt-get update
    sudo apt-get install -y kubelet kubeadm kubectl
   ```
]

.debug[[kube/setup-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/setup-k8s_fr.md)]
---

## Installation avec Kubeadm suite

.exercise[

- Configuration de Kubernetes avec Kubeadm au premier noeud du cluster:
  ```bash
    sudo kubeadm init 
    sudo mkdir -p $HOME/.kube /home/docker/.kube
    sudo cp /etc/kubernetes/admin.conf $HOME/.kube/config
    sudo cp /etc/kubernetes/admin.conf /home/docker/.kube/config
    sudo chown -R $(id -u) $HOME/.kube
    kubever=$(kubectl version | base64 | tr -d '\n')
    kubectl apply -f https://cloud.weave.works/k8s/net?k8s-version=$kubever
  ```

- Configuration de Kubernetes avec Kubeadm aux autres noeuds du cluster:
- Appliquez la commande retourné par `kubeadm init` sur le master
- Testez si les noeuds sont bien configurés avec:
  ```bash
  kubectl get nodes
  ```
]





.debug[[kube/setup-k8s_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/setup-k8s_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/container-cranes.jpg)]

---

name: toc-le-dashboard-de-kubernetes
class: title

Le dashboard de Kubernetes

.nav[
[Section précédente](#toc-installation-de-kubernetes)
|
[Retour table des matières](#toc-chapter-2)
|
[Section suivante](#toc-les-implications-de-scurit-de-kubectl-apply)
]

.debug[(automatically generated title slide)]

---
# Le dashboard de Kubernetes

- Les ressources de Kubernetes peuvent également être visualisées avec un dashboard web

- Nous allons déployer ce dashboard avec *trois commandes:*

  1) plutot *exécuter* le dashboard

  2) contourner SSL pour le dashboard

  3) Ignorer l'authentification pour le dashboard

--

Il y a une étape supplémentaire pour rendre le dashboard disponible de l'extérieur (nous y reviendrons)

--

.footnote[.warning[Oui, cela ouvrira notre cluster à toutes sortes de manigances. Ne fais pas ça à la maison.]]

.debug[[kube/dashboard_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/dashboard_fr.md)]
---

## 1) Exécution du dashboard

- Nous devons créer un * déploiement * et un * service * pour le dashboard

- Mais aussi un * secret *, un * compte de service *, un * rôle * et un * rôle contraignant *

- Toutes ces choses peuvent être définies dans un fichier YAML et créées avec `kubectl apply -f`

.exercise[

- Créer toutes les ressources du dashboard, avec la commande suivante:
  ```bash
  kubectl apply -f https://goo.gl/Qamqab
  ```

]

L'URL de goo.gl se développe pour:
<br/>
.small[https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml]

.debug[[kube/dashboard_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/dashboard_fr.md)]
---


## 2) Contournement SSL pour le dashboard

- Le dashboard Kubernetes utilise HTTPS, mais nous n'avons pas de certificat

- Les versions récentes de Chrome (63 et versions ultérieures) et Edge refusent de se connecter

  (Vous n'aurez même pas l'option d'ignorer un avertissement de sécurité!)

- Nous pourrions (et devrions!) Obtenir un certificat, par ex. avec [Let's Encrypt](https://letsencrypt.org/)

- ... Mais pour plus de commodité, pour cet atelier, nous transmettrons HTTP à HTTPS

.warning[Ne le faites pas à la maison, ou pire, au travail!]

.debug[[kube/dashboard_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/dashboard_fr.md)]
---

## Exécution du déballeur SSL

- Nous allons lancer [`socat`](http://www.dest-unreach.org/socat/doc/socat.html), en lui disant d'accepter les connexions TCP et de les relayer via SSL

- Ensuite, nous allons exposer cette instance `socat` avec un service `NodePort`

- Pour plus de commodité, ces étapes sont soigneusement encapsulées dans un autre fichier YAML

.exercise[

- Appliquez le fichier YAML pratique et annulez la protection SSL:
  ```bash
  kubectl apply -f https://goo.gl/tA7GLz
  ```

]

L'URL goo.gl se développe pour:
<br/>
.small[.small[https://gist.githubusercontent.com/jpetazzo/c53a28b5b7fdae88bc3c5f0945552c04/raw/da13ef1bdd38cc0e90b7a4074be8d6a0215e1a65/socat.yaml]]

.warning[Tout notre trafic de dashboard est maintenant en texte clair, y compris les mots de passe!]

.debug[[kube/dashboard_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/dashboard_fr.md)]
---

## Connexion au dashboard

.exercise[

- Vérifiez quel port est le dashboard:
  ```bash
  kubectl -n kube-system get svc socat
  ```

]

Vous aurez besoin du port `8080`.


.exercise[

- Connectez-vous à http://oneofournodes:3xxxx/

]

Le dashboard vous demandera ensuite l'authentification que vous souhaitez utiliser.

.debug[[kube/dashboard_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/dashboard_fr.md)]
---

## Authentification de dashboard

- Nous avons trois options d'authentification à ce stade:

  - token (associé à un role disposant des autorisations appropriées)

  - kubeconfig (par exemple en utilisant le fichier `~/.kube/config` de `node1`)

  - "skip" (utilisez le dashboard "service account")

- Utilisons "skip": nous recevons un tas d'avertissements et ne voyons pas grand-chose

.debug[[kube/dashboard_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/dashboard_fr.md)]
---

## 3) Ignorer l'authentification pour le dashboard

- La documentation du dashboard [explique comment procéder](https://github.com/kubernetes/dashboard/wiki/Access-control#admin-privileges)

- Nous avons juste besoin de charger un autre fichier YAML!

.exercise[

- Accorder des privilèges d'administrateur au dashboard afin que nous puissions voir nos ressources:
  ```bash
  kubectl apply -f https://goo.gl/CHsLTA
  ```

- Rechargez le dashboard et profitez-en!

]

--

.warning[Au fait, nous venons d'ajouter une porte dérobée à notre cluster Kubernetes!]

.debug[[kube/dashboard_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/dashboard_fr.md)]
---

## Exposer le dashboard sur HTTPS

- Nous avons pris un raccourci en transférant HTTP à HTTPS dans le cluster

- Exposons le dashboard sur HTTPS!

- Le dashboard est exposé via un service `ClusterIP` (trafic interne uniquement)

- Nous allons changer cela en un service `NodePort` (acceptant le trafic extérieur)

.exercise[

- Modifier le service:
  ```bash
  kubectl edit service kubernetes-dashboard
  ```

]

--

`NotFound`?!? Pourquoi ca ne marche pas?!?

.debug[[kube/dashboard_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/dashboard_fr.md)]
---

## Modification du service `kubernetes-dashboard`

- Si nous regardons le [YAML](https://goo.gl/Qamqab) que nous avons chargé avant, nous aurons un indice

--

- Le dashboard a été créé dans le namespace `kube-system`

--

.exercise[

- Modifier le service:
  ```bash
  kubectl -n kube-system edit service kubernetes-dashboard
  ```

- Changez `type: ClusterIP` en `type: NodePort`, sauvegardez et quittez

- Vérifiez le port qui a été assigné avec `kubectl -n kube-system get services`

- Connectez-vous à https://oneofournodes:3xxxx/ (oui, https)

]

.debug[[kube/dashboard_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/dashboard_fr.md)]
---

## Exécution sécurisée du dashboard Kubernetes

- Les étapes que nous venons de vous montrer sont *avec des buts éducatives seulement!*

- Si vous faites cela sur votre cluster de production, les gens [peuvent et vont en abuser](https://blog.redlock.io/cryptojacking-tesla)

- Pour une discussion approfondie sur la sécurisation du dashboard,
  <br/>
  vérifier [cet excellent post sur le blog de Heptio](https://blog.heptio.com/on-securing-the-kubernetes-dashboard-16b09b1b7aca)

.debug[[kube/dashboard_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/dashboard_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/container-housing.jpg)]

---

name: toc-les-implications-de-scurit-de-kubectl-apply
class: title

Les implications de sécurité de `kubectl apply`

.nav[
[Section précédente](#toc-le-dashboard-de-kubernetes)
|
[Retour table des matières](#toc-chapter-2)
|
[Section suivante](#toc-)
]

.debug[(automatically generated title slide)]

---

# Les implications de sécurité de `kubectl apply`

- Quand nous faisons `kubectl apply -f <URL>`, nous créons des ressources arbitraires

- Les ressources peuvent être mauvaises; Imaginez un `deployment` ...

--

   - démarre des mineurs bitcoin sur l'ensemble du cluster

--

   - cache dans un espace de noms autre que le "default"

--

   - bind-mounts le système de fichiers de nos nœuds

--

   - insère les clés SSH dans le compte root (sur le noeud)

--

   - crypte nos données et les garde en rancon

--

   - ☠️☠️☠️

.debug[[kube/dashboard_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/dashboard_fr.md)]
---

## `kubectl apply` est le nouveau `curl | sh`

- `curl | sh` est pratique

- Il est sûr si vous utilisez des URL HTTPS provenant de sources fiables

--

- `kubectl apply -f` est pratique

- Il est sûr si vous utilisez des URL HTTPS provenant de sources fiables

- Exemple: les instructions d'installation officielles pour la plupart des réseaux de pod

--

- Il introduit de nouveaux modes de défaillance (comme si vous essayez d'appliquer yaml à partir d'un lien qui n'est plus valide)


.debug[[kube/dashboard_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/dashboard_fr.md)]
---

## Pour aller plus loin


.exercise[
- Relancez l'exemple préçédent
  ```bash
  kubectl run pingpong --image alpine ping 1.1.1.1
  ```
- Observez le deployment et son pod. Trouvez-vous les logs du pod?

- Arrétez le deployment.

]

Vous pouvez stopper le dashboard ou le laisser. Comme le dashboard n'est pas sécurisé, nous conseillons de l'arréter.
```bash
  kubectl delete -f https://goo.gl/CHsLTA
  ```



.debug[[kube/dashboard_fr.md](https://github.com/RyaxTech/kube.training.git/tree/k8s_metropole_de_lyon/slides/kube/dashboard_fr.md)]</textarea>
    <script src="remark.min.js" type="text/javascript">
    </script>
    <script type="text/javascript">
      var slideshow = remark.create({
        ratio: '16:9',
        highlightSpans: true,
        excludedClasses: ["self-paced"]
      });
    </script>
    
    <!-- 
    These two scripts will be available only when loading the
    content using the pub/sub server. Otherwise, they'll just
    404 and that's OK.
    -->
    <script src="/socket.io/socket.io.js">
    </script>
    <script src="/remote.js">
    </script>

  </body>
</html>
